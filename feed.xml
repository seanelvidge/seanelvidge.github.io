<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://seanelvidge.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://seanelvidge.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-21T18:54:19+00:00</updated><id>https://seanelvidge.github.io/feed.xml</id><title type="html">Sean Elvidge</title><entry><title type="html">Tracking Football Team Strengths with a Bayesian Kalman Model</title><link href="https://seanelvidge.github.io/articles/2025/Football_team_rankings/" rel="alternate" type="text/html" title="Tracking Football Team Strengths with a Bayesian Kalman Model"/><published>2025-12-15T09:20:00+00:00</published><updated>2025-12-15T09:20:00+00:00</updated><id>https://seanelvidge.github.io/articles/2025/Football_team_rankings</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2025/Football_team_rankings/"><![CDATA[<p>Not all football-rating systems are the same. Many of the public ones, like the excellent <a href="http://clubelo.com/System">ClubElo</a>, do a fine job of ranking teams using elegant updates. Win and your rating rises, lose and it falls; the amount you move depends on how “surprised” the model was.</p> <p>This model begins in the same spirit but replaces those heuristic updates with a probabilistic engine: a <em>Bayesian Extended Kalman Filter</em> coupled to a modern version of the Bradley–Terry model (with added draws). The core idea is simple: treat each team’s strength as something hidden that we estimate and track through time, with uncertainty that expands between matches and contracts when new results arrive.</p> <p>Across the <a href="https://seanelvidge.com/articles/2024/All_England_football_league_results/">full historical dataset</a> (every English league match since 1888) the system achieves a mean Brier score of 0.2035. These values are significantly better than many published computer models (e.g. <a href="https://www.stat.cmu.edu/cmsac/sure/2023/showcase/soccer/report.html">Nyamdorj et al. 2014</a>, <a href="https://bsic.it/odds-at-play-testing-efficiency-in-the-premier-league-and-serie-a/">BSIC, 2024</a> and <a href="https://harvardsportsanalysis.org/2015/07/5988/">Harvard Sports Analysis Collective, 2015</a>) which indicates a stable, well-calibrated predictive performance. Crucially, because the filter quantifies its own uncertainty, it can tell us not only who is strongest, but how confident we should be in that judgement.</p> <p>The rest of this post goes into the mathematical details of the ranking algorithm, but if you want to access the underlying data it is <a href="https://github.com/seanelvidge/England-football-results">available here</a> (specifically the file <a href="https://raw.githubusercontent.com/seanelvidge/England-football-results/refs/heads/main/EnglandLeagueResults_wRanks.csv"><code class="language-plaintext highlighter-rouge">EnglandLeagueResults_wRanks.csv</code></a>).</p> <h2 id="the-big-picture">The Big Picture</h2> <p>Imagine every team has a hidden “true strength” \(s_i\). Before a match, the home and away teams carry beliefs about their current strengths, each with an associated uncertainty. When they play, the result provides new information that updates those beliefs.</p> <p>In the ClubElo framework this update is written directly as</p> \[R_{\text{new}} = R_{\text{old}} + K(S - E),\] <p>where \(S\) is the score (1 for a win, 0.5 for a draw, 0 for a loss), \(E\) is the expected probability, and \(K\) is a fixed responsiveness parameter.</p> <p>Here, the same logic is embedded in a <a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter</a>, which means the effective \(K\) is learned automatically. The update size depends on two things: how uncertain we are about the teams, and how surprising the result was. Upsets between uncertain teams lead to large updates; shocks between well-understood teams barely move the needle.</p> <p>Between matches, team strengths are not frozen. Instead, they evolve according to a <em>mean-reverting stochastic process</em>, allowing form to drift while preventing runaway behaviour. Newly promoted teams begin with large uncertainty and adapt quickly; established teams gravitate toward long-term baselines rather than rising indefinitely.</p> <h2 id="modelling-match-outcomes-wins-draws-losses">Modelling Match Outcomes (Wins, Draws, Losses)</h2> <p>Match outcomes are modelled using the <a href="https://www.jstor.org/stable/2283595">Davidson extension</a> of the <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley–Terry model</a>, which naturally incorporates draws. The probability of each outcome is</p> \[\Pr(\text{Home}) = \frac{e^{\Delta}}{e^{\Delta} + e^{-\Delta} + \kappa},\] \[\Pr(\text{Draw}) = \frac{\kappa}{e^{\Delta} + e^{-\Delta} + \kappa},\] \[\Pr(\text{Away}) = \frac{e^{-\Delta}}{e^{\Delta} + e^{-\Delta} + \kappa},\] <p>where</p> \[\Delta = \beta \bigl(s_H - s_A + h\bigr).\] <p>Here, \(s_H\) and \(s_A\) are the latent strengths of the home and away teams, \(\beta\) is a scaling parameter, \(\kappa\) controls the draw rate, and \(h\) is the home-advantage term.</p> <h2 id="home-advantage---explicit-and-time-varying">Home Advantage - Explicit and Time-Varying</h2> <p>Home advantage is not treated as a fixed constant. Instead, it is explicitly modelled and allowed to vary over time. Historical analysis shows that home-win rates in English football have declined dramatically since the late 19th century, falling from well above 60% to closer to 40–45% in the modern era (see <a href="https://seanelvidge.com/articles/2025/Home_advantage_in_English_football/">this analysis</a>).</p> <p>By allowing the home-advantage parameter \(h\) to evolve slowly with time, the model correctly distinguishes between a home match in 1890 and one in 2025. This prevents systematic bias when comparing teams across eras and is a key improvement over static-advantage rating systems.</p> <h2 id="validation-and-rating-scale">Validation and Rating Scale</h2> <p>Predictive accuracy is measured using the <a href="https://en.wikipedia.org/wiki/Brier_score">Brier score</a>, defined as the mean squared error between predicted probabilities and observed outcomes. Over the full dataset the score is 0.2035 (for the 2024/25 season it is 0.2085), indicating robust calibration both historically and in the present day.</p> <p>Internally, the filter operates on a latent “skill” scale roughly spanning \(-3\) to \(+3\). However for presentation, these values are mapped linearly onto an Elo-style scale, centred on 1000 points, with elite teams reaching 1800+ and lower-league teams clustering about a thousand points below. This transformation is purely cosmetic; all inference happens on the latent scale.</p> <h1 id="part-ii--if-you-dare-read-on-the-mathematics">Part II — If You Dare Read On: The Mathematics</h1> <h2 id="1-state-evolution-ornsteinuhlenbeck-dynamics">1. State Evolution (<a href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process">Ornstein–Uhlenbeck</a> Dynamics)</h2> <p>Each team’s latent strength evolves as</p> \[s_{i,t+1} = \rho\, s_{i,t} + (1 - \rho)\, \mu_{\text{tier}(i,t)} + \varepsilon_{i,t},\] <p>with</p> \[\varepsilon_{i,t} \sim \mathcal{N}(0, q_i).\] <p>Here, \(\rho\) controls persistence, \(\mu_{\text{tier}}\) is the long-term baseline for the team’s division tier, and \(q_i\) is the process variance. This formulation allows ratings to drift while remaining anchored to realistic league-level expectations.</p> <h2 id="2-observation-model">2. Observation Model</h2> <p>Given predicted strengths \(s_H\) and \(s_A\), the model produces a probability vector</p> \[\mathbf{p} = \begin{bmatrix} p_H \\ p_D \\ p_A \end{bmatrix},\] <p>using the Davidson–Bradley–Terry equations above. The Jacobian matrix \(H\) is computed by differentiating \(\mathbf{p}\) with respect to the state vector \([s_H, s_A]^T\), enabling linearisation of the nonlinear observation model.</p> <h2 id="3-extended-kalman-filter-update">3. Extended Kalman Filter Update</h2> <p>Prediction step:</p> \[x^- = F x_t,\] \[P^- = F P_t F^\top + Q,\] <p>where \(F = \rho I\) and \(Q\) is the process-noise covariance.</p> <p>Observation noise:</p> \[R = \operatorname{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^\top.\] <p>Update step:</p> \[K = P^- H^\top \bigl(H P^- H^\top + R\bigr)^{-1},\] \[x_{t+1} = x^- + K (y - \mathbf{p}),\] \[P_{t+1} = (I - K H) P^-.\] <p>The Kalman gain \(K\) replaces the fixed \(K\)-factor of traditional Elo systems, adapting automatically to uncertainty and surprise. A secondary control loop monitors the normalised innovation squared to ensure statistical consistency over time.</p> <h2 id="why-this-matters">Why This Matters</h2> <p>This framework enforces mathematical honesty. Uncertainty is explicit, calibration is measurable, and every parameter (\(\beta\), \(\rho\), \(q\), \(\kappa\), tier baselines, and historical home advantage) has a clear interpretation. Instead of a single magic constant, the model becomes a living system that adapts across seasons, divisions, and eras.</p>]]></content><author><name></name></author><category term="football"/><category term="mathematics"/><summary type="html"><![CDATA[The methodology behind my football team strength model.]]></summary></entry><entry><title type="html">Football Bogey Grounds and How Statistics Can Prove Them</title><link href="https://seanelvidge.github.io/articles/2025/Football_bogey_grounds/" rel="alternate" type="text/html" title="Football Bogey Grounds and How Statistics Can Prove Them"/><published>2025-12-12T19:04:00+00:00</published><updated>2025-12-12T19:04:00+00:00</updated><id>https://seanelvidge.github.io/articles/2025/Football_bogey_grounds</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2025/Football_bogey_grounds/"><![CDATA[<p>Football supporters are never short of folklore. Some of it heroic, some of it tragic, and some of it mathematically suspicious. Among the more enduring tales is the <em>bogey ground</em>: that one venue where your club never quite manages to win, no matter how many times the fixture computer sends you there with fresh optimism, a new manager and a good run of form.</p> <p>But one question always pops up in the back of my mind when I hear this: When is a bogey ground actually a bogey ground, and when is it just a trick of small numbers?</p> <p>Most fans instinctively understand the point. Losing your only ever visit to Carlisle does not make Brunton Park a cursed ground. A couple of failed trips to Luton are not evidence of eldritch forces at work. Yet, scattered across the long history of English league football, a few pairs of teams have met often enough, and still produced zero away wins, that bogey grounds become statistically real.</p> <p>In the Premier League an example of this is Fulham, and their trips to Arsenal.</p> <p>Fulham have played 32 league matches away at Arsenal. They have never won.</p> <p>Not once. Not in 1914 (lost 2-0). Not in 1964 (2-2). Not in 2014 (lost 2-0).</p> <p>Zero wins from thirty-two attempts. Seven draws. Twenty-five defeats.</p> <p>Even more striking, this isn’t just a bad example, this is, among every club in the entire Football League dataset, over 135 years of football, no team has a larger, more emphatic, more mathematically convincing record of away futility than Fulham at Arsenal.</p> <p>So let’s take a look, not just at the record itself, but at how we can quantify bogeyness in a way that recognises your intuition (“two games aren’t enough!”) but takes advantage of the huge dataset football provides.</p> <h2 id="why-never-won-isnt-enough-the-curse-of-small-n">Why “never won” isn’t enough (the curse of small \(n\))</h2> <p>Imagine you flip a coin twice and get two tails. Does that mean the coin is biased?</p> <p>Of course not. You need <em>more</em> flips before you start thinking that someone is up.</p> <p>The same goes for football. A team losing its only away visit to Manchester City tells you nothing. Losing twice? Still nothing except a faint sense of déjà vu. Losing five times? Now you’re paying attention. Losing ten times? Stop buying tickets to the away game.</p> <p>So what we need is a way of answering this question:</p> <p>Given a team has played \(n\) away matches at a ground and won none, what is a reasonable upper limit on how good their <em>true</em> chance of winning there might be?</p> <p>Fortunately statistics, as it always does, gives us a handy tool for working this out, the <a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval">Wilson confidence interval.</a></p> <h2 id="a-gentle-introduction-to-the-wilson-interval">A (gentle) introduction to the Wilson interval</h2> <p>Don’t panic; no equations are necessary. Just an idea. A Wilson interval takes two bits of information:</p> <ul> <li>the number of games played (\(n\)), and</li> <li>the number of games won (0, in our bogey-ground case)</li> </ul> <p>and asks:</p> <p>“If their true underlying chance of winning were \(p\), how big could \(p\) realistically be before the observed data (zero wins) would start to look implausible?”</p> <p>For small \(n\) (not many games played) the answer is: “\(p\) could still be quite large.”</p> <p>For large \(n\) (a large number of games played) the answer is: “\(p\) must be quite small.”</p> <p>This lines up with our intuition.</p> <p>A team with 0 wins from 2 tries? Their true win rate might easily be 30%, 40%, even 50%.</p> <p>A team with 0 wins from 32 tries? Now the interval collapses. It tells you the true away win probability is almost certainly very small, low enough that even by bad luck alone, you’d be surprised <em>not</em> to have picked up a single win after that many attempts.</p> <p>The Wilson interval is good for this particular problem (compared to other approaches like the Wald Interval) because it behaves properly at the boundaries (like at 0% or 100%), where ordinary methods can breakdown. Football fans should love it because it gives scientific legitimacy to something they’ve always known: <strong>some bogey grounds are imaginary, but a few are real</strong>.</p> <h2 id="the-most-convincing-bogey-of-them-all-fulham-at-arsenal">The most convincing bogey of them all: Fulham at Arsenal</h2> <p>So, what does Wilson say about Fulham’s 0 wins from 32 away league games at Arsenal?</p> <p>It says this:</p> <blockquote> <p>In each of Fulham’s away matches against Arsenal their probability of winning must have been (at the very most) 10% for 0 wins out of 32 to not be statistically suspicious.</p> </blockquote> <p>Mathematically this comes from plugging \(n\) (number of games) and \(z\) (which is equal to 1.96 for 95% ‘confidence’) into the equation for calculating the upper bound of the Wilson interval:</p> \[\mbox{Wilson Upper Bound} = \frac{z^2}{n+z^2} = \frac{1.96^2}{32+1.96^2} = 0.107 = 10.7\%\] <p>So the only way we could avoid saying Arsenal is Fulham’s away team nemesis is if we believe Fulham, across the whole history of the fixture (starting in 1913; <a href="https://seanelvidge.com/h2h?team1=Arsenal&amp;team2=Fulham">see my head-to-head tool here</a>), have not had better than a 10% chance of winning those games. Whilst we can’t be (mathematically) certain that it is true, it is incredibly unlikely. Typical away win rates in the top division over history are roughly 25-30% range. Even clear cut underdogs often have pre-match win probabilities around 15-20%.</p> <p>So, therefore, I think it is safe to say to conclude:</p> <blockquote> <p>Fulham away at Arsenal is not just a bogey ground, it is the bogey ground.</p> </blockquote> <p>The Premier League’s most mathematically defensible curse.</p> <h2 id="other-bogey-teams">Other bogey teams</h2> <p>Fulham–Arsenal is the only pair in the <a href="https://seanelvidge.com/articles/2024/All_England_football_league_results/">entire database</a> whose Wilson upper bound hovers at just above the 10% threshold. But several others get close, here are the best (or worst) of the rest:</p> <table style="border-collapse: collapse; width: 75%;"> <thead> <tr> <th style="border: 1px solid black; padding: 8px;">Away Team</th> <th style="border: 1px solid black; padding: 8px;">Home Team</th> <th style="border: 1px solid black; padding: 8px;">Played</th> <th style="border: 1px solid black; padding: 8px;">Record</th> <th style="border: 1px solid black; padding: 8px;">Wilson Upper</th> </tr> </thead> <tbody> <tr> <td style="border: 1px solid black; padding: 8px;">Fulham</td> <td style="border: 1px solid black; padding: 8px;">Arsenal</td> <td style="border: 1px solid black; padding: 8px;">32</td> <td style="border: 1px solid black; padding: 8px;">0W–7D–25L</td> <td style="border: 1px solid black; padding: 8px;">10.7%</td> </tr> <tr> <td style="border: 1px solid black; padding: 8px;">Grimsby Town</td> <td style="border: 1px solid black; padding: 8px;">Blackburn Rovers</td> <td style="border: 1px solid black; padding: 8px;">28</td> <td style="border: 1px solid black; padding: 8px;">0W–9D–19L</td> <td style="border: 1px solid black; padding: 8px;">12.1%</td> </tr> <tr> <td style="border: 1px solid black; padding: 8px;">Mansfield Town</td> <td style="border: 1px solid black; padding: 8px;">Reading</td> <td style="border: 1px solid black; padding: 8px;">26</td> <td style="border: 1px solid black; padding: 8px;">0W–5D–21L</td> <td style="border: 1px solid black; padding: 8px;">12.9%</td> </tr> <tr> <td style="border: 1px solid black; padding: 8px;">Tranmere Rovers</td> <td style="border: 1px solid black; padding: 8px;">Barnsley</td> <td style="border: 1px solid black; padding: 8px;">26</td> <td style="border: 1px solid black; padding: 8px;">0W–11D–15L</td> <td style="border: 1px solid black; padding: 8px;">12.9%</td> </tr> <tr> <td style="border: 1px solid black; padding: 8px;">Oldham Athletic</td> <td style="border: 1px solid black; padding: 8px;">Charlton Athletic</td> <td style="border: 1px solid black; padding: 8px;">25</td> <td style="border: 1px solid black; padding: 8px;">0W–11D–14L</td> <td style="border: 1px solid black; padding: 8px;">13.3%</td> </tr> <tr> <td style="border: 1px solid black; padding: 8px;">Newport County</td> <td style="border: 1px solid black; padding: 8px;">Luton Town</td> <td style="border: 1px solid black; padding: 8px;">24</td> <td style="border: 1px solid black; padding: 8px;">0W–7D–17L</td> <td style="border: 1px solid black; padding: 8px;">13.8%</td> </tr> <tr> <td style="border: 1px solid black; padding: 8px;">Coventry City</td> <td style="border: 1px solid black; padding: 8px;">Preston North End</td> <td style="border: 1px solid black; padding: 8px;">24</td> <td style="border: 1px solid black; padding: 8px;">0W–9D–15L</td> <td style="border: 1px solid black; padding: 8px;">13.8%</td> </tr> </tbody> </table> <p><br/></p> <p>These are not small samples. These are not casual coincidences. When you are approaching 20 or 30 visits with no victory, it is time to stop thinking you are unlucky and start accepting you have a bogey ground.</p>]]></content><author><name></name></author><category term="football"/><category term="mathematics"/><summary type="html"><![CDATA[Can a football club actually have a statistically verifiable football bogey ground or is it just bad luck?]]></summary></entry><entry><title type="html">40 points to avoid relegation?</title><link href="https://seanelvidge.github.io/articles/2025/40_points_to_avoid_relegation/" rel="alternate" type="text/html" title="40 points to avoid relegation?"/><published>2025-11-08T23:04:00+00:00</published><updated>2025-11-08T23:04:00+00:00</updated><id>https://seanelvidge.github.io/articles/2025/40_points_to_avoid_relegation</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2025/40_points_to_avoid_relegation/"><![CDATA[<html> <head> <style>.chart-figure{width:100%;margin:1rem 0}.chart-container{position:relative;width:100%;height:55vh;min-height:420px;max-height:80vh}.chart-container canvas{width:100%!important;height:100%!important;display:block}@media(max-width:640px){.chart-container{height:65vh;min-height:480px}}</style> </head> </html> <p>The number has become part of Premier League folk law. Forty points. Reach 40 and you can relax, the trapdoor to the Championship won’t open.</p> <p>At the start of the 2015/16 season (the season Leicester City won the League!) their manager Claudio Ranieri set them the target of reaching 40 points to avoid relegation. When they hit the target:</p> <blockquote> <p>“We have 40 points which was the goal. It’s champagne for my players!”</p> </blockquote> <blockquote> <p>Claudio Ranieri, 2 Jan, 2016.</p> </blockquote> <p>But where did this number come from, and it is the right target?</p> <p>Many people have noted that the 40 point mark is a “myth” (e.g. <a href="https://www.premierleague.com/en/news/3932287">The Premier League</a>, <a href="https://www.bbc.co.uk/sport/football/43049564">BBC</a>, <a href="https://www.nytimes.com/athletic/6126560/2025/02/12/leicester-city-fixtures-premier-league-relegation/">The Athletic</a>). But those articles are really just saying that, on average, you need less than 40 points to survive (and the number of points you need seems to be decreasing).</p> <figure class="chart-figure"> <div class="chart-container"> <canvas id="pointsChart"></canvas> </div> <figcaption></figcaption> </figure> <p>The plot above shows the number of points required to stay in the top division of English football since 3 points for a win was introduced in 1985 (this includes before the Premier League started in 1992). The league tables were created using my <a href="https://seanelvidge.com/leaguetable">arbitrary league table generator</a>. It is also worth noting that during the time range there have been a varying number of teams in the top division (between 20 and 22) which obviously impacts the points required. We have normalized this to a 38-game season for comparison.</p> <p>A few things are immediatly obvious:</p> <ol> <li>Most of the time you do not need 40 points to avoid relegation (34/44; 77%),</li> <li>The average number of points needed to avoid relegation over the last 40 years is 37 (in the Premier League era, it is 36 points),</li> <li>There is a clear decreasing trend of the number of points required.</li> </ol> <p>So where does the idea that you need 40 points come from? There is no obvious origin of the phrase. My best guess is that in the mid 90s, after Southampton survied (just, on goal difference) with 38 points in 1995/96, followed by Coventry City with 41 the following year, then Everton with 40 and then Southampton (again) with 41 points in 1998/99, that that run of four was enough for the (rough) number to stick.</p> <p>But despite the previous analysis (by me and many others) I think 40 is still the correct, pre-season, target. Moving to (for example) a 37 point target would only give you slightly better than a 50-50 chance of staying up (54.5%). Perhaps you would like a little more certainty…</p> <p>One way to look at this is to calculate the cumulative distribution function (CDF). The CDF is a function that shows the probability that a random variable is less than or equal to a specific value. It “accumulates” or “adds up” the probabilities for all outcomes up to a certain point.</p> <figure class="chart-figure"> <div class="chart-container"> <canvas id="cdfChart"></canvas> </div> <figcaption></figcaption> </figure> <p>The above figure shows two, slightly different, CDFs, in blue across the whole dataset (since 1985) and in red just in the Premier League era. To read the plot look at the number of points on the x-axis and then read off the corresponding probability of avoiding relegation on the y-axis for that number of points.</p> <p>Now the 40 point value makes a lot more sense, as it gives a team over an 80% chance of staying in the division, now I prefer those odds!</p> <p>So, whilst there are plenty of posts online telling you that the 40 point target is a myth, I think, as pre-season target for teams, it is still a good one.</p> <script src="https://cdn.jsdelivr.net/npm/chart.js@4"></script> <canvas id="pointsChart"></canvas> <script>function formatSeason(t){const e=parseInt(t,10);return`${e-1}/${String(e).slice(-2).padStart(2,"0")}`}function linearFit(t,e){const a=t.length,r=t.reduce((t,e)=>t+e,0),o=e.reduce((t,e)=>t+e,0),i=(a*t.reduce((t,a,r)=>t+a*e[r],0)-r*o)/(a*t.reduce((t,e)=>t+e*e,0)-r*r);return{m:i,c:(o-i*r)/a}}const chartData={datasets:[{label:"(Normalized) Points Needed to Avoid Relegation",data:[{x:"1982",y:38.9},{x:"1983",y:43.4},{x:"1984",y:44.3},{x:"1985",y:45.2},{x:"1986",y:38},{x:"1987",y:38.9},{x:"1988",y:34.2},{x:"1989",y:40},{x:"1990",y:44},{x:"1991",y:38},{x:"1992",y:38.9},{x:"1993",y:45.2},{x:"1994",y:38.9},{x:"1995",y:39.8},{x:"1996",y:39},{x:"1997",y:41},{x:"1998",y:41},{x:"1999",y:37},{x:"2000",y:34},{x:"2001",y:35},{x:"2002",y:37},{x:"2003",y:43},{x:"2004",y:34},{x:"2005",y:34},{x:"2006",y:35},{x:"2007",y:39},{x:"2008",y:37},{x:"2009",y:35},{x:"2010",y:31},{x:"2011",y:40},{x:"2012",y:37},{x:"2013",y:37},{x:"2014",y:34},{x:"2015",y:36},{x:"2016",y:38},{x:"2017",y:35},{x:"2018",y:34},{x:"2019",y:35},{x:"2020",y:35},{x:"2021",y:29},{x:"2022",y:36},{x:"2023",y:35},{x:"2024",y:27},{x:"2025",y:26}],errorBars:{"(Normalized) Points Needed to Avoid Relegation":[]}}]},years=chartData.datasets[0].data.map(t=>t.x),yPoints=chartData.datasets[0].data.map(t=>t.y),xNums=years.map(t=>parseInt(t,10)),{m:m,c:c}=linearFit(xNums,yPoints),fitY=xNums.map(t=>m*t+c),primaryDataset={label:"(Normalized) Points Needed to Avoid Relegation",data:yPoints,borderColor:"blue",borderWidth:2,pointRadius:2,tension:0},fortyLine={label:"40-point benchmark",data:years.map(()=>40),borderColor:"red",borderWidth:2,pointRadius:0,tension:0,fill:!1,order:0},fitLine={label:"Linear fit",data:fitY,borderColor:"grey",borderWidth:2,borderDash:[6,4],pointRadius:0,tension:0,fill:!1,order:2},vertical1993={label:"Start of Premier League",data:[{x:"1993",y:25},{x:"1993",y:50}],borderColor:"purple",borderWidth:.5,pointRadius:0,tension:0,fill:!1,borderDash:[10,10],order:3},ctx=document.getElementById("pointsChart").getContext("2d");new Chart(ctx,{type:"line",data:{labels:years,datasets:[fortyLine,primaryDataset,fitLine,vertical1993]},options:{responsive:!0,maintainAspectRatio:!1,plugins:{legend:{position:"top"},tooltip:{callbacks:{title:t=>formatSeason(t[0].label)}}},scales:{x:{type:"category",title:{display:!0,text:"Season"},ticks:{callback:function(t){const e=this.getLabelForValue(t);return parseInt(e,10)%5==0?formatSeason(e):""},maxRotation:0,autoSkip:!1}},y:{title:{display:!0,text:"Points"},beginAtZero:!1}},interaction:{mode:"nearest",intersect:!1}}});</script> <script>const cdfPercents=[0,0,0,0,0,0,0,0,0,0,0,2.27,4.55,4.55,6.82,6.82,9.09,9.09,9.09,20.45,38.64,43.18,54.55,61.36,75,81.82,86.36,86.36,88.64,93.18,95.45,100,100,100,100,100,100,100,100,100,100],cdfPremier=[0,0,0,0,0,0,0,0,0,0,0,3.03,6.06,6.06,9.09,9.09,12.12,12.12,12.12,27.27,48.48,54.55,69.7,72.73,81.82,87.88,93.94,93.94,96.97,96.97,96.97,100,100,100,100,100,100,100,100,100,100],cdfData1=cdfPercents.map((t,e)=>({x:e+15,y:t})),cdfData2=cdfPremier.map((t,e)=>({x:e+15,y:t})),cdfCtx=document.getElementById("cdfChart").getContext("2d");new Chart(cdfCtx,{type:"line",data:{datasets:[{label:"Historical CDF",data:cdfData1,stepped:!0,borderWidth:2,pointRadius:0},{label:"Premier League Era CDF",data:cdfData2,stepped:!0,borderWidth:2,borderDash:[6,4],pointRadius:0}]},options:{parsing:!1,maintainAspectRatio:!1,scales:{x:{type:"linear",min:15,max:55,ticks:{stepSize:5},title:{display:!0,text:"Points"}},y:{min:0,max:100,ticks:{callback:t=>t+"%"},title:{display:!0,text:"Cumulative probability (%)"}}},plugins:{legend:{display:!0},tooltip:{callbacks:{label:t=>`P(X \u2264 ${t.parsed.x}) = ${t.parsed.y.toFixed(2)}%`}}},elements:{line:{tension:0}}}});</script>]]></content><author><name></name></author><category term="football"/><category term="mathematics"/><summary type="html"><![CDATA[40 points is the classic benchmark to avoid relegation from the Premier League, is this the right value?]]></summary></entry><entry><title type="html">New space weather modelling suite enables upper atmosphere forecasting</title><link href="https://seanelvidge.github.io/articles/2025/New_space_weather_modelling_suite/" rel="alternate" type="text/html" title="New space weather modelling suite enables upper atmosphere forecasting"/><published>2025-10-08T14:31:00+00:00</published><updated>2025-10-08T14:31:00+00:00</updated><id>https://seanelvidge.github.io/articles/2025/New_space_weather_modelling_suite</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2025/New_space_weather_modelling_suite/"><![CDATA[<p>A pioneering new space weather forecasting modelling suite will enable operational modelling of the upper atmosphere at the Met Office for the first time in a major breakthrough for UK atmospheric science.</p> <p>The Advanced Ensemble Networked Assimilation System (AENeAS) is a new suite of space weather forecasting models available to the Met Office that focuses on how space weather can influence the thermosphere and ionosphere here on Earth.</p> <p>The suite, built at the University of Birmingham, and developed in collaboration with Lancaster University, the Universities of Leeds, Bath and Leicester and the British Antarctic Survey is now running on the Met Office’s new supercomputer.</p> <blockquote> <p>The deployment of this suite at the UK Met Office is the realization of a 10-year vision of SERENE, to build and deliver a state-of-the-art upper atmosphere modelling capability into operational use. The tools will be able to support a wide range of users and ultimately allow people to make informed decisions earlier, being proactive rather than reactive in their response to space weather.</p> </blockquote> <p><strong>Professor Sean Elvidge, Head of Space Environment and Radio Engineering (SERENE) - University of Birmingham</strong></p> <p>Complementing the Met Office’s existing space weather forecasting models, which include predicting the arrival of events from the surface of the Sun, this system introduces new forecasting capability for modelling impacts from space weather on satellites, aviation, communications and services which rely on GNSS.</p> <p>Professor Sean Elvidge, Head of Space Environment and Radio Engineering (SERENE) at the University of Birmingham, and the lead developer of the system said: “The deployment of this suite at the UK Met Office is the realization of a 10-year vision of SERENE, to build and deliver a state-of-the-art upper atmosphere modelling capability into operational use.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/aeneas_day_group-480.webp 480w,/assets/img/aeneas_day_group-800.webp 800w,/assets/img/aeneas_day_group-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/aeneas_day_group.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>“The tools will be able to support a wide range of users and ultimately allow people to make informed decisions earlier, being proactive rather than reactive in their response to space weather.”</p> <p>The new suite means that, for the first time, forecasters at the Met Office Space Weather Operations Centre (MOSWOC) will have access to forecast model output on the impacts of space weather on the ionosphere, as well as enhanced modelling of the thermosphere.</p> <blockquote> <p>Once again, cutting-edge British innovation is making a remarkable difference to our daily lives - this time from way up in the atmosphere. This is a really exciting example of how better understanding of what’s happening in space can protect the tech we all rely on, from GPS on our phones to keeping the power grid working.</p> </blockquote> <p><strong>UK Science Minister Lord Vallance</strong></p> <p>Met Office Space Weather Manager Simon Machin said: “This delivers a world-leading capability that provides greater confidence and forecasting skill than any models currently in operation anywhere else in the world.</p> <p>“This isn’t just about science - it’s about protecting the systems we rely on every day. From aircraft communications to GPS in your phone, space weather can affect us all.”</p> <p>Science Minister Lord Vallance said: “Once again, cutting-edge British innovation is making a remarkable difference to our daily lives - this time from way up in the atmosphere.</p> <p>“This is a really exciting example of how better understanding of what’s happening in space can protect the tech we all rely on, from GPS on our phones to keeping the power grid working.”</p> <p>The new modelling capability will be able to assimilate near real-time data about the current state of the ionosphere and thermosphere, combine with forecasts of solar activity from the Sun to produce accurate and actionable forecasts of the upper atmosphere. This will help enable service providers to take mitigating actions to prevent impacts from space weather where possible.</p> <p>Professor Farideh Honary from Lancaster University said: “We are happy to see our research being translated into a useful product to be used by industry. The research and modelling led by Lancaster is relevant to the aviation industry and in particular to flights using polar routes which are dependent on high frequency communications.</p> <p>“These flights have significantly increased since their initial opening in the 1990s due to their operational advantages such as reduced flight times and fuel consumption, which translate to cost savings and environmental benefits like lower carbon emissions.”</p> <p>More accurate and precise forecast information will help enable service providers to take mitigating actions to prevent impacts from space weather where possible. One example of this is with users of Global Navigation Satellite Systems (GNSS), such as GPS, for positioning or navigational purposes. If they understand that there is likely to be a loss of accuracy in GNSS, they can switch to using other systems.</p> <p>Together, the new modelling suite has been delivered as part of Space Weather Instrumentation, Measurement, Modelling and Risk (SWIMMR) programme, which was funded through the UKRI Strategic Priorities Fund and designed to enhance the UK’s capability for monitoring, modelling and forecasting space weather.</p> <p>Professor Ian McCrea, SWIMMR Programme Lead at STFC RAL Space, said: “These new models represent a significant step forward for the UK’s capacity to model, forecast, and understand key components of our upper atmosphere. By coupling advances in physical modelling with global scale observations, they will enable unparalleled awareness of Earth’s geospatial environment.</p> <p>“The models promise to address a wide range of use cases, ranging from radio communications to the prediction of satellite orbits, and we expect that they will be of great importance to a huge variety of stakeholders. They also provide a clear demonstration of how the collaboration between academics and end users, which SWIMMR has enabled, can benefit members of both communities.”</p>]]></content><author><name></name></author><category term="spaceWeather"/><summary type="html"><![CDATA[New suite of space weather forecasting models focuses on how space weather can influence the thermosphere and ionosphere here on Earth.]]></summary></entry><entry><title type="html">The Trend in Taylor Swift’s Mood</title><link href="https://seanelvidge.github.io/articles/2025/Taylor-Swifts-Mood/" rel="alternate" type="text/html" title="The Trend in Taylor Swift’s Mood"/><published>2025-10-04T12:00:00+00:00</published><updated>2025-10-04T12:00:00+00:00</updated><id>https://seanelvidge.github.io/articles/2025/Taylor-Swifts-Mood</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2025/Taylor-Swifts-Mood/"><![CDATA[<p>Taylor Swift doesn’t just release albums, she releases chapters of her life. From the optimism of <em>Fearless</em> to the bite of <em>Reputation</em>, from the hushed poetry of <em>folklore</em> to the midnight reflections of, well, <em>Midnights</em>. Each album is often thought of as a snapshot of where she was at that moment in time.</p> <p>But can we actually see that story in the music itself? I thought I’d try.</p> <p>The chart below shows an experiment: every dot is a Taylor Swift song, plotted by its “average pitch”. The black line is the overall trend album by album, a kind of data-driven glimpse into her changing musical mood.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/TaylorSwiftMood-480.webp 480w,/assets/img/TaylorSwiftMood-800.webp 800w,/assets/img/TaylorSwiftMood-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/TaylorSwiftMood.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here’s how it works. Every song can be broken down into thousands of tiny audio frames, and we can estimate the pitch of each. You could then just average all those frames/estimated pitches together, but then you end up something pretty meaningless: a whispered aside would count the same as a belted chorus.</p> <p>Instead, I weighted each frame by its ‘loudness’. Big, bold notes dominate, quiet moments barely move the needle. Mathematically the “weighted average pitch” looks like this:</p> \[M = \frac{\sum_{i=1}^{N}w_im_i}{\sum_{i=1}^{N}w_i},\] <p>where \(m_i\) is the pitch of a frame (in MIDI numbers) and \(w_i\) is how loud it was. Here we do this in “MIDI” because it is a musical scale. Every step is a semitone. If we used raw frequency in Hertz the maths would skew towards the low notes in a way that doesn’t really match how we actually hear music.</p> <p>So, if you look at our plot, focusing on the smoothed black line, a story starts to appear:</p> <ul> <li>Early albums like <em>Fearless</em> are higher up the stave, matching the wide-eyed optimism of youth.</li> <li><em>1989</em> soars with big pop anthems.</li> <li>Then comes the brooding dip of <em>Reputation</em>, the fallout years.</li> <li>During lockdown, <em>folklore</em> and <em>evermore</em> settle into quieter, lower ground.</li> <li><em>Midnights</em> stays low, steeped in melancholy.</li> <li>And then <em>The Tortured Poets Department</em> plunges deepest of all.</li> <li>Then finally with <em>The Life of a Showgirl</em>, the line bends upward again (significantly). Happy times have returned.</li> </ul> <p>Obviously this is a very simplistic look at the album, and its emotional impact: lyrics, harmony, production, they all matter. But still, there’s something striking about seeing the arc of her career emerge from the raw numbers. By weighting the most powerful sung moments, we get a sense of where the “centre of gravity” of each album lies.</p> <p>And it turns out that when Taylor sings her life, the data sings it back.</p> <hr/> <p>Note: If you would like to try this yourself (inc. for other artists) <a href="/assets/code/taylor_swift.py">here is the link</a>, to the code I used to make this post. You just need to pass a folder of music files to the code and it will work out the rest for you.</p>]]></content><author><name></name></author><category term="mathematics"/><category term="code"/><summary type="html"><![CDATA[Taylor Swift doesn't just release albums, she releases chapters of her life, but can we see that story in the music itself?]]></summary></entry><entry><title type="html">Pour Over Brewing Recipe Generator</title><link href="https://seanelvidge.github.io/articles/2025/Pour_over_brewing_recipe_generator/" rel="alternate" type="text/html" title="Pour Over Brewing Recipe Generator"/><published>2025-03-31T18:41:00+00:00</published><updated>2025-03-31T18:41:00+00:00</updated><id>https://seanelvidge.github.io/articles/2025/Pour_over_brewing_recipe_generator</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2025/Pour_over_brewing_recipe_generator/"><![CDATA[<p>I recently purchased a <a href="https://amzn.to/4j7ni2U">Fellow Aiden Precision Coffee Maker</a> which is a pour over coffee brewer. Able to brew individual cups to whole batches this machine is fantastic, and gives you complete control of the recipe it uses. From the bloom time and temperature to the number of pulses and the ability to control the temperature of each individual pulse.</p> <p>There are a number of default reciepes included and through the <a href="https://fellowproducts.com/pages/fellow-drops">Fellow Drops</a> programme you can get a number of tuned recipes for those coffees. Whilst, at the end of the day, coffee all coems down to personal preference, and any provided reciepe will need fine tuning, there are some fundamentals which provide a good starting point. To try and capture that information I have made the following recipe generator:</p> <p><a href="https://seanelvidge.com/brewcoffee">seanelvidge.com/brewcoffee</a></p> <p>If you visit that page, with the details of the coffee bean you have, you can have a look at what the reciepe generator says you should use. If you would like (a lot) more detail of the decisions that I have made to create the generator then read on.</p> <h1 id="roast-level-adjustments">Roast Level Adjustments</h1> <p>Roast level significantly influences extraction dynamics. Light roasts, dense and less porous, require extended blooming (initial wetting) and multiple pulses of hot water at precise intervals to fully unlock their flavors. Consequently, the recipe defaults for light roasts feature higher bloom ratios and longer bloom times, ensuring sufficient CO₂ release for consistent extraction.</p> <p>Conversely, dark roasts, due to their brittleness and faster extraction tendency, need lower extraction ratios and fewer pulses to avoid bitterness. The recipe parameters for darker roasts emphasize shorter bloom times and higher initial bloom temperatures to develop complexity without over-extraction.</p> <h3 id="specifics">Specifics</h3> <p>Light Roast:</p> <ul> <li>Brew Ratio: Increased to 17 for lighter body.</li> <li>Bloom Ratio: Increased to 3.5 to aid degassing.</li> <li>Bloom Time: Extended to 60 seconds for thorough extraction.</li> <li>Pulse Count: Increased to 6, ensuring complete flavor extraction.</li> </ul> <p>Dark Roast:</p> <ul> <li>Brew Ratio: Reduced to 15 to intensify body and reduce bitterness.</li> <li>Bloom Ratio: Decreased to 1.5 due to easier extraction.</li> <li>Bloom Time: Shortened to 40 seconds to prevent over-extraction.</li> <li>Pulse Count: Reduced to 3, limiting extraction bitterness.</li> </ul> <h1 id="bean-origin-influence">Bean Origin Influence</h1> <p>Beans from different regions exhibit unique physical traits influencing their extraction behavior. East African coffees, typically denser with higher solubility, require gentler bloom phases—lower ratios, shorter times, and reduced temperatures—to mitigate over-extraction and accentuate their bright, vibrant acidity.</p> <p>Latin American coffees often demand a longer bloom and higher temperatures to thoroughly degas and enhance sweetness and body. Indonesian coffees, known for their robust character, are best extracted with moderate bloom parameters that balance intensity with clarity.</p> <h3 id="specifics-1">Specifics</h3> <p>East African (Ethiopia, Kenya, etc.):</p> <ul> <li>Bloom Ratio: Decreased by 0.5 to minimize over-extraction.</li> <li>Bloom Time: Shortened by 5 seconds.</li> <li>Bloom Temperature: Reduced by 3°C.</li> <li>Pulse Count: Decreased by 1 to control acidity.</li> </ul> <p>Latin American (Brazil, Colombia, Guatemala):</p> <ul> <li>Bloom Ratio: Increased by 0.5 for effective degassing.</li> <li>Bloom Time: Increased by 5 seconds.</li> <li>Bloom Temperature: Raised by 3°C.</li> <li>Pulse Count: Increased by 1 to promote even extraction.</li> </ul> <p>Indonesian (Sumatra, Java):</p> <ul> <li>Bloom Ratio set to 2.5, Bloom Time standardized at 50 seconds.</li> <li>Pulse Count: Increased by 1 to maximize extraction clarity.</li> </ul> <h1 id="altitude-considerations">Altitude Considerations</h1> <p>High-altitude coffees, denser due to slower maturation, need more aggressive extraction parameters—finer grind, stronger brew ratios, and longer bloom durations—to achieve optimal flavor development. In contrast, lower-altitude beans benefit from coarser grinds and gentler extraction processes to preserve balance and avoid bitterness.</p> <h3 id="specifics-2">Specifics</h3> <p>High Altitude (&gt;1500m):</p> <ul> <li>Brew Ratio: Reduced by 0.5 for stronger extraction.</li> <li>Bloom Ratio: Increased by 0.5 to assist degassing.</li> <li>Bloom Time: Increased by 5 seconds.</li> <li>Grind Size: Finer by 2 increments.</li> </ul> <p>Low Altitude (&lt;1200m):</p> <ul> <li>Brew Ratio: Increased by 0.5 for gentler extraction.</li> <li>Bloom Ratio: Decreased by 0.5.</li> <li>Bloom Time: Reduced by 5 seconds.</li> <li>Grind Size: Coarser by 2 increments.</li> </ul> <h1 id="processing-method-adjustments">Processing Method Adjustments</h1> <p>Processing significantly impacts bean composition. Natural, honey, and anaerobic coffees, rich in sugars, require longer bloom periods and additional pulses to evenly extract these complex flavors. Washed coffees, typically cleaner and less dense, are best brewed with shorter blooms and fewer pulses, maintaining clarity and brightness.</p> <h3 id="specifics-3">Specifics</h3> <p>Natural, Honey, Carbonic, Anaerobic:</p> <ul> <li>Bloom Ratio: Increased by 0.5 (minimum of 2.5).</li> <li>Bloom Time: Extended by 5 seconds (minimum of 45 seconds).</li> <li>Pulse Count: Increased by 1 to control extraction consistency.</li> </ul> <p>Washed, Double Fermentation, Wet-Hulled:</p> <ul> <li>Bloom Ratio: Decreased by 0.5.</li> <li>Bloom Time: Reduced by 5 seconds.</li> <li>Pulse Count: Decreased by 1.</li> </ul> <h1 id="freshness-factor">Freshness Factor</h1> <p>The age of roasted coffee profoundly affects extraction due to CO₂ levels. Freshly roasted beans (0-7 days) release considerable CO₂, necessitating increased bloom water ratios, longer bloom times, and finer grinds to ensure thorough extraction without bitterness. Conversely, older beans (&gt;20 days) benefit from shorter blooms, lower temperatures, and coarser grinds to avoid over-extraction and maintain desirable flavors.</p> <h3 id="specifics-4">Specifics</h3> <p>Fresh Coffee (0–7 days):</p> <ul> <li>Bloom Ratio: Increased by 0.5.</li> <li>Bloom Time: Extended by 5 seconds.</li> <li>Grind Size: Finer by 4 increments.</li> <li>Pulse temperatures decreased by 1°C per pulse to counter CO₂ resistance.</li> </ul> <p>Older Coffee (&gt;20 days):</p> <ul> <li>Bloom Ratio: Decreased by 0.5.</li> <li>Bloom Time: Reduced by 5 seconds.</li> <li>Grind Size: Coarser by 4 increments.</li> <li>Pulse temperatures increased by 1°C per pulse to improve extraction.</li> </ul> <h1 id="flavor-profile-targeting">Flavor Profile Targeting</h1> <p>Achieving specific tasting notes requires fine-tuning extraction parameters:</p> <ul> <li>Fruity and acidic notes: Higher temperatures and finer grinds enhance bright flavors.</li> <li>Nutty and chocolatey notes: Richer bodies are cultivated through lower brew ratios and coarser grinds.</li> <li>Floral and herbal notes: Delicate aromatics emerge clearly with increased bloom ratios.</li> <li>Sweet and heavy notes: Lower bloom ratios preserve syrupy, sweet profiles.</li> <li>Creamy textures: Lower temperatures ensure smooth, rounded mouthfeels.</li> </ul> <h3 id="specifics-5">Specifics</h3> <p>Fruity/Acidic Notes:</p> <ul> <li>Brew Ratio: Increased by 0.5 for brightness.</li> <li>Bloom Temperature: Raised by 2°C.</li> <li>Grind Size: Finer by 4 increments.</li> </ul> <p>Nutty/Chocolate Notes:</p> <ul> <li>Brew Ratio: Decreased by 0.5 for richer body.</li> <li>Grind Size: Coarser by 2 increments.</li> </ul> <p>Floral/Herbal Notes:</p> <ul> <li>Bloom Ratio: Increased by 0.5 for aromatic extraction.</li> <li>Grind Size: Coarser by 2 increments.</li> </ul> <p>Heavy Sweet Notes:</p> <ul> <li>Bloom Ratio: Reduced by 0.5 for syrupy texture.</li> <li>Grind Size: Finer by 2 increments.</li> </ul> <p>Creamy Notes:</p> <ul> <li>Bloom Temperature: Reduced by 2°C to preserve smoothness.</li> </ul> <h1 id="pulse-temperature-profiles">Pulse Temperature Profiles</h1> <p>Dynamic pulse temperatures optimize extraction across roast levels:</p> <ul> <li>Light roasts gradually increase temperature across pulses, enhancing complexity.</li> <li>Medium roasts maintain stable temperatures for balanced extraction.</li> <li>Dark roasts reduce temperatures progressively to avoid bitterness, enhancing depth.</li> </ul> <p>Final pulse temperatures further adjust to emphasize specific tasting notes—higher initial temperatures highlight bright, fruity notes, while lower final temperatures preserve sweetness and depth.</p> <h3 id="specifics-6">Specifics</h3> <p>Light Roasts: Gradually increase temperature from 90°C to 96°C.</p> <p>Medium Roasts: Maintain stable bloom temperature; adjust slightly for anaerobic or carbonic processes.</p> <p>Dark Roasts: Decrease temperature progressively from 91°C to 85°C to minimize bitterness.</p> <h1 id="conclusion">Conclusion</h1> <p>The <a href="https://amzn.to/4j7ni2U">Fellow Aiden Precision Coffee Maker</a> is a great coffee brewer, giving you almost complete control of the brewing process. You can access the recipe generator here:</p> <p><a href="https://seanelvidge.com/brewcoffee">seanelvidge.com/brewcoffee</a></p>]]></content><author><name></name></author><category term="misc"/><summary type="html"><![CDATA[Generate pour over brewing coffee reciepes for a given bean]]></summary></entry><entry><title type="html">Waning Home Advantage in English League Football</title><link href="https://seanelvidge.github.io/articles/2025/Home_advantage_in_English_football/" rel="alternate" type="text/html" title="Waning Home Advantage in English League Football"/><published>2025-01-13T09:00:00+00:00</published><updated>2025-01-13T09:00:00+00:00</updated><id>https://seanelvidge.github.io/articles/2025/Home_advantage_in_English_football</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2025/Home_advantage_in_English_football/"><![CDATA[<p>For generations, “home advantage” has been a tenet of sports. The roar of the crowd, the familiarity of the pitch, the comfort of the home dressing room – all give the home team an edge. This is particularly true in English football. However, a look at the data spanning over a century of English football reveals an interesting trend: the home advantage is shrinking.</p> <p>Using my database of all English league results since 1888 (available <a href="https://github.com/seanelvidge/England-football-results">here</a>, and described <a href="https://seanelvidge.com/articles/2024/All_England_football_league_results/">here</a>) we can track the Home Win %, Draw %, and Away Win % from 1888 to the present day (at the time of writing, that is halfway through the 2024/2025 season, but the charts in this post should automatically update). The results are pretty clear:</p> <html> <div class="chart-container"> <canvas id="resultsChart"></canvas> </div> <img id="fallbackImage1" src="assets/img/home_advantage_trend_fallback.png" alt="Fallback image for home, draw, away % win" style="display: none; max-width: 100%;"/> <style>.chart-container{position:relative;width:100%;min-height:250px;height:50vh;max-height:80vh}</style> </html> <p>In the late 19th and early 20th centuries, home teams were dominant, boasting win percentages well above 60%. Away wins were a relative rarity, hovering around 20%. But as the decades have progressed, the lines have converged. Home win percentages have steadily declined, dipping towards 40% in recent years, while away wins have climbed, now consistently above 30% and with a clear upwards trajectory.</p> <p>It would be easy to assume that this trend is confined to the top tier of English football, where the largest investments in training facilities, player recruitment, and tactical analysis occur. However, if we look at the home win percentages across all four tiers of English football (where “Tier 1” is the highest tier, currently the Premier League and “Tier 4” is the lowest, EFL League Two) we can see that this is a league-wide phenomenon.</p> <html> <div class="chart-container"> <canvas id="divisionChart"></canvas> </div> <img id="fallbackImage2" src="assets/img/home_advantage_trend_fallback_by_division.png" alt="Fallback image for home win % by division" style="display: none; max-width: 100%;"/> </html> <p>As you can see, the decline in home win percentages is remarkably consistent across all levels of professional English football. While there are some minor variations between divisions, the overall trajectory is the same: downwards. This suggests that the factors driving the change are not unique to the top tier but are systemic throughout the entire football pyramid. Each division shows high home win % at the start of the time series (above 50%), but each division ends up closer to 40% by the end of the series.</p> <p>So, what’s driving this shift away from home dominance? Several factors are likely at play:</p> <ul> <li>Standardization of Playing Conditions: In the early days, pitch conditions varied wildly. Home teams were intimately familiar with their own, often quirky, pitches, giving them a distinct advantage. Over time, regulations and advancements in groundskeeping have led to more standardized, high-quality pitches across the league, leveling the playing field. This impact would be felt across all divisions.</li> <li>Improved Travel and Accommodation: Early football often involved arduous journeys for away teams, leaving them fatigued and less prepared. Modern transportation and improved accommodations have minimized the physical toll of travel, allowing away teams to arrive rested and ready to compete. Again, this is relevant to all levels of the game.</li> <li>Tactical Advancements and Analysis: The modern game is far more tactically sophisticated. Managers and analysts have access to vast amounts of data and video footage, allowing them to dissect opponents’ strengths and weaknesses, regardless of location. Away teams can now better prepare for the specific challenges posed by their opponents and their home stadiums. While the resources may be greater at the top, these advancements have filtered down through the divisions.</li> <li>Professionalization and Fitness: Players today are fitter, faster, and more technically skilled than ever before. This overall increase in athleticism can help away teams better cope with the pressures of playing in hostile environments, a trend seen across the footballing spectrum.</li> </ul> <h2 id="the-future-of-home-advantage">The Future of Home Advantage</h2> <p>Whilst home advantage may not be what it once was, it hasn’t disappeared entirely. The support of the home crowd can still provide a boost, and familiarity with the surroundings can offer a slight edge. However, the trend is undeniable. The gap between home and away performance is narrowing, and the English Football League, across all its divisions, is becoming increasingly competitive on all fronts.</p> <p>If we look to extrapolate the trends in the data we can try and estimate what will happen in the future, fitting lines of best fit to the data we can estimate that home advantage will stick around for quite a while yet!</p> <div class="chart-container"> <canvas id="finalTrendChart"></canvas> </div> <html> <script src="https://cdn.jsdelivr.net/npm/papaparse@5.3.0/papaparse.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/chart.js"></script> <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-zoom"></script> <script>function showFallbackImages(){document.getElementById("resultsChart").style.display="none",document.getElementById("divisionChart").style.display="none",document.getElementById("fallbackImage1").style.display="block",document.getElementById("fallbackImage2").style.display="block"}function processData(t){const e={},a={};t.forEach(t=>{if(!t.Season||!t.Result)return;const o=t.Season,s=t.Tier,n=t.Result;e[o]||(e[o]={H:0,D:0,A:0,total:0}),e[o][n]++,e[o].total++,a[o]||(a[o]={}),s&&!a[o][s]&&(a[o][s]={H:0,D:0,A:0,total:0}),s&&(a[o][s][n]++,a[o][s].total++)});const o=Object.keys(e).sort().pop(),s=calculatePercentages(e,o),n=calculateDivisionPercentages(a);createResultsChart(s),createDivisionChart(n),createFinalTrendChart(s)}function calculatePercentages(t,e){const a=Object.keys(t).sort(),o=[],s=[],n=[],l=[],r=[],i=[],c=[];return a.forEach(a=>{const d=t[a],u=d.total,p=parseFloat((d.H/u*100).toFixed(2)),h=parseFloat((d.D/u*100).toFixed(2)),b=parseFloat((d.A/u*100).toFixed(2)),g=calculateStandardDeviation(d.H,u),f=calculateStandardDeviation(d.D,u),C=calculateStandardDeviation(d.A,u);a&&a<=e&&(c.push(a),o.push(p),s.push(h),n.push(b),l.push(g),r.push(f),i.push(C))}),{labels:c,datasets:[{label:"Home Win %",data:o,borderColor:"blue",backgroundColor:"transparent"},{label:"Draw %",data:s,borderColor:"green",backgroundColor:"transparent"},{label:"Away Win %",data:n,borderColor:"red",backgroundColor:"transparent"}]}}function calculateDivisionPercentages(t){const e=Object.keys(t).sort(),a={};return e.forEach(e=>{Object.keys(t[e]).forEach(o=>{if(o&&!a[o]){let t;t="1"===o?"red":"2"===o?"blue":"3"===o?"green":"4"===o?"purple":getRandomColor(),a[o]={label:`Division ${o}`,data:[],borderColor:t,backgroundColor:"transparent",errorBars:[]}}if(o){const s=t[e][o],n=s.total,l=parseFloat((s.H/n*100).toFixed(2));parseFloat((s.D/n*100).toFixed(2)),parseFloat((s.A/n*100).toFixed(2));a[o].data.push({x:e,y:l}),a[o].errorBars.push(calculateStandardDeviation(s.H,n))}})}),{labels:e,datasets:Object.values(a)}}function getRandomColor(){const t="0123456789ABCDEF";let e="#";for(let a=0;a<6;a++)e+=t[Math.floor(16*Math.random())];return e}function calculateStandardDeviation(t,e){const a=t/e;return 100*Math.sqrt(a*(1-a)/e)}function createResultsChart(t){const e=document.getElementById("resultsChart").getContext("2d");resultsChart=new Chart(e,{type:"line",data:t,options:{maintainAspectRatio:!1,plugins:{legend:{labels:{usePointStyle:!0}},zoom:{zoom:{wheel:{enabled:!1},pinch:{enabled:!0},mode:"xy"}},tooltip:{callbacks:{label:function(t){let e=t.dataset.label||"";return e&&(e+=": "),e+=Number(t.parsed.y).toFixed(1)+"%",e},footer:function(t){t[0].dataset}}}},scales:{y:{beginAtZero:!0,title:{display:!0,text:"Percentage",font:{size:16}},ticks:{font:{size:14}}},x:{ticks:{callback:function(t){const e=this.getLabelForValue(t);if(e.endsWith("5")||e.endsWith("0"))return e.substring(0,4)},font:{size:14}},title:{display:!0,text:"Season",font:{size:16}}}}}})}function createDivisionChart(t){const e=document.getElementById("divisionChart").getContext("2d");divisionChart=new Chart(e,{type:"line",data:t,options:{maintainAspectRatio:!1,plugins:{legend:{labels:{usePointStyle:!0}},zoom:{zoom:{wheel:{enabled:!1},pinch:{enabled:!0},mode:"xy"}},tooltip:{callbacks:{label:function(t){let e=t.dataset.label||"";return e&&(e+=": "),e+=Number(t.parsed.y).toFixed(1)+"%",e},footer:function(t){return`\xb1${t[0].dataset.errorBars[t[0].dataIndex].toFixed(2)}%`}}},title:{display:!0,text:"% Home Wins per Division"}},scales:{y:{beginAtZero:!0,title:{display:!0,text:"Percentage",font:{size:16}},ticks:{font:{size:14}}},x:{ticks:{callback:function(t){const e=this.getLabelForValue(t);if(e.endsWith("5")||e.endsWith("0"))return e.substring(0,4)},font:{size:14}},title:{display:!0,text:"Season",font:{size:16}}}}}})}function updateChartTheme(t){const e=t?"#242424":"#FFFFFF",a=t?"#FFFFFF":"#000000";resultsChart.options.plugins.legend.labels.color=a,resultsChart.options.scales.x.title.color=a,resultsChart.options.scales.x.ticks.color=a,resultsChart.options.scales.y.title.color=a,resultsChart.options.scales.y.ticks.color=a,resultsChart.options.backgroundColor=e,resultsChart.update(),divisionChart.options.plugins.legend.labels.color=a,divisionChart.options.scales.x.title.color=a,divisionChart.options.scales.x.ticks.color=a,divisionChart.options.scales.y.title.color=a,divisionChart.options.scales.y.ticks.color=a,divisionChart.options.backgroundColor=e,divisionChart.update()}const resultsUrl="https://raw.githubusercontent.com/seanelvidge/England-football-results/main/EnglandLeagueResults.csv";let resultsChart,divisionChart;Papa.parse(resultsUrl,{download:!0,header:!0,dynamicTyping:!0,skipEmptyLines:!0,complete:function(t){if(!t||!t.data||0===t.data.length)return console.warn("No valid data returned. Showing fallback images."),void showFallbackImages();if(t.errors&&t.errors.length>0)return console.warn("Papa Parse encountered errors. Showing fallback images:",t.errors),void showFallbackImages();processData(t.data)},error:function(t){console.error("Error loading remote data",t),showFallbackImages()}});</script> <script>function linearRegression(e,t){const a=e.length;let r=0,n=0,l=0,o=0;for(let s=0;s<a;s++)r+=e[s],n+=t[s],l+=e[s]*t[s],o+=e[s]*e[s];const s=(a*l-r*n)/(a*o-r*r);return{slope:s,intercept:(n-s*r)/a}}function createFinalTrendChart(e){function t(e,t){const a=parseInt(e.substring(0,4),10)+t;return`${a}-${a+1}`}const a=e.labels,r=a.map((e,t)=>t),n=e.datasets[0].data,l=e.datasets[1].data,o=e.datasets[2].data,s=linearRegression(r,n),d=linearRegression(r,l),i=linearRegression(r,o),c=r[r.length-1],b=[];for(let e=0;e<=c+50;e++)b.push(e);const u=b.map(e=>{if(e<=c)return a[e];{const r=e-c;return t(a[c],r)}}),p=[],g=[],h=[];for(let e=0;e<b.length;e++){const t=b[e],a=s.slope*t+s.intercept,r=d.slope*t+d.intercept,n=i.slope*t+i.intercept,l=a+r+n,o=a/l*100,c=r/l*100,u=n/l*100;p.push(o),g.push(c),h.push(u)}const f={labels:u,datasets:[{label:"Home Win %",data:e.datasets[0].data.concat(Array(b.length-r.length).fill(null)),borderColor:"blue",backgroundColor:"transparent"},{label:"Draw %",data:e.datasets[1].data.concat(Array(b.length-r.length).fill(null)),borderColor:"green",backgroundColor:"transparent"},{label:"Away Win %",data:e.datasets[2].data.concat(Array(b.length-r.length).fill(null)),borderColor:"red",backgroundColor:"transparent"},{label:"Home Trend (Normalized)",data:p,borderColor:"blue",backgroundColor:"transparent",borderDash:[5,5],pointRadius:0},{label:"Draw Trend (Normalized)",data:g,borderColor:"green",backgroundColor:"transparent",borderDash:[5,5],pointRadius:0},{label:"Away Trend (Normalized)",data:h,borderColor:"red",backgroundColor:"transparent",borderDash:[5,5],pointRadius:0}]},C=document.getElementById("finalTrendChart").getContext("2d");finalTrendChart=new Chart(C,{type:"line",data:f,options:{maintainAspectRatio:!1,scales:{y:{beginAtZero:!0,title:{display:!0,text:"Percentage",font:{size:16}},ticks:{font:{size:14}}},x:{ticks:{font:{size:14},callback:function(e,t){return t%5==0?this.getLabelForValue(e):""}},title:{display:!0,text:"Season",font:{size:16}}}},plugins:{title:{display:!1,text:"Home/Draw/Away % with Normalized Trend to 100%"},tooltip:{callbacks:{label:function(e){let t=e.dataset.label||"";return t&&(t+=": "),t+=Number(e.parsed.y).toFixed(1)+"%",t}}},legend:{labels:{usePointStyle:!0}},zoom:{zoom:{wheel:{enabled:!1},pinch:{enabled:!0},mode:"xy"}}}}})}let finalTrendChart;</script> </html>]]></content><author><name></name></author><category term="football"/><summary type="html"><![CDATA[An investigation in the trend of home advantage in English football]]></summary></entry><entry><title type="html">Brewing the Perfect Coffee at Altitude</title><link href="https://seanelvidge.github.io/articles/2025/Brewing_coffee_at_altitude/" rel="alternate" type="text/html" title="Brewing the Perfect Coffee at Altitude"/><published>2025-01-09T17:06:00+00:00</published><updated>2025-01-09T17:06:00+00:00</updated><id>https://seanelvidge.github.io/articles/2025/Brewing_coffee_at_altitude</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2025/Brewing_coffee_at_altitude/"><![CDATA[<p>For those of us who consider coffee an essential part of our daily routine, the pursuit of the perfect cup is a never-ending quest. But did you know that your altitude can significantly impact your brew? This post is inspired by my new <a href="https://amzn.to/4j7ni2U">Fellows Aiden Precision Coffee Maker</a> which, on one of the first set up screens asks:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fellows_coffee-480.webp 480w,/assets/img/fellows_coffee-800.webp 800w,/assets/img/fellows_coffee-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/fellows_coffee.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </figure> </div> </div> <p>Perhaps that seems like an odd question, buf if you’ve ever tried recreating your favourite <a href="https://maps.app.goo.gl/8kBMHJEUhXWF1KvL9">Birmingham</a> coffee experience in <a href="https://maps.app.goo.gl/rfqUP96D2L4juCtT9">Boulder, Colorado</a> (which I have, many times!), you might have noticed something’s a bit “off”. Let’s dive into the science behind why:</p> <p>It all boils down to atmospheric pressure. The air around us exerts pressure, and this pressure decreases as we go higher in altitude. Why? Essentially, there’s less atmosphere above you pushing down.</p> <p>Water boiling occurs when the vapor pressure of the water (or really any liquid) equals the surrounding atmospheric pressure. So these changes in atmospheric pressure directly affects the boiling point of water. Lower atmospheric pressure at higher altitudes means water boils at a lower temperature.</p> <p>The relationship between pressure (P) and boiling point (T) can be approximated using the <a href="https://en.wikipedia.org/wiki/Clausius%E2%80%93Clapeyron_relation">Clausius-Clapeyron equation</a>:</p> \[\ln\left(\frac{P}{P_0}\right) = -\frac{\Delta H}{R}\left(\frac{1}{T} - \frac{1}{T_0}\right)\] <p>Where:</p> <ul> <li>\(P\): The vapor pressure of the liquid at the temperature of interest (in Pascals, Pa). This is what we want to find to determine the boiling point.</li> <li>\(P_0\): The vapor pressure at a known reference temperature (also in Pascals). For water, we often use standard atmospheric pressure (101325 Pa) and its corresponding boiling point of 100°C (373.15 Kelvin).</li> <li>\(\Delta H\): The enthalpy of vaporization (in Joules per mole, J/mol). This represents the energy needed to change one mole of liquid into vapor at a constant temperature. For water, \(\Delta H\) is approximately 40,700 J/mol.</li> <li>\(R\): The ideal gas constant (8.314 J/mol·K). This constant relates energy to temperature for gases.</li> <li>\(T\): The temperature in Kelvin (K) at which we want to find the vapor pressure (and ultimately, the boiling point).</li> <li>\(T_0\): The reference temperature in Kelvin (K). Again, for water, this is often 373.15 K.</li> </ul> <h2 id="birmingham-vs-boulder-a-tale-of-two-cities">Birmingham vs. Boulder: A Tale of Two Cities</h2> <p>Birmingham, UK, sits at a relatively low altitude (around 140 meters above sea level). Water here boils pretty close to 100°C. Boulder, Colorado, on the other hand, is nestled in the foothills of the Rocky Mountains at an elevation of roughly 1655 meters. This significant difference in altitude has a significant impact on the temperature water boils at.</p> <p>To use the Clausius-Clapeyron equation we first need to calculate atmospheric pressure in Boulder - there are various ways of doing this (getting progressively more difficult) but here it is sufficient to assume a ‘standard atmosphere’ where temperature decreasesd with altitude, and assume that our coffee machine is being used indoors where the temperature is about 20°C, then we can use:</p> \[P = P_0\cdot\exp\left(\frac{-gM(h - h_0)}{RT}\right)\] <p>Where:</p> <ul> <li>\(P\): The air pressure (Pa) at altitude \(h\) (this is the thing we want)</li> <li>\(P_0\): Air pressure at reference altitude \(h_0\) (as in the previous equation we will use sea level pressure of 101325 Pa)</li> <li>\(g\): Acceleration due to gravity (9.81 m/s²)</li> <li>\(M\): Molar mass of air (0.0289644 kg/mol)</li> <li>\(h\): Altitude (m) (Boulder is at 1,655m)</li> <li>\(h_0\): Reference altitude (m) (here we assume sea level, 0 m)</li> <li>\(R\): The universal gas constant (8.31432 J/(mol·K))</li> <li>\(T\): Temperature at altitude \(h\) (K) (must be in Kelvin, and so we use 293.15 K)</li> </ul> <p>Plugging those numbers in gives us:</p> \[\begin{eqnarray*} P &amp;=&amp; 101325\times\exp\left(\frac{-9.81\times 0.0289644\times (1655 - 0)}{8.31432\times 293.15}\right)\\ &amp;=&amp; 83546 \end{eqnarray*}\] <p>So the atmospheric pressure of Boulder is about 83500 Pa (you can also use this <a href="https://www.mide.com/air-pressure-at-altitude-calculator">online calculator</a> to be more accurate if you want to be, and this also provides more details on air pressure caclulations). Sub this value in (as \(P\)) into the Clausius-Clapeyroin equation and using our other reference values: \(P_0\) (101325 Pa), \(\Delta H\) (40700 J/mol), \(R\) (8.314 J/mol·K), and \(T_0\) (373.15 K) to get:</p> \[\ln\left(\frac{83500}{101325}\right) = -\left(\frac{40700}{8.314}\right)\times\left(\frac{1}{T} - \frac{1}{373.15}\right)\] <p>Solving the above equation for \(T\) requires a little bit of algebraic manipulation but you should end up with \(T \approx 367.7 K\). Which (by removing 273.15) gives a value of 94.55°C for the boiling point of water for Boulder.</p> <p><strong>Water boils at approximately 94.5°C in Boulder.</strong></p> <h2 id="why-it-matters-for-coffee">Why It Matters for Coffee</h2> <p>Coffee brewing is a delicate dance of temperature and extraction. Water acts as a solvent, pulling flavorful compounds from the coffee grounds. The ideal brewing temperature for most coffee lies between 90-96°C. But brewing in Boulder presents a unique challenge. With water boiling at a lower temperature, you have a smaller window to extract those desirable flavors before under-extraction occurs. This can result in a sour or weak cup of coffee.</p> <p>My tips for brewing at higher altitudes (but I am by no means an expert!) are:</p> <ul> <li>Grind finer: A finer grind increases the surface area of the coffee, allowing for better extraction at lower temperatures.</li> <li>Pre-infusion: Bloom your grounds with a small amount of hot water before brewing. This helps saturate the coffee evenly.</li> <li>Extend brew time: Slightly longer brew times can help compensate for the lower temperature.</li> </ul>]]></content><author><name></name></author><category term="misc"/><category term="mathematics"/><summary type="html"><![CDATA[Why your coffee tastes different in Boulder, CO compared to Birmingham, UK.]]></summary></entry><entry><title type="html">All England football league results</title><link href="https://seanelvidge.github.io/articles/2024/All_England_football_league_results/" rel="alternate" type="text/html" title="All England football league results"/><published>2024-12-28T10:09:00+00:00</published><updated>2024-12-28T10:09:00+00:00</updated><id>https://seanelvidge.github.io/articles/2024/All_England_football_league_results</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2024/All_England_football_league_results/"><![CDATA[<p>This article describes a plain text database of all England football (soccer) league results from 1888 to the present day (covering over 209,000 matches).</p> <p>You can access the latest database on its dedicated github page: <a href="https://github.com/seanelvidge/England-football-results/tree/main">England-football-results</a></p> <p>The database is updated roughly every two days (although I am looking for approaches to speed this up) for the top four tiers in England: Premier League, EFL Championship, EFL League One and EFL League Two. The motivation for making the database is that I do a lot of statistical analysis on various bits and pieces in football (you can see some <a href="https://seanelvidge.com/articles/tag/football/">here</a>), and not having an easy to read database really slows me down.</p> <p>The <a href="https://github.com/seanelvidge/England-football-results/tree/main">database</a> is a comma (“,”) delimited csv file with the following columns:</p> <table> <thead> <tr> <th>Column</th> <th>Details</th> </tr> </thead> <tbody> <tr> <td>Date</td> <td>the day of the match (string; format “YYYY-MM-DD”)</td> </tr> <tr> <td>Season</td> <td>the season the match took place in (string; format “YYYY/YYYY”)</td> </tr> <tr> <td>HomeTeam</td> <td>the home team name (string)</td> </tr> <tr> <td>AwayTeam</td> <td>the away team name (string)</td> </tr> <tr> <td>Score</td> <td>the final score (string; format “X-Z”)</td> </tr> <tr> <td>hGoal</td> <td>number of goals scored by the home team (integer; “X” from the “Score” column)</td> </tr> <tr> <td>aGoal</td> <td>number of goals scored by the away team (integer; “Z” from the “Score” column)</td> </tr> <tr> <td>Division</td> <td>name of the division the match was played in (string)</td> </tr> <tr> <td>Tier</td> <td>numerical representation of the tier which the match was from: 1, 2, 3 or 4, where “1” is the top tier (currently the Premier League) (integer)</td> </tr> <tr> <td>Result</td> <td>the result “H” (home win), “A” (away win), “D” (draw) (string)</td> </tr> </tbody> </table> <p>The data from 1888 to 2016 is based on that from: James P. Curley (2016). engsoccerdata: English Soccer Data 1871-2016. http://dx.doi.org/10.5281/zenodo.13158</p> <p>Such a long database of results leads to some confusion around team names, the answer to the most common set of questions I have received in terms of team names:</p> <ul> <li><a href="https://en.wikipedia.org/wiki/Accrington_F.C.">Accrington F.C.</a> is a different team to <a href="https://en.wikipedia.org/wiki/Accrington_Stanley_F.C.">Accrington Stanley</a>. Acrrington F.C. were one of the founder members of the Football League, but unfortunately were dissolved in 1896.</li> <li><a href="https://en.wikipedia.org/wiki/Brighton_%26_Hove_Albion_F.C.">Brighton &amp; Hove Albion</a>, <a href="https://en.wikipedia.org/wiki/New_Brighton_Tower_F.C.">New Brighton Tower</a> and <a href="https://en.wikipedia.org/wiki/New_Brighton_A.F.C.">New Brighton</a> are all different clubs. New Brighton Tower were in existence from 1896-1901 and whilst Brighton &amp; Hove Albion were formed in 1901, the “spiritual” successor to New Brighton Tower, was New Brighton (1921-1983 and 1993-2012; originally formed by the relocation of <a href="https://en.wikipedia.org/wiki/South_Liverpool_F.C._(1890s)">South Liverpool</a>)</li> <li>Burton <a href="https://en.wikipedia.org/wiki/Burton_Swifts_F.C.">Swifts</a>, <a href="https://en.wikipedia.org/wiki/Burton_Wanderers_F.C.">Wanderers</a>, <a href="https://en.wikipedia.org/wiki/Burton_United_F.C.">United</a>, <a href="https://en.wikipedia.org/wiki/Burton_Town_F.C.">Town</a> and <a href="https://en.wikipedia.org/wiki/Burton_Albion_F.C.">Albion</a> are all different teams. Burton Swifts joined with Wanderers to form Burton United in 1901, which in 1924 merged with Burton Town and in 1950 merged with the newly formed Burton Albion.</li> <li>Whilst <a href="https://en.wikipedia.org/wiki/Leeds_United_F.C.">Leeds Unitd</a> were formed following/replacing <a href="https://en.wikipedia.org/wiki/Leeds_City_F.C.">Leeds City</a> (and played in the same ground). No players or management from Leeds City moved to Leeds United so we treat them as separate football clubs.</li> <li><a href="https://en.wikipedia.org/wiki/Middlesbrough_Ironopolis_F.C.">Middlesbrough Ironopolis</a> (1889-1894) is separate team from <a href="https://en.wikipedia.org/wiki/Middlesbrough_F.C.">Middlesbrough</a> (1876-).</li> <li><a href="https://en.wikipedia.org/wiki/Rotherham_County_F.C.">Rotherham County</a> merged with <a href="https://en.wikipedia.org/wiki/Rotherham_Town_F.C._(1899)">Rotherham Town</a> in 1925 to form <a href="https://en.wikipedia.org/wiki/Rotherham_United_F.C.">Rotherham United</a>.</li> <li><a href="https://en.wikipedia.org/wiki/Wigan_Athletic_F.C.">Wigan Athletic</a> were formed (1932) a year after <a href="https://en.wikipedia.org/wiki/Wigan_Borough_F.C.">Wigan Borough</a> were wound up (1931) and we treat them separately. Wigan Athletic was the sixth attempt to create a stable football club in Wigan following the dissolving of Wigan A.F.C., <a href="https://en.wikipedia.org/wiki/Wigan_County_F.C.">County</a> (1897-1900), <a href="https://en.wikipedia.org/wiki/Wigan_United_A.F.C.">United</a> (1896-1914), <a href="https://en.wikipedia.org/wiki/Wigan_Town_A.F.C.">Town</a> (1905-1908) and <a href="https://en.wikipedia.org/wiki/Wigan_Borough_F.C.">Borough</a> (1920-1931).</li> </ul> <p>Hopefully there are lots of fun things you can do with this, please let me know about any of them! A couple of simple examples:</p> <ul> <li>You can use the form on this site to work out a league table for any given season and division or for any arbitrary date range, from 1888 to present (remembering before 1981 there was only <a href="https://en.wikipedia.org/wiki/Three_points_for_a_win">2 points for a win</a> in English football): <a href="https://seanelvidge.com/leaguetable">https://seanelvidge.com/leaguetable</a></li> <li>To find the Head-to-Head statistics of any two clubs who have played in the English Football League visit: <a href="https://seanelvidge.com/h2h">https://seanelvidge.com/h2h</a> <ul> <li>This url can have the team names passed directly into the url, e.g. to get the head-to-head statistics of Arsenal v Chelsea visit: <a href="https://seanelvidge.com/h2h?team1=Arsenal&amp;team2=Chelsea">https://seanelvidge.com/h2h?team1=Arsenal&amp;team2=Chelsea</a></li> </ul> </li> </ul>]]></content><author><name></name></author><category term="football"/><summary type="html"><![CDATA[A plain text set of all England football (soccer) league results from 1888 to present.]]></summary></entry><entry><title type="html">How aerodynamic is Santa</title><link href="https://seanelvidge.github.io/articles/2024/How_aerodynamic_is_Santa/" rel="alternate" type="text/html" title="How aerodynamic is Santa"/><published>2024-12-19T14:31:00+00:00</published><updated>2024-12-19T14:31:00+00:00</updated><id>https://seanelvidge.github.io/articles/2024/How_aerodynamic_is_Santa</id><content type="html" xml:base="https://seanelvidge.github.io/articles/2024/How_aerodynamic_is_Santa/"><![CDATA[<p>Santa is at the top of multiple engineering disciplines (see his production line for presents and ability to deliver them all). However one that is often overlooked is the space-domain. The United States’ North American Aerospace Defense Command (NORAD) not only continuously tracks 32,000 objects in space to avoid collisions, but also Santa’s global progress in delivering presents on Christmas Eve (https://www.noradsanta.org/en/). From NORAD’s tracks, we can see that he travels at 6.25 km/s (14,000 mph) over oceans (close to the speed of satellites in Low Earth Orbit at 7.7 km/s). However, he travels more slowly, at only 1.4 km/s (3,000 mph) while over the densely populated UK. His route perfectly optimised for reducing atmospheric drag.</p> <p>Whilst travelling over oceans he doesn’t need to be at low altitudes (no presents to deliver) so he takes advantage of the thinner atmosphere at higher altitudes. Also, delivering at night-time allows him to both avoid sleeping children and at the same time take advantage of the lower thermospheric densities, as the thermosphere cools and contracts while the Sun isn’t heating it. However, over populated areas, he travels more slowly in the lower, denser atmosphere, allowing more accurate present delivery while experiencing less heating due to the atmospheric drag.</p> <p>Another optimization Santa has made is in the design of his Sleigh and selection of Reindeer as the method of driving the Sleigh. In the atmospheric drag equation, there is a term known as the coefficient of drag, effectively how well something slips through the atmosphere. Smaller, pointier objects have lower coefficients of drag, think about arrows compared to parachutes. Reindeer have pointy snouts and present a relatively low cross-sectional area for their strength, allowing them to convert their power to speed. Meanwhile having nine reindeer in a 1-2-2-2-2 formation helps to efficiently cut into the air, and make it turbulent before the sleigh travels through, making it experience less drag, just like a cyclist in a peloton. However, Rudolph being at the front takes the brunt of this force due to atmospheric drag, with the majority of the kinetic energy turning into heat energy, leading to his red nose.</p> <p>Whilst we are not sure if Santa has yet upgraded his Sleigh to using the latest SANTA-NAV (GPS) technology, or whether he uses HF communications to keep in touch with the North Pole, we do know that on Christmas Eve, and throughout the Christmas period, the Space Environment research group at the University of Birmingham (SERENE) will be working with space weather centres around the world to help deliver 24/7 forecasting of the conditions in Earth’s upper atmosphere and its impacts on global position systems, HF communications, atmospheric drag and the need for satellite manoeuvres.</p>]]></content><author><name></name></author><category term="spaceWeather"/><category term="misc"/><summary type="html"><![CDATA[Santa is at the top of many engineering disciplines - his production line for presents, for example. However one that is often overlooked is the space-domain (Redirect to University of Birmingham News page)]]></summary></entry></feed>