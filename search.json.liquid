[ { "title": "Pulse Pour-Over Coffee Recipe Generator", "url": "/brewcoffee", "content": "Tool for generating coffee reciepes using a pulsed pour-over method (primarily designed for the Fellow Aiden Precision Coffee Maker). A description of the methodology is here. Name of Coffee: Roasting Level*: Select Roasting Level 1 (Light) 2 (Light-Medium) 3 (Medium) 4 (Medium-Dark) 5 (Dark) Country of Growth: Altitude of Growth (in metres): Processing Method: Select Processing Washed (Wet) Process Natural (Dry) Process Honey Process – White Honey Process – Yellow Honey Process – Red Honey Process – Black Anaerobic Fermentation Carbonic Maceration Wet-Hulled Pulped Natural Double Fermentation Days Since Roasted: Tasting Notes (select all that apply): Fruity Notes Apple Apricot Blackberry Blueberry Cherry Grapefruit Lemon Lime Mango Orange Papaya Passionfruit Peach Pear Pineapple Plum Raspberry Strawberry Nutty \u0026amp; Chocolate Notes Almond Chocolate (Dark) Chocolate (Milk) Cocoa Powder Hazelnut Peanut Walnut Sweet \u0026amp; Caramel Notes Brown Sugar Caramel Honey Maple Syrup Molasses Floral \u0026amp; Herbal Notes Basil Chamomile Hibiscus Jasmine Lavender Mint Rose Thyme Other Notes Butter Cedar Cream Fermented Fruit Leather Moss Mushroom Red Wine Rum Smoke Tobacco Toast Vanilla White Wine Yeast Select temperature unit: °C °F Generate Recipe Download Recipe" }, { "title": "English Football Head-to-Head Statistics", "url": "/h2h", "content": "This page can be directly linked to by passing team names into the url, e.g. to get the head-to-head statistics of Arsenal v Chelsea visit: https://seanelvidge.com/h2h?team1=Arsenal\u0026amp;team2=Chelsea (This page looks best on a larger screen / landscape mobile phone screen) Head-to-Head Statistics Team #1: Team #2: Start Date: End Date: Premier League Era only? Compare Reset Unknown team name! vs" }, { "title": "Hard Rock Cafe", "url": "/hrc", "content": "Total Cafés: – Visited: – The data used to make this map can be downloaded from here." }, { "title": "Kalman Filters", "url": "/kalman/", "content": "A lot of my work has, historically, been based around (Ensemble) Kalman filters. On this page I have some resouces which I hope will help you get more to grips with them. The underlying mathematics can be a little opaque, so I have tried to visualize the process here. At the 2021 International Union of Radio Science (URSI) I gave a tutorial describing the derivation of the Ensemble Kalman Filter directly from Bayes’ Theorem. Unfortunately the tutorial was not recorded, but my notes are available to anyone who is interested: However, the best way to learn about Kalman filters, is to play with them. Here is a fun little problem to get you thinking about them. Imagine dropping a bouncy ball (vertically) from some height and watch it as it slowly comes to rest: There are a whole host of variable parameters which will determine the profile of the bouncy ball including (the values in brackets are example values used to generate the above plot): Initial height of ball (10) Mass of ball (2) Radius of ball (0.25) Coefficient of restitution between ball and ground (0.8) Air density (1.1) Dynamic viscosity (1.7e-5) Drag coefficient (0.3) Example Python code of this bouncy ball model can be downloaded here. It is a simple model but saves you having to code it up. Now, given that I dropped a bouncy ball from 20 metres (initial height) and used an imperfect sensor which recorded a series of observations, use the Ensemble Kalman Filter to estimate the values of the other 6 parameters above (mass, radius, coefficient of restitution, air density, dynamic viscosity and the drag coefficient). Notes: use an internal model timestep of 0.01s (the default in the provided Python code) to create the ensemble, generate a wide range of possible (but realistic) bouncy ball states (e.g. use dynamic viscosities between \\(1.5\\times 10^{-5}\\) and \\(2\\times 10^{-5}\\)) Use the provided Python code to both define the Ball and propagate it forward" }, { "title": "English Football League Table Generator", "url": "/leaguetable", "content": "League Table Generator Season: or Year: (where '2024' would mean the season 2023/2024) or Start Date: End Date: (if only providing a start date it will assume an end date of today, if only providing an end date it will assume the start date is the beginning of that season) and Football Tier: (1, 2, 3 or 4 where 1 means the 'first' tier [currently the Premier League]) or Division: Generate Table Reset League Table * points change (details here) Download table as image" }, { "title": "Match Outcome Probabilities Calculator", "url": "/matchProbs", "content": "Team #1 (Home): Team #2 (Away): Compute Reset vs" }, { "title": "news", "url": "/news/", "content": "\u0026lt;div class=\"table-responsive\" \u0026gt; \u0026lt;table class=\"table table-sm table-borderless\"\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\"row\" style=\"width: 20%\"\u0026gt;Jan 15, 2016\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt; A simple inline announcement with Markdown emoji! :sparkles: :smile: \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\"row\" style=\"width: 20%\"\u0026gt;Nov 07, 2015\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt; \u0026lt;a class=\"news-title\" href=\"/news/announcement_2/\"\u0026gt;A long announcement with details\u0026lt;/a\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\"row\" style=\"width: 20%\"\u0026gt;Oct 22, 2015\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt; A simple inline announcement. \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;" }, { "title": "outreach", "url": "/outreach/", "content": "I give a number of talks on a variety of subjects and at a range of levels each year. I have given talks on my academic research: space weather, ionospheric modelling, data assimilation and data visualization, and in both mathematics and astronomy for public audiences. I also regularly give talks on STEM subjects at schools (both primary and secondary). I can give talks both for the general public, more specialised audiences, or somewhere in-between. If you would like me to give a talk please just drop me an email, and we can arrange something suitable. A selection of previous titles are given below. Sunny, with a mild chance of catastrophe Three Ancient Problems The Shape of the Universe The Hunt for the Fifth Equation The Storm to End All Storms Looking Closer at Flatland and Flatlanders (the physics of Flatland) Days to Decades: Forecasting Satellite Collisions Maths \u0026amp; Physics of Star Trek" }, { "title": "publications", "url": "/publications/", "content": "You can access my up-to-date list of publications here." }, { "title": "School Bills", "url": "/schoolbills", "content": "Month: Children: George and James. Prices: Breakfast (Mon/Tue/Wed/Fri) £4.50/child/day; After-school (Mon/Thu/Fri) £5.00/child/day; Lunch (Mon–Fri) £2.70/child/day. Use the weekday calendar to deselect days for each child (e.g., half-term, trips). Each week has master checkboxes to toggle that week’s column. By default all applicable days are selected. Weekday calendar (Mon–Fri only) Totals Breakfast club: £0.00 After school club: £0.00 Lunches: £0.00 Grand total: £0.00 Per child breakdown Child Clubs (Breakfast+After) Lunches Total" }, { "title": "tools", "url": "/tools/", "content": "{% if site.enable_project_categories and page.display_categories %} {% for category in page.display_categories %} {{ category }} {% assign categorized_projects = site.tools | where: \"category\", category %} {% assign sorted_projects = categorized_projects | sort: \"importance\" %} {% if page.horizontal %} {% for project in sorted_projects %} {% include projects_horizontal.liquid %} {% endfor %} {% else %} {% for project in sorted_projects %} {% include tools.liquid %} {% endfor %} {% endif %} {% endfor %} {% else %} {% assign sorted_projects = site.tools | sort: \"importance\" %} {% if page.horizontal %} {% for project in sorted_projects %} {% include projects_horizontal.liquid %} {% endfor %} {% else %} {% for project in sorted_projects %} {% include tools.liquid %} {% endfor %} {% endif %} {% endif %}" }, { "title": "Visited US States", "url": "/usstates", "content": "" }, { "title": "mathematics", "url": "/articles/tag/mathematics/", "content": "" }, { "title": "code", "url": "/articles/tag/code/", "content": "" }, { "title": "bash", "url": "/articles/tag/bash/", "content": "" }, { "title": "books", "url": "/articles/tag/books/", "content": "" }, { "title": "misc", "url": "/articles/tag/misc/", "content": "" }, { "title": "spaceWeather", "url": "/articles/tag/spaceweather/", "content": "" }, { "title": "football", "url": "/articles/tag/football/", "content": "" }, { "title": "MathsOnAMug", "url": "/articles/tag/mathsonamug/", "content": "" }, { "title": "Tracking Football Team Strengths with a Bayesian Kalman Model", "url": "/articles/2025/Football_team_rankings/", "content": "Not all football-rating systems are the same. Many of the public ones, like the excellent ClubElo, do a fine job of ranking teams using elegant updates. Win and your rating rises, lose and it falls; the amount you move depends on how “surprised” the model was. This model begins in the same spirit but replaces those heuristic updates with a probabilistic engine: a Bayesian Extended Kalman Filter coupled to a modern version of the Bradley–Terry model (with added draws). The core idea is simple: treat each team’s strength as something hidden that we estimate and track through time, with uncertainty that expands between matches and contracts when new results arrive. Across the full historical dataset (every English league match since 1888) the system achieves a mean Brier score of 0.2035. These values are significantly better than many published computer models (e.g. Nyamdorj et al. 2014, BSIC, 2024 and Harvard Sports Analysis Collective, 2015) which indicates a stable, well-calibrated predictive performance. Crucially, because the filter quantifies its own uncertainty, it can tell us not only who is strongest, but how confident we should be in that judgement. The rest of this post goes into the mathematical details of the ranking algorithm, but if you want to access the underlying data it is available here (specifically the file EnglandLeagueResults_wRanks.csv). The Big Picture Imagine every team has a hidden “true strength” \\(s_i\\). Before a match, the home and away teams carry beliefs about their current strengths, each with an associated uncertainty. When they play, the result provides new information that updates those beliefs. In the ClubElo framework this update is written directly as \\[R_{\\text{new}} = R_{\\text{old}} + K(S - E),\\] where \\(S\\) is the score (1 for a win, 0.5 for a draw, 0 for a loss), \\(E\\) is the expected probability, and \\(K\\) is a fixed responsiveness parameter. Here, the same logic is embedded in a Kalman filter, which means the effective \\(K\\) is learned automatically. The update size depends on two things: how uncertain we are about the teams, and how surprising the result was. Upsets between uncertain teams lead to large updates; shocks between well-understood teams barely move the needle. Between matches, team strengths are not frozen. Instead, they evolve according to a mean-reverting stochastic process, allowing form to drift while preventing runaway behaviour. Newly promoted teams begin with large uncertainty and adapt quickly; established teams gravitate toward long-term baselines rather than rising indefinitely. Modelling Match Outcomes (Wins, Draws, Losses) Match outcomes are modelled using the Davidson extension of the Bradley–Terry model, which naturally incorporates draws. The probability of each outcome is \\[\\Pr(\\text{Home}) = \\frac{e^{\\Delta}}{e^{\\Delta} + e^{-\\Delta} + \\kappa},\\] \\[\\Pr(\\text{Draw}) = \\frac{\\kappa}{e^{\\Delta} + e^{-\\Delta} + \\kappa},\\] \\[\\Pr(\\text{Away}) = \\frac{e^{-\\Delta}}{e^{\\Delta} + e^{-\\Delta} + \\kappa},\\] where \\[\\Delta = \\beta \\bigl(s_H - s_A + h\\bigr).\\] Here, \\(s_H\\) and \\(s_A\\) are the latent strengths of the home and away teams, \\(\\beta\\) is a scaling parameter, \\(\\kappa\\) controls the draw rate, and \\(h\\) is the home-advantage term. Home Advantage - Explicit and Time-Varying Home advantage is not treated as a fixed constant. Instead, it is explicitly modelled and allowed to vary over time. Historical analysis shows that home-win rates in English football have declined dramatically since the late 19th century, falling from well above 60% to closer to 40–45% in the modern era (see this analysis). By allowing the home-advantage parameter \\(h\\) to evolve slowly with time, the model correctly distinguishes between a home match in 1890 and one in 2025. This prevents systematic bias when comparing teams across eras and is a key improvement over static-advantage rating systems. Validation and Rating Scale Predictive accuracy is measured using the Brier score, defined as the mean squared error between predicted probabilities and observed outcomes. Over the full dataset the score is 0.2035 (for the 2024/25 season it is 0.2085), indicating robust calibration both historically and in the present day. Internally, the filter operates on a latent “skill” scale roughly spanning \\(-3\\) to \\(+3\\). However for presentation, these values are mapped linearly onto an Elo-style scale, centred on 1000 points, with elite teams reaching 1800+ and lower-league teams clustering about a thousand points below. This transformation is purely cosmetic; all inference happens on the latent scale. Part II — If You Dare Read On: The Mathematics 1. State Evolution (Ornstein–Uhlenbeck Dynamics) Each team’s latent strength evolves as \\[s_{i,t+1} = \\rho\\, s_{i,t} + (1 - \\rho)\\, \\mu_{\\text{tier}(i,t)} + \\varepsilon_{i,t},\\] with \\[\\varepsilon_{i,t} \\sim \\mathcal{N}(0, q_i).\\] Here, \\(\\rho\\) controls persistence, \\(\\mu_{\\text{tier}}\\) is the long-term baseline for the team’s division tier, and \\(q_i\\) is the process variance. This formulation allows ratings to drift while remaining anchored to realistic league-level expectations. 2. Observation Model Given predicted strengths \\(s_H\\) and \\(s_A\\), the model produces a probability vector \\[\\mathbf{p} = \\begin{bmatrix} p_H \\\\ p_D \\\\ p_A \\end{bmatrix},\\] using the Davidson–Bradley–Terry equations above. The Jacobian matrix \\(H\\) is computed by differentiating \\(\\mathbf{p}\\) with respect to the state vector \\([s_H, s_A]^T\\), enabling linearisation of the nonlinear observation model. 3. Extended Kalman Filter Update Prediction step: \\[x^- = F x_t,\\] \\[P^- = F P_t F^\\top + Q,\\] where \\(F = \\rho I\\) and \\(Q\\) is the process-noise covariance. Observation noise: \\[R = \\operatorname{diag}(\\mathbf{p}) - \\mathbf{p}\\mathbf{p}^\\top.\\] Update step: \\[K = P^- H^\\top \\bigl(H P^- H^\\top + R\\bigr)^{-1},\\] \\[x_{t+1} = x^- + K (y - \\mathbf{p}),\\] \\[P_{t+1} = (I - K H) P^-.\\] The Kalman gain \\(K\\) replaces the fixed \\(K\\)-factor of traditional Elo systems, adapting automatically to uncertainty and surprise. A secondary control loop monitors the normalised innovation squared to ensure statistical consistency over time. Why This Matters This framework enforces mathematical honesty. Uncertainty is explicit, calibration is measurable, and every parameter (\\(\\beta\\), \\(\\rho\\), \\(q\\), \\(\\kappa\\), tier baselines, and historical home advantage) has a clear interpretation. Instead of a single magic constant, the model becomes a living system that adapts across seasons, divisions, and eras." }, { "title": "Football Bogey Grounds and How Statistics Can Prove Them", "url": "/articles/2025/Football_bogey_grounds/", "content": "Football supporters are never short of folklore. Some of it heroic, some of it tragic, and some of it mathematically suspicious. Among the more enduring tales is the bogey ground: that one venue where your club never quite manages to win, no matter how many times the fixture computer sends you there with fresh optimism, a new manager and a good run of form. But one question always pops up in the back of my mind when I hear this: When is a bogey ground actually a bogey ground, and when is it just a trick of small numbers? Most fans instinctively understand the point. Losing your only ever visit to Carlisle does not make Brunton Park a cursed ground. A couple of failed trips to Luton are not evidence of eldritch forces at work. Yet, scattered across the long history of English league football, a few pairs of teams have met often enough, and still produced zero away wins, that bogey grounds become statistically real. In the Premier League an example of this is Fulham, and their trips to Arsenal. Fulham have played 32 league matches away at Arsenal. They have never won. Not once. Not in 1914 (lost 2-0). Not in 1964 (2-2). Not in 2014 (lost 2-0). Zero wins from thirty-two attempts. Seven draws. Twenty-five defeats. Even more striking, this isn’t just a bad example, this is, among every club in the entire Football League dataset, over 135 years of football, no team has a larger, more emphatic, more mathematically convincing record of away futility than Fulham at Arsenal. So let’s take a look, not just at the record itself, but at how we can quantify bogeyness in a way that recognises your intuition (“two games aren’t enough!”) but takes advantage of the huge dataset football provides. Why “never won” isn’t enough (the curse of small \\(n\\)) Imagine you flip a coin twice and get two tails. Does that mean the coin is biased? Of course not. You need more flips before you start thinking that someone is up. The same goes for football. A team losing its only away visit to Manchester City tells you nothing. Losing twice? Still nothing except a faint sense of déjà vu. Losing five times? Now you’re paying attention. Losing ten times? Stop buying tickets to the away game. So what we need is a way of answering this question: Given a team has played \\(n\\) away matches at a ground and won none, what is a reasonable upper limit on how good their true chance of winning there might be? Fortunately statistics, as it always does, gives us a handy tool for working this out, the Wilson confidence interval. A (gentle) introduction to the Wilson interval Don’t panic; no equations are necessary. Just an idea. A Wilson interval takes two bits of information: the number of games played (\\(n\\)), and the number of games won (0, in our bogey-ground case) and asks: “If their true underlying chance of winning were \\(p\\), how big could \\(p\\) realistically be before the observed data (zero wins) would start to look implausible?” For small \\(n\\) (not many games played) the answer is: “\\(p\\) could still be quite large.” For large \\(n\\) (a large number of games played) the answer is: “\\(p\\) must be quite small.” This lines up with our intuition. A team with 0 wins from 2 tries? Their true win rate might easily be 30%, 40%, even 50%. A team with 0 wins from 32 tries? Now the interval collapses. It tells you the true away win probability is almost certainly very small, low enough that even by bad luck alone, you’d be surprised not to have picked up a single win after that many attempts. The Wilson interval is good for this particular problem (compared to other approaches like the Wald Interval) because it behaves properly at the boundaries (like at 0% or 100%), where ordinary methods can breakdown. Football fans should love it because it gives scientific legitimacy to something they’ve always known: some bogey grounds are imaginary, but a few are real. The most convincing bogey of them all: Fulham at Arsenal So, what does Wilson say about Fulham’s 0 wins from 32 away league games at Arsenal? It says this: In each of Fulham’s away matches against Arsenal their probability of winning must have been (at the very most) 10% for 0 wins out of 32 to not be statistically suspicious. Mathematically this comes from plugging \\(n\\) (number of games) and \\(z\\) (which is equal to 1.96 for 95% ‘confidence’) into the equation for calculating the upper bound of the Wilson interval: \\[\\mbox{Wilson Upper Bound} = \\frac{z^2}{n+z^2} = \\frac{1.96^2}{32+1.96^2} = 0.107 = 10.7\\%\\] So the only way we could avoid saying Arsenal is Fulham’s away team nemesis is if we believe Fulham, across the whole history of the fixture (starting in 1913; see my head-to-head tool here), have not had better than a 10% chance of winning those games. Whilst we can’t be (mathematically) certain that it is true, it is incredibly unlikely. Typical away win rates in the top division over history are roughly 25-30% range. Even clear cut underdogs often have pre-match win probabilities around 15-20%. So, therefore, I think it is safe to say to conclude: Fulham away at Arsenal is not just a bogey ground, it is the bogey ground. The Premier League’s most mathematically defensible curse. Other bogey teams Fulham–Arsenal is the only pair in the entire database whose Wilson upper bound hovers at just above the 10% threshold. But several others get close, here are the best (or worst) of the rest: Away Team Home Team Played Record Wilson Upper Fulham Arsenal 32 0W–7D–25L 10.7% Grimsby Town Blackburn Rovers 28 0W–9D–19L 12.1% Mansfield Town Reading 26 0W–5D–21L 12.9% Tranmere Rovers Barnsley 26 0W–11D–15L 12.9% Oldham Athletic Charlton Athletic 25 0W–11D–14L 13.3% Newport County Luton Town 24 0W–7D–17L 13.8% Coventry City Preston North End 24 0W–9D–15L 13.8% These are not small samples. These are not casual coincidences. When you are approaching 20 or 30 visits with no victory, it is time to stop thinking you are unlucky and start accepting you have a bogey ground." }, { "title": "40 points to avoid relegation?", "url": "/articles/2025/40_points_to_avoid_relegation/", "content": "The number has become part of Premier League folk law. Forty points. Reach 40 and you can relax, the trapdoor to the Championship won’t open. At the start of the 2015/16 season (the season Leicester City won the League!) their manager Claudio Ranieri set them the target of reaching 40 points to avoid relegation. When they hit the target: “We have 40 points which was the goal. It’s champagne for my players!” Claudio Ranieri, 2 Jan, 2016. But where did this number come from, and it is the right target? Many people have noted that the 40 point mark is a “myth” (e.g. The Premier League, BBC, The Athletic). But those articles are really just saying that, on average, you need less than 40 points to survive (and the number of points you need seems to be decreasing). The plot above shows the number of points required to stay in the top division of English football (since 3 points for a win was introduced in 1985; this includes before the Premier League started in 1992). Note that during that time there have been a varying number of teams in the top division (between 20 and 22) which obviously impacts the points required. We have normalized this to a 38-game season for comparison. A few things are immediatly obvious: Most of the time you do not need 40 points to avoid relegation (34/44; 77%), The average number of points needed to avoid relegation over the last 40 years is 37 (in the Premier League era, it is 36 points), There is a clear decreasing trend of the number of points required. So where does the idea that you need 40 points come from? There is no obvious origin of the phrase. My best guess is that in the mid 90s, after Southampton survied (just, on goal difference) with 38 points in 1995/96, followed by Coventry City with 41 the following year, then Everton with 40 and then Southampton (again) with 41 points in 1998/99, that that run of four was enough for the (rough) number to stick. But despite the previous analysis (by me and many others) I think 40 is still the correct, pre-season, target. Moving to (for example) a 37 point target would only give you slightly better than a 50-50 chance of staying up (54.5%). Perhaps you would like a little more certainty… One way to look at this is to calculate the cumulative distribution function (CDF). The CDF is a function that shows the probability that a random variable is less than or equal to a specific value. It “accumulates” or “adds up” the probabilities for all outcomes up to a certain point. The above figure shows two, slightly different, CDFs, in blue across the whole dataset (since 1985) and in red just in the Premier League era. To read the plot look at the number of points on the x-axis and then read off the corresponding probability of avoiding relegation on the y-axis for that number of points. Now the 40 point value makes a lot more sense, as it gives a team over an 80% chance of staying in the division, now I prefer those odds! So, whilst there are plenty of posts online telling you that the 40 point target is a myth, I think, as pre-season target for teams, it is still a good one." }, { "title": "New space weather modelling suite enables upper atmosphere forecasting", "url": "/articles/2025/New_space_weather_modelling_suite/", "content": "A pioneering new space weather forecasting modelling suite will enable operational modelling of the upper atmosphere at the Met Office for the first time in a major breakthrough for UK atmospheric science. The Advanced Ensemble Networked Assimilation System (AENeAS) is a new suite of space weather forecasting models available to the Met Office that focuses on how space weather can influence the thermosphere and ionosphere here on Earth. The suite, built at the University of Birmingham, and developed in collaboration with Lancaster University, the Universities of Leeds, Bath and Leicester and the British Antarctic Survey is now running on the Met Office’s new supercomputer. The deployment of this suite at the UK Met Office is the realization of a 10-year vision of SERENE, to build and deliver a state-of-the-art upper atmosphere modelling capability into operational use. The tools will be able to support a wide range of users and ultimately allow people to make informed decisions earlier, being proactive rather than reactive in their response to space weather. Professor Sean Elvidge, Head of Space Environment and Radio Engineering (SERENE) - University of Birmingham Complementing the Met Office’s existing space weather forecasting models, which include predicting the arrival of events from the surface of the Sun, this system introduces new forecasting capability for modelling impacts from space weather on satellites, aviation, communications and services which rely on GNSS. Professor Sean Elvidge, Head of Space Environment and Radio Engineering (SERENE) at the University of Birmingham, and the lead developer of the system said: “The deployment of this suite at the UK Met Office is the realization of a 10-year vision of SERENE, to build and deliver a state-of-the-art upper atmosphere modelling capability into operational use. “The tools will be able to support a wide range of users and ultimately allow people to make informed decisions earlier, being proactive rather than reactive in their response to space weather.” The new suite means that, for the first time, forecasters at the Met Office Space Weather Operations Centre (MOSWOC) will have access to forecast model output on the impacts of space weather on the ionosphere, as well as enhanced modelling of the thermosphere. Once again, cutting-edge British innovation is making a remarkable difference to our daily lives - this time from way up in the atmosphere. This is a really exciting example of how better understanding of what’s happening in space can protect the tech we all rely on, from GPS on our phones to keeping the power grid working. UK Science Minister Lord Vallance Met Office Space Weather Manager Simon Machin said: “This delivers a world-leading capability that provides greater confidence and forecasting skill than any models currently in operation anywhere else in the world. “This isn’t just about science - it’s about protecting the systems we rely on every day. From aircraft communications to GPS in your phone, space weather can affect us all.” Science Minister Lord Vallance said: “Once again, cutting-edge British innovation is making a remarkable difference to our daily lives - this time from way up in the atmosphere. “This is a really exciting example of how better understanding of what’s happening in space can protect the tech we all rely on, from GPS on our phones to keeping the power grid working.” The new modelling capability will be able to assimilate near real-time data about the current state of the ionosphere and thermosphere, combine with forecasts of solar activity from the Sun to produce accurate and actionable forecasts of the upper atmosphere. This will help enable service providers to take mitigating actions to prevent impacts from space weather where possible. Professor Farideh Honary from Lancaster University said: “We are happy to see our research being translated into a useful product to be used by industry. The research and modelling led by Lancaster is relevant to the aviation industry and in particular to flights using polar routes which are dependent on high frequency communications. “These flights have significantly increased since their initial opening in the 1990s due to their operational advantages such as reduced flight times and fuel consumption, which translate to cost savings and environmental benefits like lower carbon emissions.” More accurate and precise forecast information will help enable service providers to take mitigating actions to prevent impacts from space weather where possible. One example of this is with users of Global Navigation Satellite Systems (GNSS), such as GPS, for positioning or navigational purposes. If they understand that there is likely to be a loss of accuracy in GNSS, they can switch to using other systems. Together, the new modelling suite has been delivered as part of Space Weather Instrumentation, Measurement, Modelling and Risk (SWIMMR) programme, which was funded through the UKRI Strategic Priorities Fund and designed to enhance the UK’s capability for monitoring, modelling and forecasting space weather. Professor Ian McCrea, SWIMMR Programme Lead at STFC RAL Space, said: “These new models represent a significant step forward for the UK’s capacity to model, forecast, and understand key components of our upper atmosphere. By coupling advances in physical modelling with global scale observations, they will enable unparalleled awareness of Earth’s geospatial environment. “The models promise to address a wide range of use cases, ranging from radio communications to the prediction of satellite orbits, and we expect that they will be of great importance to a huge variety of stakeholders. They also provide a clear demonstration of how the collaboration between academics and end users, which SWIMMR has enabled, can benefit members of both communities.”" }, { "title": "The Trend in Taylor Swift's Mood", "url": "/articles/2025/Taylor-Swifts-Mood/", "content": "Taylor Swift doesn’t just release albums, she releases chapters of her life. From the optimism of Fearless to the bite of Reputation, from the hushed poetry of folklore to the midnight reflections of, well, Midnights. Each album is often thought of as a snapshot of where she was at that moment in time. But can we actually see that story in the music itself? I thought I’d try. The chart below shows an experiment: every dot is a Taylor Swift song, plotted by its “average pitch”. The black line is the overall trend album by album, a kind of data-driven glimpse into her changing musical mood. Here’s how it works. Every song can be broken down into thousands of tiny audio frames, and we can estimate the pitch of each. You could then just average all those frames/estimated pitches together, but then you end up something pretty meaningless: a whispered aside would count the same as a belted chorus. Instead, I weighted each frame by its ‘loudness’. Big, bold notes dominate, quiet moments barely move the needle. Mathematically the “weighted average pitch” looks like this: \\[M = \\frac{\\sum_{i=1}^{N}w_im_i}{\\sum_{i=1}^{N}w_i},\\] where \\(m_i\\) is the pitch of a frame (in MIDI numbers) and \\(w_i\\) is how loud it was. Here we do this in “MIDI” because it is a musical scale. Every step is a semitone. If we used raw frequency in Hertz the maths would skew towards the low notes in a way that doesn’t really match how we actually hear music. So, if you look at our plot, focusing on the smoothed black line, a story starts to appear: Early albums like Fearless are higher up the stave, matching the wide-eyed optimism of youth. 1989 soars with big pop anthems. Then comes the brooding dip of Reputation, the fallout years. During lockdown, folklore and evermore settle into quieter, lower ground. Midnights stays low, steeped in melancholy. And then The Tortured Poets Department plunges deepest of all. Then finally with The Life of a Showgirl, the line bends upward again (significantly). Happy times have returned. Obviously this is a very simplistic look at the album, and its emotional impact: lyrics, harmony, production, they all matter. But still, there’s something striking about seeing the arc of her career emerge from the raw numbers. By weighting the most powerful sung moments, we get a sense of where the “centre of gravity” of each album lies. And it turns out that when Taylor sings her life, the data sings it back. Note: If you would like to try this yourself (inc. for other artists) here is the link, to the code I used to make this post. You just need to pass a folder of music files to the code and it will work out the rest for you." }, { "title": "Pour Over Brewing Recipe Generator", "url": "/articles/2025/Pour_over_brewing_recipe_generator/", "content": "I recently purchased a Fellow Aiden Precision Coffee Maker which is a pour over coffee brewer. Able to brew individual cups to whole batches this machine is fantastic, and gives you complete control of the recipe it uses. From the bloom time and temperature to the number of pulses and the ability to control the temperature of each individual pulse. There are a number of default reciepes included and through the Fellow Drops programme you can get a number of tuned recipes for those coffees. Whilst, at the end of the day, coffee all coems down to personal preference, and any provided reciepe will need fine tuning, there are some fundamentals which provide a good starting point. To try and capture that information I have made the following recipe generator: seanelvidge.com/brewcoffee If you visit that page, with the details of the coffee bean you have, you can have a look at what the reciepe generator says you should use. If you would like (a lot) more detail of the decisions that I have made to create the generator then read on. Roast Level Adjustments Roast level significantly influences extraction dynamics. Light roasts, dense and less porous, require extended blooming (initial wetting) and multiple pulses of hot water at precise intervals to fully unlock their flavors. Consequently, the recipe defaults for light roasts feature higher bloom ratios and longer bloom times, ensuring sufficient CO₂ release for consistent extraction. Conversely, dark roasts, due to their brittleness and faster extraction tendency, need lower extraction ratios and fewer pulses to avoid bitterness. The recipe parameters for darker roasts emphasize shorter bloom times and higher initial bloom temperatures to develop complexity without over-extraction. Specifics Light Roast: Brew Ratio: Increased to 17 for lighter body. Bloom Ratio: Increased to 3.5 to aid degassing. Bloom Time: Extended to 60 seconds for thorough extraction. Pulse Count: Increased to 6, ensuring complete flavor extraction. Dark Roast: Brew Ratio: Reduced to 15 to intensify body and reduce bitterness. Bloom Ratio: Decreased to 1.5 due to easier extraction. Bloom Time: Shortened to 40 seconds to prevent over-extraction. Pulse Count: Reduced to 3, limiting extraction bitterness. Bean Origin Influence Beans from different regions exhibit unique physical traits influencing their extraction behavior. East African coffees, typically denser with higher solubility, require gentler bloom phases—lower ratios, shorter times, and reduced temperatures—to mitigate over-extraction and accentuate their bright, vibrant acidity. Latin American coffees often demand a longer bloom and higher temperatures to thoroughly degas and enhance sweetness and body. Indonesian coffees, known for their robust character, are best extracted with moderate bloom parameters that balance intensity with clarity. Specifics East African (Ethiopia, Kenya, etc.): Bloom Ratio: Decreased by 0.5 to minimize over-extraction. Bloom Time: Shortened by 5 seconds. Bloom Temperature: Reduced by 3°C. Pulse Count: Decreased by 1 to control acidity. Latin American (Brazil, Colombia, Guatemala): Bloom Ratio: Increased by 0.5 for effective degassing. Bloom Time: Increased by 5 seconds. Bloom Temperature: Raised by 3°C. Pulse Count: Increased by 1 to promote even extraction. Indonesian (Sumatra, Java): Bloom Ratio set to 2.5, Bloom Time standardized at 50 seconds. Pulse Count: Increased by 1 to maximize extraction clarity. Altitude Considerations High-altitude coffees, denser due to slower maturation, need more aggressive extraction parameters—finer grind, stronger brew ratios, and longer bloom durations—to achieve optimal flavor development. In contrast, lower-altitude beans benefit from coarser grinds and gentler extraction processes to preserve balance and avoid bitterness. Specifics High Altitude (\u0026gt;1500m): Brew Ratio: Reduced by 0.5 for stronger extraction. Bloom Ratio: Increased by 0.5 to assist degassing. Bloom Time: Increased by 5 seconds. Grind Size: Finer by 2 increments. Low Altitude (\u0026lt;1200m): Brew Ratio: Increased by 0.5 for gentler extraction. Bloom Ratio: Decreased by 0.5. Bloom Time: Reduced by 5 seconds. Grind Size: Coarser by 2 increments. Processing Method Adjustments Processing significantly impacts bean composition. Natural, honey, and anaerobic coffees, rich in sugars, require longer bloom periods and additional pulses to evenly extract these complex flavors. Washed coffees, typically cleaner and less dense, are best brewed with shorter blooms and fewer pulses, maintaining clarity and brightness. Specifics Natural, Honey, Carbonic, Anaerobic: Bloom Ratio: Increased by 0.5 (minimum of 2.5). Bloom Time: Extended by 5 seconds (minimum of 45 seconds). Pulse Count: Increased by 1 to control extraction consistency. Washed, Double Fermentation, Wet-Hulled: Bloom Ratio: Decreased by 0.5. Bloom Time: Reduced by 5 seconds. Pulse Count: Decreased by 1. Freshness Factor The age of roasted coffee profoundly affects extraction due to CO₂ levels. Freshly roasted beans (0-7 days) release considerable CO₂, necessitating increased bloom water ratios, longer bloom times, and finer grinds to ensure thorough extraction without bitterness. Conversely, older beans (\u0026gt;20 days) benefit from shorter blooms, lower temperatures, and coarser grinds to avoid over-extraction and maintain desirable flavors. Specifics Fresh Coffee (0–7 days): Bloom Ratio: Increased by 0.5. Bloom Time: Extended by 5 seconds. Grind Size: Finer by 4 increments. Pulse temperatures decreased by 1°C per pulse to counter CO₂ resistance. Older Coffee (\u0026gt;20 days): Bloom Ratio: Decreased by 0.5. Bloom Time: Reduced by 5 seconds. Grind Size: Coarser by 4 increments. Pulse temperatures increased by 1°C per pulse to improve extraction. Flavor Profile Targeting Achieving specific tasting notes requires fine-tuning extraction parameters: Fruity and acidic notes: Higher temperatures and finer grinds enhance bright flavors. Nutty and chocolatey notes: Richer bodies are cultivated through lower brew ratios and coarser grinds. Floral and herbal notes: Delicate aromatics emerge clearly with increased bloom ratios. Sweet and heavy notes: Lower bloom ratios preserve syrupy, sweet profiles. Creamy textures: Lower temperatures ensure smooth, rounded mouthfeels. Specifics Fruity/Acidic Notes: Brew Ratio: Increased by 0.5 for brightness. Bloom Temperature: Raised by 2°C. Grind Size: Finer by 4 increments. Nutty/Chocolate Notes: Brew Ratio: Decreased by 0.5 for richer body. Grind Size: Coarser by 2 increments. Floral/Herbal Notes: Bloom Ratio: Increased by 0.5 for aromatic extraction. Grind Size: Coarser by 2 increments. Heavy Sweet Notes: Bloom Ratio: Reduced by 0.5 for syrupy texture. Grind Size: Finer by 2 increments. Creamy Notes: Bloom Temperature: Reduced by 2°C to preserve smoothness. Pulse Temperature Profiles Dynamic pulse temperatures optimize extraction across roast levels: Light roasts gradually increase temperature across pulses, enhancing complexity. Medium roasts maintain stable temperatures for balanced extraction. Dark roasts reduce temperatures progressively to avoid bitterness, enhancing depth. Final pulse temperatures further adjust to emphasize specific tasting notes—higher initial temperatures highlight bright, fruity notes, while lower final temperatures preserve sweetness and depth. Specifics Light Roasts: Gradually increase temperature from 90°C to 96°C. Medium Roasts: Maintain stable bloom temperature; adjust slightly for anaerobic or carbonic processes. Dark Roasts: Decrease temperature progressively from 91°C to 85°C to minimize bitterness. Conclusion The Fellow Aiden Precision Coffee Maker is a great coffee brewer, giving you almost complete control of the brewing process. You can access the recipe generator here: seanelvidge.com/brewcoffee" }, { "title": "Waning Home Advantage in English League Football", "url": "/articles/2025/Home_advantage_in_English_football/", "content": "For generations, “home advantage” has been a tenet of sports. The roar of the crowd, the familiarity of the pitch, the comfort of the home dressing room – all give the home team an edge. This is particularly true in English football. However, a look at the data spanning over a century of English football reveals an interesting trend: the home advantage is shrinking. Using my database of all English league results since 1888 (available here, and described here) we can track the Home Win %, Draw %, and Away Win % from 1888 to the present day (at the time of writing, that is halfway through the 2024/2025 season, but the charts in this post should automatically update). The results are pretty clear: In the late 19th and early 20th centuries, home teams were dominant, boasting win percentages well above 60%. Away wins were a relative rarity, hovering around 20%. But as the decades have progressed, the lines have converged. Home win percentages have steadily declined, dipping towards 40% in recent years, while away wins have climbed, now consistently above 30% and with a clear upwards trajectory. It would be easy to assume that this trend is confined to the top tier of English football, where the largest investments in training facilities, player recruitment, and tactical analysis occur. However, if we look at the home win percentages across all four tiers of English football (where “Tier 1” is the highest tier, currently the Premier League and “Tier 4” is the lowest, EFL League Two) we can see that this is a league-wide phenomenon. As you can see, the decline in home win percentages is remarkably consistent across all levels of professional English football. While there are some minor variations between divisions, the overall trajectory is the same: downwards. This suggests that the factors driving the change are not unique to the top tier but are systemic throughout the entire football pyramid. Each division shows high home win % at the start of the time series (above 50%), but each division ends up closer to 40% by the end of the series. So, what’s driving this shift away from home dominance? Several factors are likely at play: Standardization of Playing Conditions: In the early days, pitch conditions varied wildly. Home teams were intimately familiar with their own, often quirky, pitches, giving them a distinct advantage. Over time, regulations and advancements in groundskeeping have led to more standardized, high-quality pitches across the league, leveling the playing field. This impact would be felt across all divisions. Improved Travel and Accommodation: Early football often involved arduous journeys for away teams, leaving them fatigued and less prepared. Modern transportation and improved accommodations have minimized the physical toll of travel, allowing away teams to arrive rested and ready to compete. Again, this is relevant to all levels of the game. Tactical Advancements and Analysis: The modern game is far more tactically sophisticated. Managers and analysts have access to vast amounts of data and video footage, allowing them to dissect opponents’ strengths and weaknesses, regardless of location. Away teams can now better prepare for the specific challenges posed by their opponents and their home stadiums. While the resources may be greater at the top, these advancements have filtered down through the divisions. Professionalization and Fitness: Players today are fitter, faster, and more technically skilled than ever before. This overall increase in athleticism can help away teams better cope with the pressures of playing in hostile environments, a trend seen across the footballing spectrum. The Future of Home Advantage Whilst home advantage may not be what it once was, it hasn’t disappeared entirely. The support of the home crowd can still provide a boost, and familiarity with the surroundings can offer a slight edge. However, the trend is undeniable. The gap between home and away performance is narrowing, and the English Football League, across all its divisions, is becoming increasingly competitive on all fronts. If we look to extrapolate the trends in the data we can try and estimate what will happen in the future, fitting lines of best fit to the data we can estimate that home advantage will stick around for quite a while yet!" }, { "title": "Brewing the Perfect Coffee at Altitude", "url": "/articles/2025/Brewing_coffee_at_altitude/", "content": "For those of us who consider coffee an essential part of our daily routine, the pursuit of the perfect cup is a never-ending quest. But did you know that your altitude can significantly impact your brew? This post is inspired by my new Fellows Aiden Precision Coffee Maker which, on one of the first set up screens asks: Perhaps that seems like an odd question, buf if you’ve ever tried recreating your favourite Birmingham coffee experience in Boulder, Colorado (which I have, many times!), you might have noticed something’s a bit “off”. Let’s dive into the science behind why: It all boils down to atmospheric pressure. The air around us exerts pressure, and this pressure decreases as we go higher in altitude. Why? Essentially, there’s less atmosphere above you pushing down. Water boiling occurs when the vapor pressure of the water (or really any liquid) equals the surrounding atmospheric pressure. So these changes in atmospheric pressure directly affects the boiling point of water. Lower atmospheric pressure at higher altitudes means water boils at a lower temperature. The relationship between pressure (P) and boiling point (T) can be approximated using the Clausius-Clapeyron equation: \\[\\ln\\left(\\frac{P}{P_0}\\right) = -\\frac{\\Delta H}{R}\\left(\\frac{1}{T} - \\frac{1}{T_0}\\right)\\] Where: \\(P\\): The vapor pressure of the liquid at the temperature of interest (in Pascals, Pa). This is what we want to find to determine the boiling point. \\(P_0\\): The vapor pressure at a known reference temperature (also in Pascals). For water, we often use standard atmospheric pressure (101325 Pa) and its corresponding boiling point of 100°C (373.15 Kelvin). \\(\\Delta H\\): The enthalpy of vaporization (in Joules per mole, J/mol). This represents the energy needed to change one mole of liquid into vapor at a constant temperature. For water, \\(\\Delta H\\) is approximately 40,700 J/mol. \\(R\\): The ideal gas constant (8.314 J/mol·K). This constant relates energy to temperature for gases. \\(T\\): The temperature in Kelvin (K) at which we want to find the vapor pressure (and ultimately, the boiling point). \\(T_0\\): The reference temperature in Kelvin (K). Again, for water, this is often 373.15 K. Birmingham vs. Boulder: A Tale of Two Cities Birmingham, UK, sits at a relatively low altitude (around 140 meters above sea level). Water here boils pretty close to 100°C. Boulder, Colorado, on the other hand, is nestled in the foothills of the Rocky Mountains at an elevation of roughly 1655 meters. This significant difference in altitude has a significant impact on the temperature water boils at. To use the Clausius-Clapeyron equation we first need to calculate atmospheric pressure in Boulder - there are various ways of doing this (getting progressively more difficult) but here it is sufficient to assume a ‘standard atmosphere’ where temperature decreasesd with altitude, and assume that our coffee machine is being used indoors where the temperature is about 20°C, then we can use: \\[P = P_0\\cdot\\exp\\left(\\frac{-gM(h - h_0)}{RT}\\right)\\] Where: \\(P\\): The air pressure (Pa) at altitude \\(h\\) (this is the thing we want) \\(P_0\\): Air pressure at reference altitude \\(h_0\\) (as in the previous equation we will use sea level pressure of 101325 Pa) \\(g\\): Acceleration due to gravity (9.81 m/s²) \\(M\\): Molar mass of air (0.0289644 kg/mol) \\(h\\): Altitude (m) (Boulder is at 1,655m) \\(h_0\\): Reference altitude (m) (here we assume sea level, 0 m) \\(R\\): The universal gas constant (8.31432 J/(mol·K)) \\(T\\): Temperature at altitude \\(h\\) (K) (must be in Kelvin, and so we use 293.15 K) Plugging those numbers in gives us: \\[\\begin{eqnarray*} P \u0026amp;=\u0026amp; 101325\\times\\exp\\left(\\frac{-9.81\\times 0.0289644\\times (1655 - 0)}{8.31432\\times 293.15}\\right)\\\\ \u0026amp;=\u0026amp; 83546 \\end{eqnarray*}\\] So the atmospheric pressure of Boulder is about 83500 Pa (you can also use this online calculator to be more accurate if you want to be, and this also provides more details on air pressure caclulations). Sub this value in (as \\(P\\)) into the Clausius-Clapeyroin equation and using our other reference values: \\(P_0\\) (101325 Pa), \\(\\Delta H\\) (40700 J/mol), \\(R\\) (8.314 J/mol·K), and \\(T_0\\) (373.15 K) to get: \\[\\ln\\left(\\frac{83500}{101325}\\right) = -\\left(\\frac{40700}{8.314}\\right)\\times\\left(\\frac{1}{T} - \\frac{1}{373.15}\\right)\\] Solving the above equation for \\(T\\) requires a little bit of algebraic manipulation but you should end up with \\(T \\approx 367.7 K\\). Which (by removing 273.15) gives a value of 94.55°C for the boiling point of water for Boulder. Water boils at approximately 94.5°C in Boulder. Why It Matters for Coffee Coffee brewing is a delicate dance of temperature and extraction. Water acts as a solvent, pulling flavorful compounds from the coffee grounds. The ideal brewing temperature for most coffee lies between 90-96°C. But brewing in Boulder presents a unique challenge. With water boiling at a lower temperature, you have a smaller window to extract those desirable flavors before under-extraction occurs. This can result in a sour or weak cup of coffee. My tips for brewing at higher altitudes (but I am by no means an expert!) are: Grind finer: A finer grind increases the surface area of the coffee, allowing for better extraction at lower temperatures. Pre-infusion: Bloom your grounds with a small amount of hot water before brewing. This helps saturate the coffee evenly. Extend brew time: Slightly longer brew times can help compensate for the lower temperature." }, { "title": "All England football league results", "url": "/articles/2024/All_England_football_league_results/", "content": "This article describes a plain text database of all England football (soccer) league results from 1888 to the present day (covering over 209,000 matches). You can access the latest database on its dedicated github page: England-football-results The database is updated roughly every two days (although I am looking for approaches to speed this up) for the top four tiers in England: Premier League, EFL Championship, EFL League One and EFL League Two. The motivation for making the database is that I do a lot of statistical analysis on various bits and pieces in football (you can see some here), and not having an easy to read database really slows me down. The database is a comma (“,”) delimited csv file with the following columns: Column Details Date the day of the match (string; format “YYYY-MM-DD”) Season the season the match took place in (string; format “YYYY/YYYY”) HomeTeam the home team name (string) AwayTeam the away team name (string) Score the final score (string; format “X-Z”) hGoal number of goals scored by the home team (integer; “X” from the “Score” column) aGoal number of goals scored by the away team (integer; “Z” from the “Score” column) Division name of the division the match was played in (string) Tier numerical representation of the tier which the match was from: 1, 2, 3 or 4, where “1” is the top tier (currently the Premier League) (integer) Result the result “H” (home win), “A” (away win), “D” (draw) (string) The data from 1888 to 2016 is based on that from: James P. Curley (2016). engsoccerdata: English Soccer Data 1871-2016. http://dx.doi.org/10.5281/zenodo.13158 Such a long database of results leads to some confusion around team names, the answer to the most common set of questions I have received in terms of team names: Accrington F.C. is a different team to Accrington Stanley. Acrrington F.C. were one of the founder members of the Football League, but unfortunately were dissolved in 1896. Brighton \u0026amp; Hove Albion, New Brighton Tower and New Brighton are all different clubs. New Brighton Tower were in existence from 1896-1901 and whilst Brighton \u0026amp; Hove Albion were formed in 1901, the “spiritual” successor to New Brighton Tower, was New Brighton (1921-1983 and 1993-2012; originally formed by the relocation of South Liverpool) Burton Swifts, Wanderers, United, Town and Albion are all different teams. Burton Swifts joined with Wanderers to form Burton United in 1901, which in 1924 merged with Burton Town and in 1950 merged with the newly formed Burton Albion. Whilst Leeds Unitd were formed following/replacing Leeds City (and played in the same ground). No players or management from Leeds City moved to Leeds United so we treat them as separate football clubs. Middlesbrough Ironopolis (1889-1894) is separate team from Middlesbrough (1876-). Rotherham County merged with Rotherham Town in 1925 to form Rotherham United. Wigan Athletic were formed (1932) a year after Wigan Borough were wound up (1931) and we treat them separately. Wigan Athletic was the sixth attempt to create a stable football club in Wigan following the dissolving of Wigan A.F.C., County (1897-1900), United (1896-1914), Town (1905-1908) and Borough (1920-1931). Hopefully there are lots of fun things you can do with this, please let me know about any of them! A couple of simple examples: You can use the form on this site to work out a league table for any given season and division or for any arbitrary date range, from 1888 to present (remembering before 1981 there was only 2 points for a win in English football): https://seanelvidge.com/leaguetable To find the Head-to-Head statistics of any two clubs who have played in the English Football League visit: https://seanelvidge.com/h2h This url can have the team names passed directly into the url, e.g. to get the head-to-head statistics of Arsenal v Chelsea visit: https://seanelvidge.com/h2h?team1=Arsenal\u0026amp;team2=Chelsea" }, { "title": "How aerodynamic is Santa", "url": "/articles/2024/How_aerodynamic_is_Santa/", "content": "Santa is at the top of multiple engineering disciplines (see his production line for presents and ability to deliver them all). However one that is often overlooked is the space-domain. The United States’ North American Aerospace Defense Command (NORAD) not only continuously tracks 32,000 objects in space to avoid collisions, but also Santa’s global progress in delivering presents on Christmas Eve (https://www.noradsanta.org/en/). From NORAD’s tracks, we can see that he travels at 6.25 km/s (14,000 mph) over oceans (close to the speed of satellites in Low Earth Orbit at 7.7 km/s). However, he travels more slowly, at only 1.4 km/s (3,000 mph) while over the densely populated UK. His route perfectly optimised for reducing atmospheric drag. Whilst travelling over oceans he doesn’t need to be at low altitudes (no presents to deliver) so he takes advantage of the thinner atmosphere at higher altitudes. Also, delivering at night-time allows him to both avoid sleeping children and at the same time take advantage of the lower thermospheric densities, as the thermosphere cools and contracts while the Sun isn’t heating it. However, over populated areas, he travels more slowly in the lower, denser atmosphere, allowing more accurate present delivery while experiencing less heating due to the atmospheric drag. Another optimization Santa has made is in the design of his Sleigh and selection of Reindeer as the method of driving the Sleigh. In the atmospheric drag equation, there is a term known as the coefficient of drag, effectively how well something slips through the atmosphere. Smaller, pointier objects have lower coefficients of drag, think about arrows compared to parachutes. Reindeer have pointy snouts and present a relatively low cross-sectional area for their strength, allowing them to convert their power to speed. Meanwhile having nine reindeer in a 1-2-2-2-2 formation helps to efficiently cut into the air, and make it turbulent before the sleigh travels through, making it experience less drag, just like a cyclist in a peloton. However, Rudolph being at the front takes the brunt of this force due to atmospheric drag, with the majority of the kinetic energy turning into heat energy, leading to his red nose. Whilst we are not sure if Santa has yet upgraded his Sleigh to using the latest SANTA-NAV (GPS) technology, or whether he uses HF communications to keep in touch with the North Pole, we do know that on Christmas Eve, and throughout the Christmas period, the Space Environment research group at the University of Birmingham (SERENE) will be working with space weather centres around the world to help deliver 24/7 forecasting of the conditions in Earth’s upper atmosphere and its impacts on global position systems, HF communications, atmospheric drag and the need for satellite manoeuvres." }, { "title": "Levelling the Playing Field, Adjusting Goal Records in International Football", "url": "/articles/2024/Adjusted_England_Goals/", "content": "In recent years, we’ve witnessed a surge in international football records being shattered. Modern players are scoring at unprecedented rates, and while this is undoubtedly exciting for fans, it raises an important question: Are today’s goal-scoring feats truly comparable to those of past legends? The landscape of international football has evolved significantly. The expansion of FIFA membership has introduced many new, smaller nations into the competitive arena. This influx has led to more matches where traditional football powerhouses face off against developing teams, often resulting in lopsided scorelines. Consequently, contemporary players have more opportunities to inflate their goal tallies against weaker opposition. This shift poses a challenge when attempting to compare the goal-scoring records of current players with those from previous eras. The legends of the past often played fewer matches against a more consistent level of competition, making their goal tallies a reflection of performances against relatively equal opponents. So, how can we level the playing field and make fair comparisons across different football eras? Introducing a Weighted Goal System To address this disparity, I embarked on a project to adjust goal records by accounting for the relative strength of the opposition. The idea is to assign a weighting to each goal based on the difficulty of scoring against a particular team at the time of the match. This method aims to provide a more nuanced evaluation of a player’s goal-scoring achievements. Utilizing Elo Ratings The foundation of this approach lies in the Elo rating system, a method originally devised for ranking chess players but now widely adopted across various sports, including football. Elo ratings provide a dynamic measure of a team’s strength, updating after each match based on the result and the quality of the opposition. By leveraging historical Elo ratings, we can assess the relative strength of any two teams at the time they played. This allows us to calculate a weighting factor for each goal scored, reflecting the challenge posed by the opponent. The Weighting Formula The weighting for each goal is determined using the following formula: \\[\\text{Weight} = 1 - k\\times\\frac{E-O}{E}\\] Where, \\(E\\) is the Elo rating of England before the match, \\(O\\) is the Elo rating of the opposition before the match and \\(k\\) a scaling constant that adjusts the sensitivity of the weighting to the difference in Elo ratings. This formula adjusts the weight of a goal based on how much stronger or weaker the opposition is relative to England: If England faces a stronger team (\\(O \u0026gt; E\\)), the weighting increases above 1, acknowledging the greater difficulty. If England faces a weaker team (\\(O \u0026lt; E\\)), the weighting decreases below 1, reflecting the relatively easier challenge. The constant \\(k\\) controls how much the difference in ratings affects the weighting. Interpreting the Weighting Adjusting the \\(k\\) value alters the impact of the opposition’s strength: A higher \\(k\\) value (e.g., 2) amplifies the effect, giving more weight to goals against stronger teams and less to those against weaker ones. A lower \\(k\\) value (e.g., 1) minimizes the effect, resulting in a more uniform weighting across different opponents. By experimenting with different \\(k\\) values, we can fine-tune the system to balance fairness and sensitivity, ensuring that exceptional performances against top-tier teams are appropriately recognized while maintaining a reasonable value for consistent scoring against all opponents. For the rest of this post we use a value of \\(k=2\\). Results When I last wrote a post similar to this, Wayne Rooney had just broke the England National team goal record with 50 goals. Since then Kane has broke this record again (at the time of writing) with 69 goals. Using the above approach the updated top 10 of England goal scorers are (the “Ranking” is based on the weighted goals column): Ranking Name Goals Weighted Goals 1 Harry Kane 69 45 2 Gary Lineker 48 37 3 Bobby Charlton 49 36 4 Jimmy Greaves 44 35 5 Wayne Rooney 53 34 6 Michael Owen 40 30 7 Alan Shearer 30 23 8 Tom Finney 30 23 9 Nat Lofthouse 30 22 10 Frank Lampard 29 21 You can see that whilst Kane is still number 1 - his number of goals are significantly less. (Note that the adjusted number of goals here have been rounded to the nearest goal for ease of reading). As may be expected Kane has the biggest change between actual goals scored (69) and the weighted number of goals (45) with a difference of 24, perphaps reaffirming the theory that modern day players are scoring a significant amount of their goals against weaker opposition than in the past. Gary Lineker moves from 4th in the all time list to 2nd. At the other end of the table we end up with a number of people (41) who (because of the rounding as well) move from having scored at least 1 for England to being on 0 goals, but only four who go from more than 1 to zero: Name Goals Weighted Goals Tammy Abraham 3 0 Paul Ince 2 0 Tyrone Mings 2 0 James Ward-Prowse 2 0 Unfortuntaely Tammy Abraham’s 3 goals for England were scored against very much weaker opposition in the form of Montenegro (England won 7-0 on 14/11/19), Andorra (England won 0-5 on 09/10/21) and San Marino (England won 0-10 on 15/11/21). If you want to have a look at the data you can access it all here) (which also includes the “Adjusted Total” which weights Friendlies as 0.5 and “finals” as 2x (as per the description here). Data If you want to have a look at the data in a little more detail (but without downloading the whole datefile above) the below is a sortable table for England players with at least 40 caps): NameCapsTotal GoalsGoals per CapWeighted GoalsWeighted Goals per Cap Peter Shilton12500.0000.00 Wayne Rooney120530.44340.28 David Beckham115170.15120.10 Steven Gerrard114210.18130.11 Bobby Moore10820.0210.01 Ashley Cole10700.0000.00 Bobby Charlton106490.46360.34 Frank Lampard106290.27210.20 Billy Wright10530.0320.02 Harry Kane103690.67450.44 Kyle Walker9310.0110.01 Bryan Robson90260.29180.20 Michael Owen89400.45300.34 Kenny Sansom8610.0110.01 Gary Neville8500.0000.00 Ray Wilkins8430.0430.04 John Stones8330.0420.02 Raheem Sterling82200.24140.17 Rio Ferdinand8130.0420.02 Jordan Henderson8130.0420.02 Gary Lineker80480.60370.46 John Barnes79110.1480.10 John Terry7860.0850.06 Stuart Pearce7850.0640.05 Terry Butcher7730.0420.03 Tom Finney76300.39230.30 David Seaman7500.0000.00 Joe Hart7500.0000.00 Sol Campbell7310.0110.01 Jordan Pickford7300.0000.00 Gordon Banks7300.0000.00 Alan Ball7280.1160.08 Martin Peters67200.30140.21 Paul Scholes66140.21110.17 Tony Adams6650.0840.06 Dave Watson6540.0630.05 Harry Maguire6470.1140.06 Alan Shearer63300.48230.37 Kevin Keegan63210.33150.24 Ray Wilson6300.0000.00 David Platt62270.44200.32 Emile Heskey6270.1150.08 Chris Waddle6260.1050.08 Declan Rice6250.0840.06 Emlyn Hughes6210.0210.02 Gary Cahill6150.0840.07 James Milner6110.0200.00 Ray Clemence6100.0000.00 Marcus Rashford60170.28140.23 Peter Beardsley5990.1570.12 Des Walker5900.0000.00 Phil Neville5900.0000.00 Jimmy Greaves57440.77350.61 Jermain Defoe57200.35100.18 Paul Gascoigne57100.1870.12 Gareth Southgate5720.0410.02 Johnny Haynes56180.32140.25 Joe Cole56100.1860.11 Stanley Matthews54110.2090.17 Kieran Trippier5410.0210.02 Glen Johnson5410.0210.02 Glenn Hoddle5380.1560.11 Gareth Barry5330.0620.04 Paul Ince5320.0400.00 David James5300.0000.00 Trevor Francis52120.2390.17 Teddy Sheringham51110.2290.18 Phil Neal5050.1030.06 Geoff Hurst49240.49180.37 Ron Flowers49100.2080.16 Eric Dier4930.0630.06 Colin Bell4890.1980.17 Jimmy Dickinson4800.0000.00 Theo Walcott4780.1750.11 Trevor Brooking4750.1150.11 Mick Channon46210.46170.37 Gary Stevens4600.0000.00 Mark Wright4510.0210.02 Bukayo Saka43120.2860.14 Phil Foden4340.0930.07 Martin Keown4320.0510.02 Chris Woods4300.0000.00 Jimmy Armfield4300.0000.00 Peter Crouch42220.52140.33 Danny Welbeck42160.3890.21 Tony Woodcock42160.38110.26 Steve Coppell4270.1750.12 Phil Thompson4210.0210.02 David Batty4200.0000.00 Owen Hargreaves4200.0000.00 Mick Mills4200.0000.00 Paul Robinson4100.0000.00 Bob Crompton4100.0000.00 Jude Bellingham4060.1550.13 Phil Jagielka4030.0720.05" }, { "title": "Solar Storms are Like Buses You Wait 20 Years for One, and Then Two Come at Once!", "url": "/articles/2024/Solar_storms_are_like_buses/", "content": "This was the second time this year they have been broadly visible across the UK, with the last major display occurring on the evening of May 10th, 2024. For many, seeing these incredible lights twice in a single year is almost unheard of, and for good reason: before 2024, the UK hadn’t experienced a space weather event of these magnitudes, in over two decades. What has caused this sudden surge in activity? The answer lies in the Sun’s solar cycle, which is rapidly approaching the peak of its activity, and, just like waiting for a bus, after 20 years of relative calm, the UK has now seen two major solar storms in the space of six months. Solar Maximum: The Driver Behind the Lights At the heart of this increased auroral activity is the Sun itself. The Sun goes through cycles of roughly 11 years, moving from periods of quiet (solar minimum) to periods of high activity (solar maximum). We’re currently approaching solar maximum, and this is why we’re seeing more frequent and powerful solar storms. These storms occur when the Sun releases huge quantities matter and magnetic field in the form of a coronal mass ejection (CME) which are also very often associated with solar flares. When these high-speed and high-energy CMEs reach Earth, they can interact and disturb our planet’s magnetic field, causing the stunning lights of the aurora. During solar maximum, the Sun produces more of these storms, leading to auroras that are brighter, more widespread, and visible at lower latitudes—like the UK. What Makes These Storms So Special? Solar storms are typically classified according to the NOAA Space Weather Scale which, for geomagnetic storms, run from G1 (minor) to G5 (extreme), whilst the storm back in May was classified as a G5 (extreme) storm last night the storm peaked at a G4 (severe) event. For context, the last time the UK saw storms of this ‘size’ was over 20 years ago, in October 2003. What makes these storms particularly fascinating is that they push the auroral oval, the region around the poles where the Northern Lights occur, further south. Normally, the UK is too far from the poles to regularly see auroras, but during a G4 or G5 storm, the lights can be visible much further south than the usually are. The Smartphone Effect However this was still a relatively mild storm, and can be expected to occur on average about once every 3 or 4 years. One thing that has been particularly different this time around (for both last night and the storm in May), however, is the number of pictures flooding social media. Thanks to the widespread use of smartphones equipped with high-quality cameras, capturing the aurora has never been easier. Modern smartphones now have the ability to take long-exposure shots, which allows them to capture the faint light of the aurora in ways the naked eye cannot. As a result, these natural light shows are being documented and shared like never before, over the coming months and year it may make it feel as if the auroras are more frequent than they might have seemed during previous solar cycles. SERENE: Understanding Space Weather While the visual spectacle of an aurora is captivating, there’s serious science behind these storms. The Space Environment and Radio Engineering (SERENE) group at the University of Birmingham is at the forefront of research into space weather and its effects on Earth. As a world-class research team, SERENE focuses on understanding the impacts of solar storms and helping society prepare for the disruptions they can cause. Space weather doesn’t just create auroras; it can also affect critical systems such as communications, GPS, satellite orbits and even the power grid. SERENE’s work is vital for mitigating the risks posed by these powerful geomagnetic storms. Their research is ensuring that as we enjoy the beauty of the Northern Lights, we’re also better equipped to protect the technology that supports modern life. Looking Ahead: More Auroras to Come? As we approach the peak of the solar cycle, we can expect more opportunities to witness auroras in the UK. Solar storms will continue to erupt from the Sun and head toward Earth, and if the conditions are right (including cloud-free skies!), we could see even more stunning displays over the next couple of years. For those eager to catch the lights, staying tuned to space weather forecasts during this period of increased activity is key. The next time you see those vibrant reds and greens shimmering in the night sky, you’ll know that it’s not just a stroke of luck - it’s the Sun, cycling through its periodic bursts of activity. And thanks to research groups like SERENE, we can enjoy the beauty of the aurora with the peace of mind that we’re ready for whatever space weather may bring. Originally available from: https://www.birmingham.ac.uk/news/2024/solar-storms-are-like-buses-you-wait-20-years-for-one-and-then-two-come-at-once" }, { "title": "Rules of Acquisition", "url": "/articles/2024/Rules_of_Acquisition/", "content": "I am a big Star Trek fan and also have been enjoying the recent explosion of Large Language Models (LLMs), so I thought I would come up with a way they could be combined. The ‘Rules of Acquisition’ are a collection of sayings of which the Ferengi society is based: Every Ferengi business transaction is governed by these rules to ensure a fair and honest deal for all parties concerned. (DS9: “The Maquis, Part I”) According to various sources, by the mid-22nd century there were 173 rules (ENT: “Acquisition”), by the 2370s there were 285 (DS9: “Rules of Acquisition”, “Body Parts”; VOY: “False Profits”), and 289 by the 2380s (LD: “Old Friends, New Planets”). The exact rules appear fairly randomly throughout the Star Trek canon, scattered throughout a number of episodes (predominatly in Star Trek Deep Space 9). Two excellent wikis exist to document the vast Star Trek universe, Memory Alpha (using information from all released television series and feature films) and Memory Beta (using information from all TV, films and licensed Star Trek works, including novels, comic books, RPG sourcebooks, video games and any other licensed works). In the “primary” canon, 52 of the 289 rules were defined and in the extended canon 144 of the 285 rules were defined (although Rule #2 is defined twice, in different ways). In September 2024 OpenAI released their “o1-preview” model which was “trained to spend more time thinking through problems before they respond, much like a person would. Through training, they learn to refine their thinking process, try different strategies, and recognize their mistakes”. It seems like a very powerful model so I decided to put it to the test… I fed it the complete biographical information of the Ferengi’s, their biology, psychology (greed and self preservation), societal information and culture (profit and loss, beliefs, rituals and diplomacy) which are well defined, and referenced here. Along with that I provided the 144 existing rules from the wider canon and asked it to fill in the missing rules. And what a wonderful job it did! The complete list is below, merging the ChatGPT-derived rules along with the existing canon (with those by ChatGPT clearly highlighted) but some of my favourites it came up with are: #12 “Anything worth doing is worth doing for profit.” #24: “Never mix business with pleasure unless it enhances profit.” #28: “Morality is always negotiable in the pursuit of profit.” #80: “If it works, sell it; if it doesn’t, sell it as an antique.” #96: “For every opportunity, there’s an equal and opposite opportunity—exploit both.” #114: “The small print holds the biggest profits.” #132: “A lie isn’t a lie until someone else knows the truth.” #161: “Never work for free, unless there’s future profit.” Complete List of the Rules of Acquisition Note: if the source is not ChatGPT then these rules come from the Ferengi Rules of Acquisition by Memory Beta, which contains more details including links to the source material/epsiode guide. Rule Number Rule Source 1 Once you have their money... you never give it back. DS9 episode: \"The Nagus\", PRD episode: \"First Con-tact\" 2 Money is everything. ST - Strange New Worlds 9 short story: \"The Last Tree on Ferenginar: A Ferengi Fable From the Future\" 3 Never spend more for an acquisition than you have to. DS9 episode: \"The Maquis, Part II\" 4 Sedition and treason are always profitable. ST video game: Star Trek Online 5 Always exaggerate your estimates. SCE eBook: Cold Fusion 6 Never allow family law stand in the way of opportunity. DS9 episode: \"The Nagus\" 7 Keep your ears open. DS9 episode: \"In the Hands of the Prophets\" 8 Small print leads to large risk. DS9 reference: The Ferengi Rules of Acquisition 9 Opportunity plus instinct equals profit. DS9 episode: \"The Storyteller\" 10 Greed is eternal. DS9 episode: \"Prophet Motive\" 11 Opportunity waits for no one—seize it quickly. ChatGPT 12 Anything worth doing is worth doing for profit. ChatGPT 13 Anything worth doing is worth doing for money. Legends of the Ferengi (DS9 novel) 14 Sometimes the quickest way to find profits is to let them find you. Fortune of War (TTN novel) 15 Dead men close no deals. Demons of Air and Darkness (DS9 novel) 16 A deal is a deal... until a better one comes along. The Ferengi Rules of Acquisition (DS9 reference book) 17 A contract is a contract is a contract... but only between Ferengi DS9 episode: \"Body Parts\" 18 A Ferengi without profit is no Ferengi at all. DS9 episode: \"Heart of Stone\", DS9 - The Dog of War comic: \"Issue 3\" 19 Satisfaction is not guaranteed. Legends of the Ferengi (DS9 novel) 20 He who dives under the table today lives to profit tomorrow. Ferenginar: Satisfaction is Not Guaranteed (DS9 novella) 21 Never place friendship above profit. DS9 episode: \"Rules of Acquisition\", PRD episode: \"First Con-tact\" 22 A wise man can hear profit in the wind. DS9 episode: \"Rules of Acquisition\" 23 Nothing is more important than your health... except for your money. ENT episode: \"Acquisition\" 24 Never mix business with pleasure unless it enhances profit. ChatGPT 25 There's no profit in altruism. ChatGPT 26 Customers are like latinum; treasure them and they'll return. ChatGPT 27 There's nothing more dangerous than an honest businessman. Legends of the Ferengi (DS9 novel) 28 Morality is always negotiable in the pursuit of profit. ChatGPT 29 What's in it for me? Highest Score (DS9 novel) 30 \"Confidentiality equals profit.\" DS9 - The Badlands novel: Part IV 31 Never make fun of a Ferengi's mother. Insult something he cares about instead. DS9 episode: \"The Siege\", Elite Force II 32 Enemies are potential customers—never burn a bridge. ChatGPT 33 It never hurts to suck up to the boss. DS9 episode: \"Rules of Acquisition\" 34 War is good for business. DS9 episode: \"Destiny\"; The 34th Rule (DS9 novel) 35 Peace is good for business. DS9 episode: \"Destiny\" 36 Too many partners dilute the profit. ChatGPT 37 The early investor reaps the most interest. ST novella: Reservoir Ferengi 38 Knowledge is power; power yields profit. ChatGPT 39 Don't tell customers more than they need to know. Ascendance (DS9 novel) 40 She can touch your lobes but never your latinum. The Ferengi Rules of Acquisition (DS9 reference book) 41 Profit is its own reward. The Ferengi Rules of Acquisition (DS9 reference book) 42 What's mine is mine, and what's yours is negotiable. ChatGPT 43 Feed your greed, but not enough to choke it. The Buried Age (TNG novel) 44 Never confuse wisdom with luck. The Ferengi Rules of Acquisition (DS9 reference book) 45 Expand or die. ENT episode: \"Acquisition\" 46 Make your own opportunities when none arise. ChatGPT 47 Don't trust a man wearing a better suit than your own. DS9 episode: \"Rivals\" 48 The bigger the smile, the sharper the knife. DS9 episode: \"Rules of Acquisition\" 49 Age and treachery will always overcome youth and skill. ChatGPT 50 Gratitude is a profitable emotion—encourage it in others. ChatGPT 51 Reward those who contribute to your wealth. ChatGPT 52 Never ask when you can take. The Ferengi Rules of Acquisition (DS9 reference book) 53 Never trust anybody taller than you. Mission Gamma: Twilight (DS9 novel) 54 Rate divided by time equals profit. (Also known as \"The Velocity of Wealth.\") Raise the Dawn (Typhon Pact novel) 55 Take joy from profit, and profit from joy. Worlds of Deep Space Nine: Bajor: Fragments and Omens (DS9 novel) 56 Only negotiate when you're certain to profit. ChatGPT 57 Good customers are as rare as latinum—treasure them. DS9 episode: \"Armageddon Game\" 58 There is no substitute for success. The Ferengi Rules of Acquisition (DS9 reference book) 59 Free advice is seldom cheap. DS9 episode: \"Rules of Acquisition\" 60 Keep your lies consistent. The Ferengi Rules of Acquisition (DS9 reference book) 61 Never buy what you can acquire by other means. ChatGPT 62 The riskier the road, the greater the profit. DS9 episode: \"Rules of Acquisition\" 63 Work is the best therapy-at least for your employees. \"Over a Torrent Sea\" (TTN novel) 64 The customer is always right—until you get their money. ChatGPT 65 Win or lose, there's always Hupyrian beetle snuff The Ferengi Rules of Acquisition (DS9 reference book) 66 Someone's always got bigger ears. What Happens Now (Star Trek: The Last Generation #3, TNG comic) 67 Breaking even is a sign of failure. ChatGPT 68 Risk doesn't always equal reward. Star Trek Online 69 Ferengi are not responsible for the stupidity of other races. Balance of Power (TNG novel) 70 Secure the payment before delivering the product. ChatGPT 71 Close the deal first; details can be settled later. ChatGPT 72 Trust is a commodity best sold, not given. ChatGPT 73 Time spent talking is time away from profit. ChatGPT 74 Knowledge equals profit. VOY episode: \"Inside Man\" 75 Home is where the heart is... but the stars are made of latinum. DS9 episode: \"Civil Defense\" 76 Every once in a while, declare peace. It confuses the hell out of your enemies. DS9 episode: \"The Homecoming\" 77 If you break it, I'll charge you for it! Star Trek Online 78 When the boss is wrong, pretend to agree; then profit from his mistake. ChatGPT 79 Beware of the Vulcan greed for knowledge. The Ferengi Rules of Acquisition (DS9 reference book) 80 If it works, sell it; if it doesn't, sell it as 'vintage'. ChatGPT 81 There's a customer born every minute—find them. ChatGPT 82 The flimsier the product, the higher the price. The Ferengi Rules of Acquisition (DS9 reference book) 83 Quick profit is the best profit. ChatGPT 84 A friend is a customer waiting to happen. ChatGPT 85 Never let the competition know what you're thinking. The Ferengi Rules of Acquisition (DS9 reference book) 86 Sentiment is the enemy of profit. ChatGPT 87 Learn the customer's weaknesses, so that you can better take advantage of him. Highest Score (DS9 novel) 88 It ain't over 'til its over. Ferenginar: Satisfaction is Not Guaranteed (DS9 novella) 88 Vengeance will cost you everything. The Poisoned Chalice (DS9 novel) 89 Ask not what your profits can do for you, but what you can do for your profits. The Ferengi Rules of Acquisition (DS9 reference book) 89 [It is] better to lose some profit and live than lose all profit and die. \"Best Tools Available\" (DS9 short story) 90 Never tell the truth when a good lie will do. ChatGPT 91 Your boss is only worth what he pays you. \"Old Friends, New Planets\" 92 There are many paths to profit. Highest Score (DS9 novel) 93 Latinum lasts longer than wisdom. ChatGPT 94 Females and finances don't mix. DS9 episode: \"Ferengi Love Songs\" 95 Expand or die. DS9 novel: Legends of the Ferengi, VOY episode: \"False Profits\" 96 For every opportunity, there's an equal and opposite opportunity—exploit both. ChatGPT 97 Enough... is never enough. The Ferengi Rules of Acquisition (DS9 reference book) 98 Every man has his price. DS9 episode: \"In the Pale Moonlight\" 98 If you can't take it with you, don't go. I, Q (DS9 novel) 99 Trust is the biggest liability of all. The Ferengi Rules of Acquisition (DS9 reference book) 100 When it's good for business, tell the truth. Ascendance (DS9 novel) 101 Profit trumps emotion. The Long Mirage (DS9 novel) 102 Nature decays, but latinum lasts forever. DS9 episode: \"The Jem'Hadar\" 103 Sleep can interfere with opportunity. DS9 episode: \"Rules of Acquisition\" 104 Faith moves mountains... of inventory. The Ferengi Rules of Acquisition (DS9 reference book) 105 Don't trust anyone who trusts too easily. ChatGPT 106 There is no honor in poverty. Sacraments of Fire (DS9 novel) 107 First to the market gets the biggest profit. ChatGPT 108 A woman wearing clothes is like a man without any profits. DS9 Novel 108 Hope doesn't keep the lights on. The Ferengi Rules of Acquisition (DS9 reference book) 109 Dignity and an empty sack is worth the sack. DS9 episode: \"Rivals\" 110 Exploitation begins at home. VOY episode: \"False Profits\" 111 Treat people in your debt like family... exploit them. DS9 episode: \"Past Tense, Part I\" 112 Never have sex with the boss' sister. DS9 episode: \"Playing God\" 113 Always have sex with the boss. The Ferengi Rules of Acquisition (DS9 reference book) 114 The small print holds the biggest profits. ChatGPT 115 Let nothing distract you from the pursuit of profit. ChatGPT 116 There's always a loophole—find it. ChatGPT 117 You can't free a fish from water. The Ferengi Rules of Acquisition (DS9 reference book) 118 Never settle for less than your worth. ChatGPT 119 A customer's appearance can be deceiving; assess their wealth carefully. ChatGPT 121 Everything is for sale, even friendship. The Ferengi Rules of Acquisition (DS9 reference book) 122 Never Sleep with the bosses sister \"Playing God\" (DS9 episode) 123 Even a blind man can recognize the glow of Latinum. The Ferengi Rules of Acquisition (DS9 reference book) 124 Let others keep their pride; you keep their latinum. ChatGPT 125 You can't make a deal if you're dead. \"The Siege of AR-558\" (DS9 episode) 126 Double-check your profits. ChatGPT 127 Neutrality sells to all sides. ChatGPT 129 Fear of loss motivates customers—use it. ChatGPT 130 Opportunities are never missed—only seized by others. ChatGPT 131 Information is the greatest commodity. ChatGPT 132 A lie is as profitable as the truth. ChatGPT 133 Blame shrewdly to protect your interests. ChatGPT 134 Profit above all else, even friendship. ChatGPT 135 Listen to secrets, but never repeat them. Ascendance (DS9 novel) 136 Seek profit in every situation. ChatGPT 137 The closer to the source, the greater the profit. ChatGPT 138 Everything is negotiable—absolutely everything. ChatGPT 139 Wives serve, brothers inherit. \"Necessary Evil\" (DS9 episode) 140 Your profit margin is the true measure of success. ChatGPT 141 Only fools pay retail. The Ferengi Rules of Acquisition (DS9 reference book) 142 You can never know too much about your customers. ChatGPT 143 Risk is the price of opportunity. ChatGPT 144 There's nothing wrong with charity... as long as it winds up in your pocket. The Ferengi Rules of Acquisition (DS9 reference book) 145 A convincing lie is as good as the truth. ChatGPT 146 Necessity breeds profit. ChatGPT 147 People love the bartender. Fearful Symmetry (DS9 novel) 148 Always negotiate from a position of strength. ChatGPT 149 Everything has a price—find it. ChatGPT 150 Respect is good; latinum is better. ChatGPT 151 Even when you're a customer, sell yourself. The Long Mirage (DS9 novel) 153 Sell the sizzle, not the steak. \"Deep Space Mine\" (DS9 comic) 154 Pain is temporary; profit is eternal. ChatGPT 155 Every being is a potential customer. ChatGPT 156 Don't invest in what you don't understand. ChatGPT 158 Fair play is seldom profitable. ChatGPT 161 Never work for free unless there's future profit. ChatGPT 162 Even in the worst of times someone turns a profit. The Ferengi Rules of Acquisition (DS9 reference book), DS9 video game: Harbinger 163 The secret of business is to know something others don't. ChatGPT 164 Use others' resources whenever possible. ChatGPT 166 When profits wane, raise your prices. ChatGPT 167 Time is latinum; don't waste either. ChatGPT 168 Whisper your way to success. \"Treachery, Faith, and the Great River\" (DS9 episode) 169 Competition breeds innovation—and profit. ChatGPT 170 Secure the contract before revealing the cost. ChatGPT 171 Family is important, but profit is paramount. ChatGPT 172 Desire fuels profit. ChatGPT 173 A deal in hand is worth two in negotiation. ChatGPT 174 If it sells once, it will sell again. ChatGPT 175 Value is determined by what someone will pay. ChatGPT 176 Never reveal your profit margin. ChatGPT 177 Know your enemies... but do business with them always. The Ferengi Rules of Acquisition (DS9 reference book) 178 The customer is always right—especially when they're wrong. ChatGPT 180 A bribe opens more doors than an honest plea. ChatGPT 181 Not even dishonesty can tarnish the shine of profit. The Ferengi Rules of Acquisition (DS9 reference book) 182 Details are profitable; read the fine print. ChatGPT 183 When life hands you ungaberries, make detergent. Hollow Men (DS9 novel) 184 A Ferengi waits to bid until his opponents have exhausted themselves. Balance of Power (TNG novel) 185 Every rose has its profit. ChatGPT 187 Profit often hides in unlikely places. ChatGPT 188 Not even dishonesty can tarnish the shine of profit. Star Trek Online 189 Let others keep their reputation. You keep their money. The Ferengi Rules of Acquisition (DS9 reference book) 190 Hear all, trust nothing. DS9 episode: \"Call to Arms\" 191 Neither time nor latinum waits for anyone. ChatGPT 192 Never cheat a Klingon... unless you're sure you can get away with it. The Ferengi Rules of Acquisition (DS9 reference book) 193 It's never too late to fire the staff. Cathedral (DS9 novel) 193 Trouble comes in threes. Star Trek Online 194 It's always good business to know about new customers before they walk in your door. \"Whispers\" (DS9 episode) 195 Watch your expenses; small leaks sink big ships. ChatGPT 197 An inventive excuse can nullify a bad contract. ChatGPT 198 Every bargain has a hidden cost. ChatGPT 199 Location, location, location. The Soul Key (DS9 novel) 200 A Ferengi chooses no side but his own. (DS9 novel: Ferenginar: Satisfaction is Not Guaranteed) 201 Diversify your investments. ChatGPT 202 The justification for profit is profit. The Ferengi Rules of Acquisition (DS9 reference book) 203 New customers are like razor-toothed gree worms. They can be succulent, but sometimes they bite back. DS9 episode: \"Little Green Men\" 204 Only a Ferengi truly understands another Ferengi. ChatGPT 205 Without customers, there is no profit. ChatGPT 207 A Tribble Always Means Customer Satisfaction Adventures RPG: Tribble Player Character Supplement 208 Sometimes, the only thing more dangerous than a question is an answer. DS9 episode: \"Ferengi Love Songs\" 209 An empty account can't fund ventures. ChatGPT 210 Deep pockets attract deep profits. ChatGPT 211 Employees are the rungs on the ladder of success. Don't hesitate to step on them. DS9 episode: \"Bar Association\" 212 A good lie is easier to believe than the truth. Star Trek Online 213 Personal relations should serve business goals. ChatGPT 214 Never begin a (business) negotiation on an empty stomach. DS9 episode: \"The Maquis, Part I\" 215 Contracts between friends must be written doubly tight. ChatGPT 216 Never gamble with a telepath. DS9 novel: The Laertian Gamble 217 You can't free a fish from water. \"Past Tense, Part I\" (DS9 episode) 218 Sometimes what you get free costs entirely too much. Baby on Board (DS9 Malibu Comics) 219 Possession is eleven-tenths of the law! TNG novel: Balance of Power 220 Those who own nothing have nothing to lose. ChatGPT 221 Latinum can't buy happiness, but it can lease it. ChatGPT 222 Knowledge is profit. ChatGPT 223 Beware the man who doesn't take time for Oo-mox. The Ferengi Rules of Acquisition (DS9 reference book) 224 Your greatest enemy could be your greatest asset. ChatGPT 225 Profit is the noblest pursuit. ChatGPT 226 Honesty is a luxury for those who can afford it. ChatGPT 227 If that's what's written, then that's what's written. Star Trek Online 228 A satisfied customer is good; a dependent customer is better. ChatGPT 229 Latinum lasts longer than lust. DS9 episode: \"Ferengi Love Songs\" 230 Opportunities expire—act swiftly. ChatGPT 231 When profits are at stake, listen carefully. ChatGPT 232 Need creates demand; demand creates profit. ChatGPT 233 Find the need, then fill it—for a price. ChatGPT 234 Latinum can't buy love, but it can rent it for a while. ChatGPT 235 Duck; death is tall. Mission Gamma: Twilight (DS9 novel) 236 You can't buy fate. The Ferengi Rules of Acquisition (DS9 reference book) 237 Negotiation is preferable to confrontation. ChatGPT 238 Another's honor can be your profit. ChatGPT 239 Never be afraid to mislabel a product. DS9 episode: \"Body Parts\" 240 Time, like latinum, is a highly limited commodity. Star Trek Online 241 Greed is a universal constant—capitalize on it. ChatGPT 242 More is good... all is better. The Ferengi Rules of Acquisition (DS9 reference book) 243 Always leave yourself an out. Sacraments of Fire (DS9 novel) 244 Time invested wisely yields profit. ChatGPT 245 Too many customers? Raise your prices. ChatGPT 246 Secrecy is profitable. ChatGPT 247 A customer's deception is a seller's opportunity. ChatGPT 248 The definition of insanity is trying the same failed scheme \u0026amp; expecting different results Department of Temporal Investigations 3: Shield of the Gods (novel) pg 11 249 Bankruptcy can be a strategic move. ChatGPT 250 There's always profit in chaos. ChatGPT 251 Guarantees are for the seller, not the buyer. ChatGPT 252 If you can't sell it, don't buy it. ChatGPT 253 Beware of beneficiaries bearing gifts. ChatGPT 254 Maximize profit with minimal effort. ChatGPT 255 A wife is luxury... a smart accountant a neccessity. The Ferengi Rules of Acquisition (DS9 reference book) 256 A genuine smile can close a deal. ChatGPT 257 When the messenger comes to appropriate your profits, kill the messenger. False Profits (VOY episode) 258 Family ties are secondary to profit opportunities. ChatGPT 259 Opportunity, not intelligence, leads to wealth. ChatGPT 260 Life is finite; profit is infinite. ChatGPT 261 A wealthy man can afford anything except a conscience. The Ferengi Rules of Acquisition (DS9 reference book) 263 Never allow doubt to tarnish your lust for latinum. DS9 episode: \"Bar Association\" 264 A friendly façade can conceal sharp intent. ChatGPT 265 Trust no one wearing better shoes than you. ChatGPT 266 When in doubt, lie. The Ferengi Rules of Acquisition (DS9 reference book) 267 If you believe it, they believe it. Taking Wing (DS9 novel) 268 When uncertain, charge extra. ChatGPT 269 It's easier to ask forgiveness than permission—especially after the profit is made. ChatGPT 270 If you can acquire it for free, do so. ChatGPT 271 Always have an escape plan. ChatGPT 272 Always inspect the merchandise before making a deal. Star Trek Online 273 Wealth attracts attention; use it wisely. ChatGPT 274 Fear can be a profitable motivator. ChatGPT 275 Revenge is unprofitable. ChatGPT 279 Never lose sight of profit—the ultimate goal. ChatGPT 280 If it ain't broke, don't fix it. DS9 novel: Ferenginar: Satisfaction is Not Guaranteed) 283 Rules can be bent—not broken—to serve profit. ChatGPT 284 Deep down, everyone's a Ferengi. The Ferengi Rules of Acquisition (DS9 reference book) 285 No good deed ever goes unpunished. DS9 episode: \"The Collaborator\"" }, { "title": "The M69 Derby", "url": "/articles/2024/The_M69_Derby/", "content": "This weekend I watched Coventry City FC vs Leicester City FC in the “M69” derby (formerly the “A46” derby). It was a great game, but I’ve always found the name a bit odd, but then it isn’t the only football match referred to by a road name, but there seems to be no consistent reason why we do or do not. Since the naming of these derbys is essentially arbitrary I’ve decided that a match between two teams is the “something” derby if more than half the journey (by road) between the two teams’ stadia is on one road. For all English Premier League, Championship, League 1, League 2 and Scottish Premier League teams that results in the following matrix of derby names Which gives rise to a lot of potential derby’s, most of which would be called the “M1” or “M6” derby. To narrow it down, only unique road matches will be called derby’s (e.g. the only appearance of the M69 in the table is between Cov and Leicester; green in the previous image). The A19 Derby: Newcastle United v Middlesbrough The A23 Derby: Brighton \u0026amp; Hove Albion v Crawley Town The A4040 Derby: Aston Villa v West Bromwich Albion The A59 Derby: Harrogate Town v Accrington Stanley The A6010 Derby: Manchester City v Salford City The A630 Derby: Sheffield United v Rotherham United The A66 Derby: Middlesbrough v Carlisle United The M23 Derby: Crystal Palace v Crawley Town The M27 Derby: Southampton v Portsmouth The M55 Derby: Blackpool v Preston North End The M69 Derby: Coventry City v Leicester City The Tees–Wear derby between Middlesbrough and Sunderland has been called the A19 derby, but our research shows it would be better for Newcastle v Middlesborough. Also the “M27 Derby” as defined above between Southampton and Portsmouth is more commonly called the South Coast Derby. If you want to download the Spreadsheet with all the data in, it is available here." }, { "title": "Space weather can affect our daily lives — we need a better warning system", "url": "/articles/2024/A_new_space_weather_warning_system/", "content": "In May, millions of people were dazzled by the vibrant hues of the aurora borealis and australis — the northern and southern lights. The product of a large solar storm, curtains of green, red and purple light rippled across the night sky in regions where the spectacle is seldom seen. I saw them for the first time in the United Kingdom, from my back garden just outside Birmingham. More displays are expected in the coming months as solar activity starts to reach its peak. But what most people didn’t see were the repercussions — and the preparations that went on behind the scenes to mitigate them. Radio communications systems experienced blackouts; Starlink, the satellite Internet provider, faced outages; and disruptions to global navigation satellite systems caused problems for sectors reliant on positioning. Meanwhile, flights were re-routed and electric grids were safeguarded. This illustrates a dilemma faced by space-weather scientists such as myself: how can we issue and communicate effective warnings when even such a significant storm changes little in most people’s lives? I think part of the problem lies in how we classify space weather. Current systems are simplistic; space weather is not. Geomagnetic storms are classified using scales developed by the US National Oceanic and Atmospheric Administration in collaboration with the space-weather community. The geomagnetic storm scale (G-scale), solar radiation storm scale (S-scale) and radio blackout scale (R-scale) range from one to five, with five denoting an extreme event. These scales have been invaluable in highlighting the risk of space weather to industries and governments, but they are due a refresh. The geomagnetic storm that caused the May auroras was classified as G5, or ‘extreme’. However, the effects of a solar storm are challenging to quantify. This storm was triggered by a rapid sequence of at least seven coronal mass ejections — massive bursts of solar matter and magnetic fields expelled from the surface of the Sun. As these collided with Earth’s magnetic field, they compressed and disturbed it, triggering geomagnetic storms. But many factors, including the speed, mass, duration and magnetic orientation of coronal mass ejections, influence a storm’s impact. In terms of peak geomagnetic activity, the May storm was a 1-in-10-year event. What set this storm apart was its prolonged duration. Geomagnetic disturbances remained high for 24 hours, making it more like a 1-in-75-year event. Auroras were visible for longer, allowing more people to witness them. Such details make it hard to convey the risks of space weather without overstating or understating them. If an event such as that seen in May, deemed ‘extreme’, results in minimal obvious disruption, how should the risks of an even stronger 1-in-100-year storm be conveyed? A superstorm is a looming reality. Costs could reach billions of dollars. Electric grids could be disrupted, causing local blackouts. Satellites, and the essential systems they support, could be impaired. Radio signals, crucial for aviation, maritime operations and emergency services, would be disrupted. For example, in October 2003, geomagnetic storms affected satellite communications, GPS systems and power supplies in Europe, North America and Africa. To communicate the severity of truly extreme storms, scientists need to rethink the classification scales. Some researchers suggest extending severity levels to six and above. Others propose covering more phenomena, including the D-Scale (radiation dose rate) and the T-Scale (radio-wave propagation through Earth’s upper atmosphere). My preferred approach is to shift to a ‘traffic light’ model of warnings for specific sectors. For example, yellow space-weather warnings could alert industries, such as aviation and agriculture, that might be affected by minor geomagnetic storms. An orange warning might require users, such as power grid and radar operators, to consider taking pre-emptive action to protect their services and prepare for interruptions. A red warning would signal that dangerous space weather is expected, with potentially significant impacts requiring immediate action, and power companies, satellite operators and emergency services must implement contingency plans without delay. Such a system would quickly alert those who should be most concerned. It could accommodate uncertainties by allowing for updates and escalations in warnings as data become available. For example, a geomagnetic event might start with a yellow warning (high impact, but unlikely) when a coronal mass ejection is first observed. It could be upgraded to orange or red if the probability of disruption increases with further modelling and data. Its reach could easily be expanded by adding sectors to the warning list. A unified approach to refining space weather reporting and response strategies is crucial, and space weather centres worldwide must come together to discuss and agree on an updated approach. They must ensure that terms such as ‘extreme’ are not misused and that these align with events that truly warrant attention and action, and that industries and the public are adequately prepared for the real risks. As we navigate the peak of the solar cycle, it is important to acknowledge that space weather affects our daily lives. By refining classification and reporting systems, scientists can better align public perception with reality, ensuring that we are neither crying wolf nor caught unprepared. Nature 630, 271 (2024) doi: https://doi.org/10.1038/d41586-024-01715-z" }, { "title": "Google Trends for Solar Flares", "url": "/articles/2024/Google_trends_for_solar_fares/", "content": "Google Trends shows the popularity of search queries in Google Search across various regions and languages. Below I have plotted the data for “Solar Flare”. Google Trends report this as “interest over time”, whcih is defined as: Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. A score of 0 means that there was not enough data for this term. Since this is quite odd, I have decided to removed outliers (by using a 95% Z-transform threshold) and have then plotted the rolling 12-month average: I think it shows a pretty good solar cycle (although shifted by about 2yrs) in terms of peak sunspot number." }, { "title": "Unlocking the Scientific Secrets of the Solar Eclipse", "url": "/articles/2024/2024_Solar_Eclipse/", "content": "This celestial event, where the moon passes directly between the Earth and the Sun, temporarily cloaking daylight in darkness, offers not just a spectacle for the eyes but also a unique scientific opportunity to study the effects of eclipses on the Earth’s near-space environment. Solar eclipses have fascinated humanity throughout history, serving as subjects of wonder, fear, and intense scientific inquiry. They occur when the Moon’s orbit aligns perfectly to cast a shadow over the Earth, temporarily blocking the Sun’s light. Beyond their awe-inspiring visual display, these events provide scientists with a natural laboratory to study phenomena otherwise hidden in the light of day. This year, David Themens and Ben Reid from the Space Environment and Radio Engineering (SERENE) research group at the University of Birmingham are undertaking an ambitious scientific experiment during the total solar eclipse on April 8th, 2024. Stationed in Fredericton, Canada, they are investigating the eclipse’s impacts on the ionosphere – a layer of Earth’s upper atmosphere filled with charged particles, known as plasma, that plays a crucial role in radio communication. Joining forces with the University of New Brunswick’s EclipseNB mission, Themens and Reid will utilise an advanced network of newly developed sounding instruments and observatories spanning the Canadian path of totality. This initiative builds on the curiosity sparked by observations during the “Great American Eclipse” of August 21, 2017, propelling forward our understanding of the ionosphere’s dynamics. The insights gained from this experiment have the potential to improve our understanding of the ionosphere’s response to solar eclipses Dr David Themens, Space Environment and Radio Engineering Group (SERENE) Central to their research is the Assimilation Canadian High Arctic Ionospheric Model (A-CHAIM), an innovative space weather model that Themens and Reid have developed. By integrating data from a constellation of Global Navigation Satellite System (GNSS) observatories and other sensors, A-CHAIM will offer unprecedented insights into the ionosphere’s behaviour under the shadow of the eclipse. This analysis is not merely academic; the ionosphere’s plasma can significantly interfere with GNSS signals, such as those from GPS, leading to navigation errors. Understanding and modelling the eclipse’s effects on these systems is vital for improving the reliability of communication and navigation technologies that underpin modern society. The insights gained from this experiment have the potential to improve our understanding of the ionosphere’s response to solar eclipses. By tracking the formation of plasma instabilities and assessing their impact on GNSS signal integrity, Themens and Reid’s work will enhance our ability to predict and mitigate these effects in the future. Moreover, this research will test the limits of A-CHAIM, and other ionospheric models developed by SERENE, predictive capabilities, with the hope of refining these critical tool for global users. The knowledge acquired will not only shed light on the processes at play during an eclipse but also help fortify our defences against space weather phenomena that can disrupt our technological systems. As the world looks up to witness the majesty of the total solar eclipse, SERENE and the University of New Brunswick’s EclipseNB team will be delving deep into the shadows to illuminate the mysteries of our near-Earth environment. Their work stands at the forefront of space weather research, promising to expand our understanding of the universe and safeguard our technological future." }, { "title": "An Argument for xG", "url": "/articles/2024/An_argument_for_xG/", "content": "The use of expected goals (xG) in football has its proponents and, very high profile, detractors. Amongst those detractors includes Gary Lineker, Alan Shearer and Micah Richards and it is mentioned regulalry on their podcast. (If you are unfamiliar with xG there is a good description of it here) One thing that has come up a few times during the podcast is their negative opinion on xG. Whilst this post might be a bit like Billy Beane talking to the Oakland A’s scouting team, I wanted to at least present an “argument for” xG: First off, the negative opinion of xG is not surprising, very often the stat is misused, to the point where it is nonsensical. Not out of malice by anyone, but just because there are subtleties involved (and social media character limits often stop the discussion – so in this case I hope you forgive the length of this post). Even the name is quite confusing, “expected” does not mean what it regularly means in English but rather its mathematical definition (I’ll leave out any actual mathematics, but “the average outcome of an event if it were repeated a large number of times”). Naively I think it makes sense to have a metric which evaluates the quality of chances in a game (whether that is xG or something else). For example, following the Liverpool v Manchester United game on December 17th (which ended 0-0) there was a comment on the podcast that Liverpool had 34 shots (to United’s 6). The comment (I believe) was meant to suggest that the result was surprising. Perhaps it was? Certainly 34 is more than 6, but how good were those shots, perhaps United’s 6 shots were all from within the 6 yard box and Liverpool’s all from 40 yards away? We could look at “shots on target” (8 – 1) but that still doesn’t tell the full story: one of those on target shots was from 30 yards out (Elliott; 81st minute) which is surely much less likely to result in a goal than one from within the 6 yard box (Gravenbech; 16th minute). xG is used to distinguish between the quality of those shots (Elliott’s shot had an xG of 0.02, whereas Gravenbech’s was 0.41). Overall the xG for that match was 2.38 - 0.75, but that does not mean Liverpool “deserved” to win that particular game (you can not “win the xG” as Thomas Tuchel said about Bayern against Frankfurt), it tells us that by the end of the game, Liverpool had the higher quality of chances, but it doesn’t tell us about the expected outcome. What is an even more crazy way of using xG (which comes up worryingly often) is looking at/analysing/commenting on a single goals xG – it is essentially meaningless. Take a penalty, which has a fixed xG in most models (a little more on that in a minute), of 0.79. That means that on average 79% of penalties are scored (Shearer’s Premier League record was 56 out of 67, 83.6%) but the value of 0.79xG doesn’t take into account match context, it is easier to score a penalty when you are 4-0 up at home, than if it is 1-1 in the 90th minute, away from home, to win the league (for example). And that is true for any individual xG ‘event’, context matters, an individual attempt is just that, individual and unique. The stupidity of this was highlighted by the discussion of the xG of Garnacho’s overhead kick against Everton (0.08xG) which Alan on the podcast (rightly) described as a “pile of sh!t”. However I would implore you to appreciate that discussing that goal as being scored 8 times in 100 is just xG being used wrong, rather than xG actually being wrong. As a further example it is like commenting on the roll of a dice (die). The “expected” value of rolling one dice is 3.5 (yes – the expected value is not a value you can ever even get), but for a dice roll any of the numbers, 1, 2, …, 6 are equally likely and commenting on the fact that you didn’t get a “3.5” makes absolutely no sense. However if you took 1000 dice rolls, and look at the average that you got over that period if it was significantly different from 3.5 then you might start to suspect that something might be up (e.g. weighted dice). That is where xG comes into its own, over long time periods, investigating the difference between xG and actual goals scored across half a season, a full season etc., that’s where you can start to draw conclusions about teams/players and how they are performing. Is a team/player consistently under/over-performing their xG? Is a particular striker a great finisher, consistently getting in the right positions and doing better than expected or are they doing what you would expect for a team creating that many opportunities? Now we are halfway through the season we are starting to have enough data for interesting discussions to happen (just like there is no point in looking at the Premier League table after the first game of the season, there is no point at looking at xG until enough games have been played), perhaps there could be discussion around (for example) Son Heung-min’s continual “over-performance” in terms of goals scored. Or for a further example, Lineker and Shearer have mentioned a number of times that, whilst not diminishing Haaland’s excellent return last season, they both would have done very well in that Man City team. Well, Haaland’s 22/23 season xG was 33, and he scored 35 goals, so that is pretty much what you would “expect” (for someone getting in those positions, and of course there is a huge amount of skill in being the player in “the right place, at the right time” to get that xG). In fact since 2019 Haaland has racked up an xG of 99 and scored 112, so he is above what we “expect” which makes sense for a player of his obvious quality. But, using xG, we can start to look at interesting things across the worlds best players, perhaps a way of investigating the differences in their finishing ability from their positional play, and the quality of the teams they play in, for example look at the table below comparing goals scored against their accumulative xG (only using data from 2014 onwards). I’ve highlighted (and ranked) the players most over and under performing according to this metric for a selection of the worlds best: Name Goals Scored Accumulative xG % difference Son Heung-Min 126 92 +37 Harry Kane 231 198 +16 Lionel Messi 253 220 +15 Kylian Mbappé 183 161 +14 Erling Haaland 112 99 +13 Cristiano Ronaldo 234 229 +2.2 Karim Benzema 166 169 -1.8 Robert Lewandowski 269 276 -2.5 Victor Osimhen 70 73 -4.1 Alexander Isak 51 55 -7.3 Gabriel Jesus 72 97 -26 One final thought about the xG values that you will have seen presented. Different peoples and companies xG values are not all of the same quality. The original xG models “just” considered the position of where the shot was taken which has clear weaknesses because you know nothing about whether you are 15 yards out, but with an open goal, or 15 yards out on the half-turn with three defenders around you. More modern xG calculations (such as the one from Opta used by Match of the Day) are built on over a million shots considering, amongst others: Distance to the goal, Angle to the goal, Goalkeeper position, The clarity the shooter has of the goal mouth, based on the positions of other players, The amount of pressure they are under from the opposition defenders, Shot type, such as which foot the shooter used or whether it was a volley/header/one-on-one, Pattern of play (e.g., open play, fast break, direct free-kick, corner kick, throw-in etc.), Information on the previous action, such as the type of assist (e.g., through ball, cross etc.) So, when seeing xG analysis across the media you also have to be careful of the underlying xG model being used (and that information is very rarely provided). Whilst there are a lot of challenges and caveats in using xG, and its misuse in the past means that people are sceptical of its merit now, I believe that there is potentially interesting analysis that can be done with it (if only more professional pundits would give it a chance…). Football is a game which is inherently “low-scoring” and so tools that analysts can use to enhance discussion could offer real benefits. Unfortunately it is often misused and misunderstood." }, { "title": "The Northern Lights and technology's vulnerability to space weather", "url": "/articles/2023/Northern_lights_and_technology_vulnerability_to_SpWx/", "content": "Skies across the UK have been lit up this week by the spectacular Aurora Borealis (Northern Lights). The Lights are a beautiful sight, and photos of them have appeared throughout the media – there was even a live ‘Can you spot the Northern Lights across the UK?’ page on the BBC website running until 2am. As majestic as they are, they also serve as a reminder of the considerable impact that our Sun can play on modern technological systems. Sitting at the centre of the Solar System is the Sun, a giant ball of ionized plasma - a state of matter made up of positively and negatively charged particles. Some of this plasma is constantly streaming out into space, which we call the solar wind. Thankfully the Earth has a Star Trek-esque shield, the magnetosphere, protecting our atmosphere from being blown away. However some of the high-energy electrons in the solar wind make it through the shield and can hit molecules in our atmosphere. During these collisions some of the electrons energy is transferred to the molecules and, if the conditions are just right, cause them to vibrate and emit a tiny amount of light. The colour of this light depends on the amount of energy involved and the altitude of the interaction. Enough vibrating molecules, enough emitted light and voilà, we get the aurorae (Borealis or Australis), which typically form in the high latitudes above 65 degrees north or south, but this can vary significantly. ‘Space Weather’ is a term which describes variations in the near-Earth space environment and the influence that has on technology and human life and health. Just like terrestrial weather, space weather is impacted by storms. Minor storms, like we have experienced this week, are relatively common, a major storm may occur once or twice per decade and solar ‘superstorms’ occur once every 100 to 200 years. During these storms the aurora can be seen at lower latitudes which is why they have been visible over the UK recently. In the most severe storms, famously during the “Carrington Event” in 1859, the aurora were observed in the Caribbean and Hawaiian Islands! But the aurora are not the only consequence of space weather. Aside from pictures of the aurora, space weather does not feature in the everyday news and goes largely unrecognised by the public. However, space weather must be accommodated in the design and operation of an increasing number of modern technological systems. For example, high energy particles associated with space weather can cause satellite failures and can render satellite-based navigation systems, such as GPS - a now ubiquitous technology - useless for several days. Other impacts are experienced on the electricity grid, satellite and cellular communications systems, avionics and the radiation doses experienced by air passengers and crew. Even communications with aeroplanes are impacted, and can be completely disrupted, forcing airlines to change their routes and rely on backup systems. Looking to the future, space weather will be a major hazard on any crewed mission to Mars. The impact of space weather on all these systems will be much more significant during a solar superstorm with serious consequences for the national infrastructure, UK industry and the wider public. As a result, in 2011 space weather was added to the Government National Risk Register of Civil Emergencies, with its likelihood currently judged as comparable to that of an emerging infectious disease. Only by providing sufficient advance warning and having robust plans in place for reacting to an extreme event, can we be confident of minimising the consequences. The Space Environment and Radio Engineering (SERENE) research group at the University of Birmingham investigate innovative mathematical methods, and develop novel small scale, and non-linear physics, to create the next generation of space weather forecast models. These models can be used to provide accurate, and actionable, forecasts to improve airline route planning to avoid communications losses, predict outages of GPS, warn satellite operators of potential collisions, and even tell you when and where you are most likely going to be able to take your own photo of the aurora." }, { "title": "Should You Really Worry About Solar Flares (Scientific American)", "url": "/articles/2023/Should_you_worry_about_solar_flares/", "content": "On January 6 a powerful solar flare erupted from the surface of the sun. It was the first X-class flare—the strongest type on the flare intensity scale—in around two months. Two further X-class flares followed within the next few days, marking a distinct uptick in solar activity—and in breathless speculations that the flurry of flares could threaten us here on Earth. One U.K. tabloid raised the possibility of “major continent-wide power blackouts.” “Terrifying solar storm coming to Earth,” read another outlet’s headline. Heliophysicists and other scientists studying “space weather” warn that flares and related solar outbursts can indeed interfere with modern life by damaging power grids, as well as by increasing radiation exposures for occupants of space habitats and high-altitude aircraft. But even so, such experts say, the risk of harm arising from inclement space weather remains far lower than many media reports often suggest. Does that mean we shouldn’t worry any time our star decides to belch in our general direction? Not exactly. Knowing what to worry about and when, however, requires proper context and perspective. Solar flares arise within the sun’s roiling soup of plasma when charged particles thrash around one another to form intense magnetic field lines that can tangle and interweave. When these magnetic fields suddenly shift or realign—as often happens in sunspot-pocked “active regions” of the solar surface—an enormous amount of energy is released. There is no question that these solar eruptions, and particularly X-class flares, are mind-bogglingly powerful. One of the strongest solar flares ever seen, the Carrington Event of 1859, may have released as much energy as 10 billion megatons of exploding TNT. Imagine the infamous Tsar Bomba—the most powerful thermonuclear weapon ever detonated—and you’re about five billionths of the way there. Most solar eruptions are nowhere near this energetic. But they can still propel enormous, planet-engulfing clouds of plasma called coronal mass ejections (CMEs) toward Earth, as well as anywhere else in the solar system. Direct hits on our world by CMEs can and do occur, and each event packs more than enough oomph to discernibly disrupt some modern electronics. Even so, such disruptions are usually subtle and have little to no impact on most peoples’ daily lives. Another Day, Another Flare In aggregate, spates of space-weather scaremongering are easy to predict because they’re usually tied to solar activity itself, which follows a roughly 11-year cycle. Each round of the cycle sees our star oscillate once between two main states: the solar minimum, in which flare activity tends to be at its lowest, and the solar maximum, in which flare activity tends to be strongest. Currently, the sun is slightly more than two years away from its maximum, so heightened activity (and media alertness) is to be expected now and for the next few years. “At this point, I do not need to convince you that we are not facing the apocalypse,” says Erika Palmerio, a heliophysicist at solar research company Predictive Science. “The point is that solar activity is rising at the moment because that is what is expected of the solar cycle.” X-class flares, for instance, are quite common during periods of heightened solar activity despite their ominous “strongest” ranking. According to Palmerio, a typical solar cycle produces almost 200 of them, with most occurring towards the maximum. Statistically speaking, that implies we can expect more than a dozen X-class flares to erupt this year alone. When the next one happens, it’s important to remember a key caveat often overlooked in subsequent media coverage: While X-class events occupy the highest tier of flare intensity, this tier has no upper limit. So despite all X-class flares residing in the “strongest” category, they still can vary enormously in strength. The Carrington Event, for example, would by some estimates have registered as an X40 flare, whereas the flares at the start of this month all ranked at far more humdrum levels of between X1 and X2. The Realistic Threats Simply put, even the most severe solar outbursts in living memory have had relatively subdued effects. This is partly because, despite the alarming but unlikely possibility of widespread electricity blackouts, space weather does not tend to affect phones, laptops and other everyday electronic devices at all. “I can’t think of any consumer electronic impacts that would even be possible,” says Sean Elvidge, head of space environment research at the University of Birmingham in England. CMEs can and do pose threats to power grids by interacting with Earth’s magnetic field and inducing excess electrical currents along and through the planet’s surface that sometimes last for hours. Given sufficiently mighty currents from a strong solar storm, unprotected transformers along power grids can be severely damaged and knocked offline, which reduces a grid’s capacity and necessitates time-consuming and expensive repair and replacement work. “But this really can only happen in very long cables or pipelines,” Elvidge adds. “So we don’t see that in your house. Your house just isn’t a problem for that kind of thing.” Palmerio says there is one highly unlikely way in which an electronic device might be affected by a solar storm: If it were to be struck by a single, very strongly charged solar particle that transfers most of its energy to the device upon impact. Scientists refer to such incidents as a single-event upset. “So you’re at your computer, and this one extremely strong particle hits your computer and fries it,” Palmerio says. “But obviously, the probabilities are very low. It’s definitely not something on the order of ‘all the mobile phones in this entire country are fried.’ That’s not going to happen.” Such events do become more likely at higher latitudes because of the tendency for a solar storm’s swarms of charged particles to accumulate around Earth’s poles, where the planet’s magnetic field is strongest. They become more likely at higher altitudes as well, where the thinner atmosphere offers less protection against incoming particles. Such diminished protections are also why jet travel increases one’s total radiation exposure and why transcontinental flights are sometimes routed away from polar regions during severe solar storms. “[A single event upset is] sort of where the planes-falling-out-of-the-sky idea comes from,” Elvidge says. “You can exaggerate it to the point where it seems comical and absurd and therefore not going to happen—or, you know, it’s sensible to talk about the real impacts. If you’ve got a pilot who’s just had 2,000 warning lights come on, nothing’s going to break [the aircraft], but it’s going to be fairly stressful.” The wider and more realistic concern for electronic systems is damage to power grids. For that reason, space weather forecasting is key. Looking Ahead Key to space weather forecasting is the distinction between solar flares and CMEs. If a flare is the muzzle flash of a gunshot, the CME is the bullet. The x-rays and extreme ultraviolet radiation from flares reach Earth at near light-speed, and by the time we observe them, their effects—such as disruptions of high-frequency radio signals or satellite navigation systems—are already being felt. “We cannot predict when a flare will occur,” Palmerio says, although closely monitoring sunspots and other denizens of solar active regions can allow for educated guesses. “There is a lot of research going on. For example, are there tiny features we can see on the sun that can be considered precursors of flaring activity?” CMEs are relatively easier to forecast, in large part because they almost always begin with an especially strong flare yet move far more slowly. While flares take just eight minutes or so to reach Earth, CMEs can take up to three days. In addition, certain space weather satellites can act as an early warning system and send back data on the strength of any incoming CME before it starts washing over the planet’s power grids. The Geostationary Operational Environmental Satellites (GOES) operated by the U.S. National Oceanic and Atmospheric Administration are examples. “We’ve worked closely with the power industry to help them prepare for these things and mitigate the impact,” says Rob Steenburgh, acting lead of NOAA’s Space Weather Forecast Office. “So we notify them when we think we know what’s coming.” Grid operators, Steenburgh says, can then take actions to mitigate possible threats, such as decreasing total power output or bringing more backup equipment online. None of this is to say there are no threats. A major unknown in space weather forecasting is when the next “big one” will occur—the next Carrington-level storm or even a storm 100 times more energetic than that. Estimates vary from once per 100 years for a Carrington-level event, according to Elvidge, to once per millennia for ones that are much more powerful. A 2022 study of tree rings, which can record prehistoric solar storms in their growth, found evidence that huge radiation spikes, dubbed Miyake events, occurred several times across millennia. If caused by gargantuan solar flares, such events would be enough to cause significant disruption to power grids and satellites today. Yet the events do not seem to have a consistent relationship with the solar cycle, and some appeared to last longer than a year—far longer than any known flare—so their true origins remain a mystery. “As a person who works with this every day, I am way more scared of a ‘doomsday’ derived from terrestrial weather like forest fires, hurricanes and extreme weather,” Palmerio concludes. “At every solar cycle, it’s as if we forgot what happened in the previous one. In [the cycle that stretched from 1996 to 2008], we saw really strong events. And I’m pretty sure that most people didn’t even know those events happened…. We have to monitor, and we have to be prepared. But we do not have to lose sleep over this.”" }, { "title": "Maths on a Mug 15", "url": "/articles/2023/Maths_on_a_Mug_15/", "content": "This is a “simple” #MathsOnAMug. Simple, but very important. This is Bayes’ theorem: \\[P(A|B)=P(B|A)P(A)P(B).\\] This relates the probability of observing \\(A\\) given that \\(B\\) is true where \\(A\\) and \\(B\\) are events, we also require that the probability of \\(B\\) does not equal 0 (i.e. \\(P(B)\\neq 0\\)). It allows us to find the probability of a cause, given an effect. Bayes’ can lead to some surprising, count-intuitive, results. Dr. House never thinks his patients have Lupus. Lupus is a condition where antibodies that are supposed to attack foreign cells to prevent infections instead attack plasma proteins which can lead to blood clots. It is believed that 2% of the population have lupus. Suppose that a test is 98% accurate if a person has the disease and 74% accurate if they do not. Given the 98% accuracy of the test, should House really assume his patient with a positive result doesn’t have Lupus? We can check the probability using Bayes’ theorem. Let L represent Lupus, T the test and 1 and 0 positive/negative respectively (I’ll leave as an exercise to the reader to calculate the individual probabilities): \\[\\begin{eqnarray*} P(L=1|T=1) \u0026amp;=\u0026amp; \\frac{P(T=1|L=1)P(L=1)}{P(T=1)},\\\\ \u0026amp;=\u0026amp; \\frac{(0.98)(0.02)}{0.0196 + 0.2548},\\\\ \u0026amp;=\u0026amp; \\frac{0.0196}{0.2744},\\\\ \u0026amp;=\u0026amp; 0.0715 \\end{eqnarray*}\\] So House’s insight is sensible, even given a positive result there is only a 7.14% chance of the person actually having Lupus! How we interpret Bayes’ theorem depends on how we interpret probability, a Bayesian or Frequentist interpretation. This is a long running fight in mathematics, and it is worth reading about it to see where you come down on the argument. I’ll leave you with a quote from Savage (1954). It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "As-many-days-together-as-not Calculator", "url": "/articles/2022/As_many_days_together_as_not/", "content": "Milestone Date Calculator Enter your date of birth and the date you started dating your significant other to find out the date when you'll have been with them longer than not with them. Date of Birth: Date You Started Dating: Calculate" }, { "title": "Maths on a Mug 14", "url": "/articles/2021/Maths_on_a_Mug_14/", "content": "Keeping with my longing running theme of talking about the pound coin and shapes of constant diameter here is a nice little result known as Barbier’s theorem. Simply, it says that for any curve of constant diameter (width; \\(w\\)) the perimeter of that shape is \\(\\pi\\times w\\). A nice result. The proof follows from Crofton’s formula. Interestingly though the result doesn’t hold for higher dimensional shapes – e.g. the surface area for solids of constant width depends on more than the width alone… Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Euro 2020 – The Final Modelling Results", "url": "/articles/2021/Euro_2020_Final_Modelling_Results/", "content": "With a disappointing end to the tournament (for us England fans anyway!) here is the final conclusions of how the model performed. Unfortunately a mixture of work commitments, and games coming thick and fast, I was unable to keep this website up-to-date with my predictions for each fixture, but I did carry on doing them offline. (As a reminder: the model description is here and the other models we compare against are here.) The Round of 16 gave us some truly surprising results, with every single model (including the bookies!) doing (a lot) worse than our Lazy prediction of 33% for Home, Draw and Away. This then seemed to be a sign of what was to come, the semifinal and final was worse than Lazy for most of the tested models. It made for an exciting tournament but a disaster for predictions! The final performance analysis of the models was: Model Brier Score HAL 9000 0.586 Bookies 0.588 Average 0.609 BDC 0.655 Lazy 0.667 SSC 0.728 Immediate things to note are that HAL won (just)! Clearly that is a great result. It is interesting though that SSC (small “smart” crowd) ended up doing much worse than Lazy, which is terrible! I would have expected that to do much better. The poor performance likely comes from the fact that there were a number of “perfect” wrong answers. By that I mean that the model (my friends) all predicted something would definitely happen (e.g. England to beat Denmark after 90 minutes). That gave it a probability of 100%, when it didn’t happen, the maximum penalty of 2.0 was assigned. Nothing in football is ever going to be 100% certain (the most one sided result that the bookies predicted was a Germany win against Hungary in the group stage [80.7%] – it ended up as a draw!). If we set a rule that SSC can never predict more than an 80% result then the Brier Score drops to 0.627 and ends up beating BDC and Lazy – more like I would have expected! Making SSC’s maximum prediction 60% further drops the Brier Score to 0.577 – making it the winning model! However this is likely because there was a large number of surprising results in the tournament, so this is really only a post-tournament model, and not a fair assessment. Setting the maximum to 80% (matching the Bookies maximum value) seems like a fair compromise though! Model Brier Score HAL 9000 0.586 Bookies 0.588 Average 0.609 SSC 0.627 BDC 0.655 Lazy 0.667 Overall, a great result for HAL. Also, overall, the model came 3rd in Futbolmetrix’s Sophisticated Prediction Contest which I am very happy with. The results post has some interesting statistical analysis and definitely worth checking out." }, { "title": "Euro 2020 – Round 1 Results \u0026 Validation Methodology", "url": "/articles/2021/Euro_2020_Round_1_Results_and_Validation/", "content": "With the first round of fixtures complete we can have a first peak of how well our model is doing (for the details about my Euro’s prediction model see my previous post.) A hugely important thing for any model is validation. How well is our model doing? One way to investigate this is by looking at the Brier Score of the model. The Brier score is used to measure the accuracy of probabilistic predictions. Fundamentally it is the mean square error of the forecast and varies from 0 (perfect prediction) to 2. So the lower the score the better. For each result a Brier score is calculated and then an overall score is given to the models by taking the mean value. As a baseline, a simple prediction of 33% for each result (win/draw/loss) gives you a Brier score of 0.666. So we should always be looking to do better than that. Likely the best model of this tournament will be that of the bookies (it usually is!). Our version of the bookies model will be to take the average of a range of bookies (rather than taking any one organisation) which can be found on oddsportal.com, these are then converted to a probability. Two final models for comparison come from ‘crowd wisdom’. Firstly from SofaScore, where people can vote on a particular result (typically over 170,000 people vote on these results). Very little thought is needed, you simply press a button about what result you think is most likely, and this is a “secondary” reason for using the app (it is for finding out sports scores). An alternative crowd model comes from a smaller, “smarter”, crowd. For the Premier League, FA Cup, World Cup and Euro’s my friends and I take part in a prediction competition, we predict the result of every fixture (with a suitably complicated scoring system based on the correct result, goals a team score and goal difference). Looking at these 10 peoples predictions we can assign probabilities based on the results they projected (e.g.if 7 people think “Team A” will win, then we assign a 70% probability). So that rounds off the models we are going to compare: HAL 9000 (that is the name of my model) Bookies BDC (Big Dumb Crowd) SSC (Small Smart Crowd) Lazy (all results 33%) Model Brier Score HAL 9000 0.522 SSC 0.547 Bookies 0.575 BDC 0.663 Lazy 0.667 Currently HAL is winning with a Brier score of 0.522 and the SSC is also beating the bookies (but we’ve only played 12 of 51 matches so plenty of time for that to change!). At least we can have some confidence in our model." }, { "title": "Maths on a Mug 13", "url": "/articles/2021/Maths_on_a_Mug_13/", "content": "A question I really like because of the logical way of thinking about the answer. First we assume that we do not allow the trivial answer (e.g. where \\(n=m=q=0\\)). Then rewrite the equation as: \\[2^n+3^m=5^q+1.\\] Now \\(2^n\\) must be even for all values of \\(n\\), and \\(3^m\\) must be odd for all values of \\(m\\). Thus \\(2^n+3^m\\) is odd for all values of \\(n\\) and \\(m\\). Similarly, \\(5^q\\) is odd for all values of \\(q\\), thus \\(5^q+1\\) must be even. Therefore the left hand side of the rewritten equation must be odd, whilst the right hand side must be even. Since this is not possible there are no positive integer solutions. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Euro 2020 Predictions", "url": "/articles/2021/Euro_2020_Predictions/", "content": "For the 2018 World Cup I created a prediction model for the tournament (you can read the details here. Overall the model did well and I was pretty happy with it. For this tournament, Euro 2020 2021 2020 (it took me a while to realize that this tournament is called Euro 2020) I have built a new model which is much more simple than my World Cup variant. There are a few reasons for this decision, primarily based around the fact that this particular tournament is not going to be hosted in one country (as it is usually), but instead will be played over 11 countries. Whilst this could be accounted for in the modelling (as many other people have done) I am not sure exactly how this home advantage will pan out – because the change of format is something which we just have no data on. Not to mention that this year, for the first time, there was a higher % of away wins (40) than home wins (38) [obviously this past year almost every game has been played behind closed doors]. So my model for this tournament first requires a rating for each team. This is similar to the World Football Elo ratings (WFER) but with some changes I have made (I am currently writing up the exact details). It then takes every World Cup and European Championship fixture, excludes the home team (since I do not want that impact in my model) and then looks at the “home” team rating and “away” team rating (these are now meaningless terms) and what the result was. Scatter plot of ‘[H]ome win’, ‘[D]raw’ and ‘[A]way win’ by team rating. Whilst this plot is obviously quite messy, overall we can see some rough trends. These highlight an immediate problem. Given that I thought I had removed home advantage we should expect that the black line would be exactly on the line y=x. That is, if two teams are equally rated you would expect this is more likely to result in a draw. However you can see from the figure that this is not true. Although the red, ‘away win’ and blue ‘home win’ regressions do confirm what we would expect – e.g. if the home team has a higher rating than the away team then it is more likely to result in a home win. The issues could be explained by either residual home/away impacts in the data, or an issue with my team rating system. Using this data we could now fit any number of machine learning classification models. For this model we use a naive Bayes classifier, which gives the following probabilities for a home or away win, or a draw: Home win probabilities (note the change in colour scale for the draw plot) Away win probabilities (note the change in colour scale for the draw plot) Draw probabilities (note the change in colour scale for the draw plot) The above plots show how the probability of a given event changes as a function of the two teams rating. Combining all of these plots into one shows the decision ‘surfaces’, as the model transitions from ‘H’ to ‘D’ to ‘A’. Plot of decision surfaces for the Euro 2020 prediction model. This model can then be used to predict the outcome of the European Championships. Simulating the entire tournament 20,000 times (this was made slightly difficult by the way [some] third-placed teams enter the knockout phase) gives rise to the following probabilities for tournament winners: Probabilities of teams winning Euro 2020. My model predicts that France is most likely to win Euro 2020 closely followed by Belgium. Unfortunately the model only ranks England as 7th most likely with odds of 6.6%. I have entered my model in the Futbolmetrix sophisticated prediction contest, so it will be interesting to see how that turns out." }, { "title": "Maths on a Mug 12", "url": "/articles/2020/Maths_on_a_Mug_12/", "content": "A classic! This is one of my favourite results from Graph Theory. The branch of mathematics invented by Euler, this theorem is known as Euler’s Formula. It is a remarkable fact that for any planar graph (edges only intersect at their endpoints) the number of faces (regions bound by an edge) plus the number of vertices (corner points) minus the number of edges is equal to 2. Take for example a cube, which has 6 faces, 8 vertices (corners) and 12 edges: \\[\\begin{eqnarray*} F+V−E \u0026amp;=\u0026amp; 2,\\\\ 6+8−12 \u0026amp;=\u0026amp; 2. \\end{eqnarray*}\\] I think this is a lovely little fact. The proof is quite straight forward, using induction on the number of edges: For \\(E=0\\) (i.e. there are no edges) then it is fairly trivial, since there can be only one vertex (\\(V=1\\)) and one face (\\(F=1\\)). So clearly \\(F+V−E=2\\). Now assume it holds for all planar graphs with less than \\(E\\) edges (where \\(E\\ge 1\\)). Let \\(G\\) be a graph with \\(E\\) edges. If \\(G\\) is a tree, then \\(V=E+1\\) and \\(F=1\\), so \\(F+V−E=2\\). The only remaining case is if \\(G\\) is not a tree. In which case let \\(m\\) be a cycle edge of \\(G\\) and let us investigate \\(G−m\\). The connected planar graph \\(G−m\\) has \\(V\\) vertices, \\(E−1\\) edges and \\(F−1\\) faces so using our inductive hypothesis \\((F−1)+V−(E−1)=2\\), which implies that \\(F+V−E=2\\) as required. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Maths on a Mug 11", "url": "/articles/2020/Maths_on_a_Mug_11/", "content": "This is one of the more common things that students ask me. Mostly this comes from people who have watched the incredibly successful Numperphile video, but the result was well known before then. There are a number of ways to ‘prove’ this result, giving rise to much internet angst, my favourite (and simplest) is that of Ramanujan: \\[\\begin{eqnarray*} c \u0026amp;=\u0026amp; 1 + 2 + 3 + 4 + 5 + 6 + \\cdots,\\\\ 4c \u0026amp;=\u0026amp; \\quad 4 \\qquad +8 \\qquad +12 + \\cdots,\\\\ -3c \u0026amp;=\u0026amp; 1 - 2 + 3 - 4 + 5 - 6 + \\cdots,\\\\ -3c \u0026amp;=\u0026amp; \\frac{1}{(1+1)^2},\\\\ -3c \u0026amp;=\u0026amp; \\frac{1}{4},\\\\ c \u0026amp;=\u0026amp; \\frac{-1}{12}. \\end{eqnarray*}\\] Can you spot the mistake? Fundamentally the flaws of these argument often arise from treating infinite sums like finite sums. In general, associativity and commutativity do not hold for infinite series. As an example, take the series \\(1−1+1−1+1−1+1−1+\\cdots=0\\), and then add some brackets: \\[\\begin{eqnarray*} (1−1)+(1−1)+(1−1)+(1−1)+\\cdots=0,\\\\ 1+(−1+1)+(−1+1)+(−1+1)+(−1+\\cdots=1. \\end{eqnarray*}\\] Which give you different answers! For an excellent description of all the errors with the \\(1+2+3+\\cdots=\\frac{-1}{12}\\) proof see the excellent Plus magazine article. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "COVID-19 and Virtual Quizzes", "url": "/articles/2020/COVID_19_and_virtual_quizzes/", "content": "It is pretty easy to tell what the UK has been upto during the coronavirus lockdown, virtual pub quizzes! Obviously the scale of the two units, Google search hits (as a fraction of the most hits) and COVID-19 are very different. So I’ve taken a simple Z-transform to plot the data on the same scale. I’ve also taken the rolling 7-day average to see the overall trend. Clearly the most popular day to prepare your quiz is a Saturday!" }, { "title": "Maths on a Mug 10", "url": "/articles/2020/Maths_on_a_Mug_10/", "content": "Another University of Birmingham Mathematics department mug. The ‘Blakelet’ named after Professor John Blake (formerly of the University of Birmingham) is a based upon a 1971 paper investigating the velocity and pressure fields for Stokes flow due to a point force. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Maths on a Mug 9", "url": "/articles/2019/Maths_on_a_Mug_9/", "content": "Fermat’s Last Theorem is a celebrated result in mathematics, simply it says that: There are no three positive integers \\(a,b,c\\) such that \\(a^n+b^n=c^n\\) for any value of \\(n\u0026gt;2\\). For the value \\(n=2\\) this is the well known Pythagoras theorem. Fermat’s Last Theorem is easy to state, but which was fiendishly hard to prove. It was first stated by Fermat in 1637 and took until May of 1995 to be finally proved by Sir Andrew Wiles. However there is a fun ‘near-miss’ solution which was shown in the Simpsons, and is on the mug this morning. If you type this solution into your handy pocket calculator you will think you’ve found a counterexample: \\[3987^{12}+4365^{12}=4472^{12}\\] However, the numbers are so large (e.g. \\(4472^{12}=63976656348486725806862358322168575784124416\\)) that your calculator doesn’t actually work out the whole values and there is a ‘rounding error’, which is what gives rise to the apparent counterexample, perfectly highlighted by The Simpsons. An excellent video briefly explaining this is available here: It turns out The Simpsons is riddled with mathematical facts and quirks, and these are brilliant captured in The Simpsons and Their Mathematical Secrets by Simon Singh. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "The Impact of Form in Fantasy Football", "url": "/articles/2019/Impact_of_form_in_fantasy_football/", "content": "I am a keen player of Fantasy Football. Not only is the game huge fun (and very addictive) it also provides an excellent opportunity to play around with a whole host of mathematics. I posted a controversial thread on Twitter which has been reasonably popular (and even inspired a few Reddit threads and mentioned in three podcasts [that I am aware of]). The focus was on the impact of “form” on points scored in fantasy football. I will assume if you are reading on you understand, at least the basics, of how fantasy football (from herein, FPL), works. Put simply: a manager picks a team of 15 players with a limit of at most 3 from anyone team and budget constraints, players actions during the games get converted into points, most points at the end of the season and you win! This one version of the game (the “official version”) is played by over 7.5M people! Another action you have as a manager is the ability to transfer players in and out of your team. Obviously you want to make the right choice, bringing in a player just before he (for example) scores, is a great feeling. A lot of discussion about who to bring in is often based around “form”. My hypothesis is, “in FPL there is no such thing as form”. The problem comes from Apophenia. The tendency to perceive connections and meaning between unrelated things. This is also commonly thought of as the “Gambler’s fallacy”. The other issue, especially when considering form, is that people are very poor at thinking about ‘random’ patterns. Ask a group of people to spread themselves out randomly in a room and they inevitably place themselves evenly. In practice, randomness comes in ‘clumps’. These clumps are then mistakenly thought to be non-random. Take for example a player which scores 15 goals in a season. A random shuffle of these goals throughout the GWs may look like this: 1,1,1,1,0,1,0,0,1,0,0,1,0,1,0,0,0,0,0,0,0,1,1,0,1,1,0,0,1,0,0,1,0,0,0,1,0,0 (this random sequence was generated in Python). Where ‘1’s are goals. If this was a striker in FPL would everyone be transferring in in those first 6 weeks citing fantastic form thanks to a good rest over the summer and bemoaning the run of 7 games without a goal in the middle of the season? Perhaps during the busy festive periods? Perfectly reasonable, but we’re just finding patterns in randomness. We know this for certain since this pattern was generated randomly! Now, lets try to quantify the above with data. First, lets make our dataset. Consider every player over the last 2 seasons and for their points we will use the ‘adjusted’ points [AP] model (points with the appearance points removed). Further to this I have also removed any bonus points scored and tried to account for fixture difficulty. To account for fixture difficulty I have calculated the average AP scored and conceded by a team respectively. A (reasonable?) way to account for the difficulty is then removing these 2 numbers. For example, consider Man City v Watford. City players score on average 1.12 AP points per game and Watford concede 0.61. So in that match City players have 1.73 points removed. However Watford score 0.26 and Man City concede -0.02 so Watford players have 0.24 points removed. Then for each player, for each season, the average (mean) of their scores are removed. That leaves us with this kind of thing: Time series of avg. removed adjusted points for Sterling for the 2018/19 season, the gaps are games where he played no part. Looking across the whole dataset we have 20,875 data points. Of those 7,180 are positive (above the mean) and 13,767 are negative (below the mean). Distribution of points from this analysis. So, overall, there is a 34% chance that a player will exceed their average points in a given GW. Note this doesn’t vary hugely depending on the player. For 2 examples: in 2018/19 Sterling exceeded his average 38% of the time, in 2017/18 Maguire exceeded his 37% of the time. Using this value as a basis, we can work out how this compares with the number of “above mean” weeks in weeks following one above the mean. E.g. the impact of form. If form exists we would expect there to be groupings of “above mean” weeks rather than them being random. To draw this conclusion we need to know what the statistically expected values would be. To calculate those we assume that the scores are distributed by a binomial distribution. This is a distribution for calculating the number of successes (in this case above mean weeks) in a sequence of trials. Using a base rate of 34% (if we randomly pick a player and a GW there is a 34% chance that it is above the mean) we can compare stats with the actual data. First we look at “3 week form” that is, if a player scores above their mean in a given GW, what is the chance that they score above the mean at least once in the next 2 weeks? From the statistical distribution, we would expect there to be a 57% chance of this happening, and if we look at the cold hard numbers from the last 2 seasons then it happened 52% of the time. Pretty close! If we do the same for “4 week form” (following an above mean score the player posts another above mean score at least once in the next 3 weeks). The binomial distribution says that we should expect that 71% of the time, the data? 61%. “5 week form” stats says: 81%, our data: 72% and finally “6 wk form”, stats: 88%, data: 78%. So what conclusions can we draw from that? If a player scores above average (for that player) in a given GW, it has no impact on their expected returns in any of the following 5 weeks. Similarly if you look at something like the chance of “2 above mean weeks in 4”, or “3 above in 5” or any such combo, the same thing happens. The empirically calculated results are (very much) in line with the statistically expected values. Dare I conclude: There is no such thing as form in FPL. Clearly there are other short term factors which can impact a player, which keen FPL players should focus on. For example a short term injury to a key player could have a large impact on another (e.g. an injury to Kane might be a good time to bring in Moura/Son). Perhaps different positions or prices would experience it differently? Here is a plot showing that analysis. Values above the diagonal line are when we observe returns in the data more than expected (form). Whilst this shows a number of values above the line (suggesting form) they are not statistically significant (when breaking it down by position and price there are far fewer data points) and there doesn’t seem to be any clear patterns. For example whilst forwards between 9-10M show a deviation between observed and expected (they experience form) the opposite is seen for forwards between 8-9M and 10M+. Overall, this extra bit of analysis helps to support the conclusion that there is no observable impact of form in FPL. For more discussion checkout either the original Twitter thread, or the Reddit comments." }, { "title": "Maths on a Mug 8", "url": "/articles/2019/Maths_on_a_Mug_8/", "content": "This is the first #MathsOnAMug post related to my day job. I’ve been using a branch of statistics called ‘Extreme Value Theory’ in order to come up with a statistical analysis of extreme space weather events. What is Extreme Value Theory? At its core, Extreme Value Theory (EVT) is about studying the tail ends of probability distributions - the parts where extreme events occur. While most statistics deal with “typical” values close to the center of a distribution, EVT zooms in on the rare and unusual. It provides a mathematical framework to model the likelihood and magnitude of extremes in a given dataset. For example: Weather: EVT can estimate the probability of a 100-year flood or a record-breaking temperature. Finance: EVT helps assess the risk of catastrophic losses in financial markets. Engineering: EVT informs the design of structures like dams or bridges to withstand rare but severe stresses. How Does EVT Work? EVT works by identifying patterns in the extreme values of data. It primarily uses two key approaches: Block Maxima Method: Data is divided into fixed blocks (e.g., annual rainfall totals). The maximum value from each block is extracted. These maxima are analyzed to fit one of three extreme value distributions: Gumbel (light tails, for events like temperature extremes), Fréchet (heavy tails, for events like financial crashes), Weibull (bounded tails, for events like wave heights). Peaks Over Threshold (POT) Method: Instead of focusing on block maxima, this method examines all data points exceeding a certain threshold. These exceedances are modeled using the Generalized Pareto Distribution, which provides a flexible way to describe the tail of the distribution. Both methods aim to estimate the probability of extreme events and their expected severity. Why is EVT Important? Risk Assessment Extreme events often have disproportionate consequences. Whether it’s designing flood defenses, insuring against hurricanes, or managing financial risks, EVT provides a way to predict the likelihood of extreme scenarios and prepare accordingly. Safety and Resilience Engineers use EVT to ensure that infrastructure can withstand rare but catastrophic events. For example, bridges are built to endure not just the strongest winds observed historically, but also the strongest winds predicted by EVT. Understanding a Changing World In a world affected by climate change, the frequency and intensity of extreme weather events are shifting. EVT helps track and quantify these changes, guiding adaptation strategies. Applicability Across Disciplines From environmental science to cybersecurity, EVT is a universal tool. It can estimate the risk of rare events like devastating cyberattacks or even the spread of misinformation in social networks. The Bottom Line Extreme Value Theory gives us a lens to look beyond the ordinary, peering into the rare and unpredictable. By understanding the mathematics of extremes, we gain the tools to prepare for, and potentially mitigate, the most significant challenges nature and society throw our way. It’s a reminder that in life, as in statistics, the most unexpected events often carry the greatest consequences. EVT in Space Weather There has been a lot of work on using EVT in space weather, on a wide range of topics. Personally I have used it to estiamte the run times of solar flares, the aa index and the probability of the May 2024 solar storms: Elvidge, S., \u0026amp; Angling, M. J. (2018). Using extreme value theory for determining the probability of Carrington-like solar flares. Space Weather, 16, 417–421. https://doi.org/10.1002/2017SW001727 Elvidge, S. (2020). Estimating the occurrence of geomagnetic activity using the Hilbert-Huang transform and extreme value theory. Space Weather, 17, e2020SW002513. https://doi.org/10.1029/2020SW002513 Elvidge, S., \u0026amp; Themens, D. R. (2024). The Probability of the May 2024 Geomagnetic Superstorm. Authorea Preprints. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Not Pi Day, Feigenbaum Delta Day", "url": "/articles/2019/Not_Pi_Day/", "content": "Today everyone celebrates being confused why the 14th March (14/3) is celebrated as Pi Day. Instead let us celebrate what it should really be: Feigenbaum Delta Day. Mitchell Feigenbaum is an award winning mathematical physicist who’s research has made possible the systematic study of chaotic systems. Systems which are very sensitive to their initial conditions. Consider the double pendulum which is an example of chaotic motion. Minor changes in the initial position of the pendulum give totally different paths that are then mapped out. (Chaos theory is also often described using “a butterfly flapping its wings in Brazil can cause a hurricane in Texas”. There are issues with this analogy, but the fundamental idea is right, in chaotic systems small changes can have big impacts). Part of Feigenbaum’s groundbreaking work was in bifurcation theory. In dynamical systems a bifurcation occurs when a small smooth change made to a parameter of that system causes a sudden qualitative change in its behaviour. That means that at a bifurcation the stable properties of the thing you’re interested in (e.g. equilibria, periodic orbits) can change (locally). The easiest way (I find) to think of this is to consider a “period-doubling bifurcation”: doubling, quadrupling, octupling, … Image (credit [Wolfram](https://mathworld.wolfram.com/Bifurcation.html){:target=\"\\_blank\"}) of a logistic map (nicely described [here](http://www.kierandkelly.com/what-is-chaos/logistic-map/){:target=\"\\_blank\"}). The blue lines highlight the bifurcations where the red line is splitting, this is the onset of chaos. Feigenbaum looked at the ratio of the interval between bifurcations (in the period-doubling example we had above) and noticed something intriguing about it: it tends to a constant. If \\(a(n)\\) is the location of a (the nth) period-doubling bifurcation and a(n-1) is the location of the one before then: \\[R1 = a(n) – a(n-1)\\] is the distance between them. Similarly, \\[R2 = a(n-1) – a(n-2)\\] describes the distance between the previous bifurcations. The ratio we are interested in is then simply \\(R2\\) divided by \\(R1\\) (\\(frac{R2}{R1}\\)). You can calculate this ratio of intervals for any 3 consecutive bifurcations. For example, in the logistic map case (our earlier image) let’s read straight off the diagram three consecutive bifurcations: \\[a(n-2) = 3.5644073 [furthest left blue line],\\\\ a(n-1) = 3.5440903,\\\\ a(n) = 3.4494897\\] Calculating the ratio of the interval length gives us a value of 4.6562. If we did the same calculation for the next set of bifurcations we get a value of 4.6683, then 4.6686, then 4.6680, … This list of ratios goes on forever. If we could (somehow) look right at the end of the list we would see that this ratio (called \\(\\delta\\) [delta]) is a constant: 4.669201609102990671853203821… (Mathematically speaking we are looking at the limit as \\(n\\) tends to infinity) But \\(\\delta\\) doesn’t just appear for these period-doubling bifurcations. The same value arises for all 1D maps with a single quadratic maximum. Headline: all chaotic systems which meet this description bifurcate at the same rate! \\(\\delta\\) in bifurcation theory, is like \\(\\pi\\) in geometry and \\(e\\) in calculus. Fundamental. \\(\\delta\\) to 3 significant figures is 4.67. Today’s date, 14/3, is 4.67. Today is Feigenbaum Delta Day." }, { "title": "Pointless Scoring England Goalkeepers", "url": "/articles/2019/Pointless_scoring_GKs/", "content": "The TV show “Pointless” is based around correctly guessing obscure answers to questions. Season 5, Episode 48 had a question about England goalkeepers. Specifically the question was: We gave 100 people 100 seconds to name as many goalkeepers who have played for England men’s team as they could. (anyone who’s played in goal for England between 1966 and 2011 – no unused substitutes) Well this falls right into an area of something I am interested in, quizzes and football! The strategy seems obvious, the more caps a goalie received likely the more well known he would be and the more points you would score. So we want to find a player with as few caps as possible. Obviously I wanted to check the hypothesis. The theory holds – the fewer caps, the better the score in Pointless. There are two interesting players to mention. Robert Green – who for relatively few caps scores highly (it’s not hard to guess why) and Ron Springett who played 33 games for England, and was even part of the 1966 World Cup winning squad, but seems fairly unknown." }, { "title": "Maths on a Mug 7", "url": "/articles/2018/Maths_on_a_Mug_7/", "content": "I saw someone solving a Sudoku on the train this morning, which made again think about the minimum number of ‘clues’ (numbers in the grid) required to solve a puzzle. It has been proven that you need at least 17 clues to have a unique, solvable, solution. The proof is not for the faint hearted, for a long time it had been conjectured that 17 clues were the minimum necessary, but no one had actually proved it. The authors of the proof conducted an exhaustive computer search for 16-clue Sudoku puzzles and found none, thereby proving that the minimum number of clues required is indeed 17. Their approach involved several key steps: Cataloging All Possible Sudoku Grids: They generated a comprehensive list of all valid, completed Sudoku grids. Given the vast number of possible grids, this was a significant computational task. Developing an Efficient Search Algorithm: The team created a program named “checker” to systematically search each completed grid for potential 16-clue puzzles that have a unique solution. This required implementing efficient algorithms capable of handling the computational complexity involved. Utilizing Unavoidable Sets and Hitting Set Algorithms: They employed the concept of unavoidable sets—specific cell groupings that must contain at least one clue to ensure a unique solution. By identifying these sets, they could focus their search more effectively. To manage the computational challenges, they developed a novel algorithm for enumerating hitting sets, which are minimal sets of clues intersecting all unavoidable sets. This approach was crucial, as traditional backtracking algorithms would have been too slow for an exhaustive search. Conducting the Exhaustive Search: With their optimized algorithms, they performed a thorough search across all possible Sudoku grids. The absence of any valid 16-clue puzzles in their findings led them to conclude that at least 17 clues are necessary for a Sudoku puzzle to have a unique solution. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Penalty Shootouts are Stressful!", "url": "/articles/2018/Penalty_shootouts_are_stressful/", "content": "It is fair to say that during England’s (fantastic) World Cup win over Colombia last night I had my “heart in my mouth”. It was rather nerve-racking! Fortunately I was wearing my Fitbit and was able to download the timestamped data. The celebration when Dier scored was epic." }, { "title": "Maths on a Mug 6", "url": "/articles/2018/Maths_on_a_Mug_6/", "content": "I was out of the office today so didn’t have access to my chalkboard mug. Luckily I’ve got other mugs with maths on! This one has the Curtis MOG on it. Curtis (1976. It is a fantastic piece of work, fits wonderfully on a mug, and was partially to blame for me starting the ‘Maths on a Mug’ “thing”. Sadly these mugs are not available to buy online. You need to complete a Maths degree at the University of Birmingham, and then they’ll give you one on graduation day! Previous Maths on a Mug Next Maths on a Mug" }, { "title": "World Cup 2018 Predictions", "url": "/articles/2018/World_Cup_2018_Predictions/", "content": "The World Cup is almost upon us and so it’s time to make some predictions… My model, like a lot of others, is based on the World Football Elo ratings (WFER). Which is a system that uses the Elo rating system. You can read all about Elo on those two links, but basically after every game a team gains or loses points. The change in points is dependent on the two rankings of the teams. If there is a large difference between the two teams and the lower ranking team wins then they would gain a lot more points than if the higher ranking team won (since it is expected that they should win). The rating system gives different weightings for the tournaments that games take place in. They rank them in the following order: World Cup finals, Continental championship finals and major intercontinental tournaments, World Cup and continental qualifiers and major tournaments, All other tournaments, Friendly matches. The WFER site takes into account all games that results could be found for (so goes all the way back to the first official International football game, between Scotland and England on 30 November 1872, 0-0). My model uses a combination of ensemble machine learning methods for classification problems, random decision forests. Each decision tree in the ensemble is taken from a bootstrap sample of some training set where observations about an item (represented in the branches) can be used to drawn conclusions about the item’s target value (represented in the leaves). It is a standard technique which has a wide range of uses, including the evaluation of Wikipedia articles quality and importance. The model uses the WFER data for training, where all friendly matches have been removed (because very often sides don’t play a full strength team in those matches). Different combinations of training data results in different resulting models. Specifically, giving the model the rankings between the two teams and the result of the match (win, draw, loss) gives different results than if you just give the two teams rankings, but only results from major tournaments. For the final results the model has been trained using 76 decision trees with three different datasets: All (non-friendly) fixtures, All (non-friendly) fixtures since 1980, Fixtures from the World Cup, European Championships, Africa Cup of Nations and Copa America. The final results are then an average of the output of each of the three trained models. To test the success of the model part of the data (15% in this case) is held back and then compared against. After training the model, the accuracy of the model predictions are compared to the held back data. In this case the model was shown to be 94.8% accurate in predicting a win, draw or loss. The World Cup can then be simulated running the model for each match. The model returns probabilities of each of the outcomes so the same results don’t happen each time. The simulations are ran 10,000 times in order to get success probabilities for each team: ou can see the model predicts that Brazil have by far the greatest chance of winning the World Cup, over twice as much as the next most likely team, Germany. As I mentioned earlier in the post, a lot of people are making predictions of the World Cup results. A selection of those by EightyFivePoints, UBS, Gracenotes and Bet365 are compared below: These models (apart from Bet365’s, which is more mysterious and in the above plot is found by taking the odds for each team from the Bet365 website) all use the Elo ranking system in someway (click on the links to find more about each one). Modelling theory would suggest that, without any further details and assuming each of the above models are “good”, the best model would be the combination of them: Which gives Brazil a 19.6% chance, followed by Germany (15.0%) and Spain (11.6%; although this doesn’t take into account that they sacked their manager 2 days before their first game of the tournament!)." }, { "title": "Maths on a Mug 5", "url": "/articles/2018/Maths_on_a_Mug_5/", "content": "Last nights Powerball lottery in the US was an incredible $1.5 bn! But what were the odds of winning? You need to get 5 numbers from a pot of 69 in the main draw, and then one ‘bonus’ ball, from a pot of 26. So the odds are 69 choose 5 multiplied by 26, which gives you the equation on the mug. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Number of Days Between Easter's", "url": "/articles/2018/Num_Days_Between_Easter/", "content": "My three year old asked me a pretty interesting question this morning, “how many sleeps is it until next Easter?” Okay, on the one hand the question is pretty straight forward. Easter this year (2018) was on April 1st, next year (2019) Easter falls on April 21st, so 385 days. But what is more interesting is what is the range and distribution of the possible number of days between consecutive Easter’s? I’ve previously written about the distribution of pancake day/Shrove Tuesday. In that article I discussed the (very) complicated way that Easter is calculated and provide a computus for finding Easter Sunday. Looping through the years 1 to 9999 (the extreme limits of Python’s datetime module) we actually find that there are only 4 possible options for the number of days until next Easter: 350, 357, 378 and 385 days. Given that Easter can fall on any day between March 22nd and April 25th I think this is quite a surprising result. The four options are not evenly distributed: 350 days – 24.5% 357 days – 38.7% 378 days – 5.1% 386 days – 31.7% and we won’t have a 378 day Easter gap until 2021/22 and then 2048/49 after that." }, { "title": "Was there room on the door for two?", "url": "/articles/2018/Was_there_room_on_the_door_for_two/", "content": "Could Jack Dawson (Leonardo DiCaprio) and Rose DeWitt Bukater (Kate Winslet) from the motion picture ‘Titanic’ both have survived by both floating on the door? (Clearly there are a load of spoilers in this article – but the film did come out over 20 years ago so I’m not going to worry too much about it). [Note: This is a copy of an article I wrote in 2011, which is no longer available online so I’ve repeated it here for prosperity] Introduction During the 1997 motion picture ‘Titanic’ by James Cameron there is a famous scene involving the two main characters, Jack Dawson (as played by Leonardo DiCaprio) and Rose DeWitt Bukater (Kate Winslet) towards the end of the film. The scene involves Jack in the ocean and Rose floating on a wooden object, Jack dies and Rose survives, however the question we ask is, need Jack of died? Was there room (and spare buoyancy) for two? This is a question which appears repeatedly (e.g. [1] and [2]) on the internet but with varying answers, but hopefully this article can draw a conclusion to the discussion. The first point to settle is that Rose is not floating on a door and is instead floating on a piece of wood panelling. The panelling in the film is based upon a recovered piece, homed at the Maritime Museum of the Atlantic, in Halifax, Nova Scotia [3]. Figure 1. Screenshot from `Titanic’. Rose is on the panelling, Jack floats nearby. Image from a copyrighted film, the use of this image for critical commentary and/or discussion of the film and its contents qualifies as fair usage under copyright law. There are two main issues that need to be resolved: Could they both have fit on the panelling? Would their combined weight have been greater than the upward force due to buoyancy (i.e. would they have sank)? Eureka! In this section we introduce the basics of buoyancy, which will be crucial in the later sections. The history of buoyancy is a long one, and has provided one of the most famous stories in science. Archimedes supposedly exclaimed “Eureka!” when he discovered a fundamental principle of buoyancy, which relates buoyancy to displacement. Indeed this principle is still named after him, and first appears as Proposition 5 in [4]. Archimedes Principle: Any solid lighter than a fluid will, if placed in the fluid, be so far immersed that the weight of the solid will be equal to the weight of the fluid displaced. This tells us that the buoyancy force, \\(F_B\\), is equal to the weight, \\(W\\), of the object placed in the fluid, so \\(F_B=W\\). Weight is equal to the mass of the object times gravity, \\(g\\), and mass is equal to density, \\(\\rho\\), times volume, \\(V\\), giving rise to the more well known equation: \\[F_B=\\rho Vg.\\] Could they both fit? The first question to resolve is, “Could Rose and Jack have both fit on the panelling?”. The solution to this problem has been ‘demonstrated’ previously: Figure 2. Jack and Rose could have fit on the panel together [[5]](#5). However, both of them fitting on the panel is not where the problem lies. Variables Before we can dive into the problem of buoyancy we first need to consider a number of variables which will need to be approximated. Namely: What is the size of the panel? What is the density of the panel (what is it made from) What is the mass of Rose? What is the mass of Jack? What is the density of the water Panel Size First off we approximate the size of the panelling. A common mistake (or at least a common oversight) when approaching this problem is taking the panelling to be a rectangle. This is obvious from the overhead shot of the panelling and so I’ve created a more reasonable approximation of the panelling: Figure 3. The approximate shape of the panel. Now that we have the shape of the panelling we need to find the correct dimensions. All that is required is the length of one of the sides, and everything else can be calculated from that scale. From personal communication with the Maritime Museum of the Atlantic the size of the actual panel which the fictional one is based on is 150 cm by 105 cm by (~)3 cm (“the thickness of this panel is not recorded in the record, it is currently behind glass and is estimated at 3 cm”). However we can quickly see that the panelling in the film is larger than this, we can see this by comparing it with Rose’s height. It is a pretty safe assumption that Rose’s height is equal to Kate Winslet’s height, and she is quoted as saying she is 5 foot 6 inches (167.6 cm) [6]. This now gives the scale factor for the panelling (comparing the height of Rose to the length of the panel): Figure 4. Using the height of Rose to calculate the size of the panelling. Using this scaling factor gives the visible area of the panelling to be \\(1.91 m^2\\). This leaves the question of the depth of the panelling, something which is considerably harder to find. There are very few images that provide a clue of the depth, and they are not conclusive: Figure 5. Screenshot from 'Titanic'. Side on view of panelling. Figure 6. Screenshot from 'Titanic'. Side on view of panelling. Figure 7. Screenshot from 'Titanic'. Side on view of panelling. There are a multitude of issues with getting the scale from these images, mainly due to the angle of the camera. The first image seems to imply a depth of approximately 0.1 metres, which we find, via scaling, from Jack’s hand. However we can see that the edge which is above the water is clearly thicker than the rest of the door. Something that should be considered (especially when calculating the mass of the door). In the other figures we see that the panelling gets considerably thicker (about twice as thick again) further down (the section is underwater in the first figure). We are left to speculate whether this extra depth covers the whole of the panel, as in: Figure 8. One possibility of the side on view of the panelling. or just a small section as in: Figure 9. Another possibility of the side on view of the panelling. or anything in between. In both of these cases the head on view of the panelling is the same: Figure 10. Head on view of the panelling. There can be no way of knowing for sure which possibility is the correct one. However the first option would result in a considerable mass, which would probably result in the panelling not being able to float, which we know is a fallacy. So for the rest of the document we consider that we are in the second case. Rose’s Mass Again, let us assume that Rose’s mass is the same as Kate Winslet’s. This is hard to pin down, but via various internet sources this appears to be between 54 and 59kg [7] [8], so, for an approximation, we take the midpoint and use 56.5kg. It is also worth noting that the wet clothes that Rose was wearing would have added a considerable mass to this total. However, for simplicity, we ignore this factor. Jack’s Mass For Jack’s mass we use Leonardo DiCaprio’s mass, which is between 72.5 and 75kg [9]. So, for our calculations, we use 74kg. Again this would have been greater due to the wet clothes. Panel Density The density of the wood panelling is key in our study as this will determine the mass of the panel, which is an important factor on whether the panelling will sink or float. We can immediately deduce some bounds on the density, thanks again to the Maritime Museum of the Atlantic in Halifax. According to the museum the piece of panelling is made of oak. Then from [10] the density of oak varies between 600 and 900 \\(kg m^{−3}\\). Initially a value of 750 \\(kg m^{−3}\\) (the midpoint) is used and then later look at what values of panel density would require which panel depths. Panel Mass Using the panel density the mass of the panel can be calculated. The volume of the panel in Figure 9 can be found by calculating the volume of the three individual sections which make up the panel. The ‘top’ section has a volume of \\(0.013 m^3\\). The ‘middle’ section \\(0.19 m^3\\) and finally the ‘bottom’ section is \\(0.011 m^3\\). This gives a total volume of \\(0.214 m^3\\). Which, using the density value from the previous section, equates to a mass of \\(160.5 kg\\). Water Density The density of water depends on two main factors, its temperature and the water salinity. On the night of the Titanic’s sinking the water temperature was reported to be 31 degrees Fahrenheit [11] [12], which is approximately −0.56C. The salinity of ocean water varies with location. The Atlantic Ocean, where the Titanic sank, varies between 33 and 37 parts per thousand. According to NASA [13] the approximate salinity value of the Atlantic at the location where the Titanic sank (41.725, -130.05) is 36 parts per thousand. We can then use the water density calculator provided at [14] with the above values to find a calculated water density of $1029 kg m^{−3}$. Buoyancy We now come to the crux of the problem. Could the panelling have supported both Jack and Rose? First we consider the case which occurred in the film, Rose lying on the panelling. To do this we consider the forces acting upon the panelling, the downward force due to weight, and the upward force due to buoyancy. The only variable we do not know is the displaced volume. Since we know the area of the panel which is in contact with the water we are left to find the height that makes up the submerged portion of the panelling. Thus we actually find the depth to which the panelling sinks, clearly we require this to be greater than the total depth of the panel, in order for some of it to remain above water. We equate the buoyancy force, \\(F_B\\), with the total weight in the system. We use \\(M_R\\), \\(M_P\\) and \\(M_J\\) to represent the mass of Rose, the panel and Jack respectively. \\[\\begin{eqnarray*} F_B \u0026amp;=\u0026amp; W, \\\\ \\rho Vg \u0026amp;=\u0026amp; (M_R+M_P)g,\\\\ \\rho V \u0026amp;=\u0026amp; M_R+M_P,\\\\ 1029\\times 1.9\\times H \u0026amp;=\u0026amp; 56.5+160.5,\\\\ 1955H \u0026amp;=\u0026amp; 217,\\\\ H \u0026amp;=\u0026amp; 0.111. \\end{eqnarray*}\\] Which says that the door sinks 11.1cm, but we only assumed a panel depth of 10cm! Thus, using our assumptions, Rose could not have floated on top of the panel and been completely above the water. However, since the only real knowledge of this system is that Rose can float on the panel we must adjust our assumptions. The first assumption to try changing would be our choice of wood density, since this was chosen to be the midpoint of a fairly large range. By rearranging Equation 5 we can plot a graph of densities against how much the panelling would sink: Figure 11. Graph of wood density against depth the panel would sink to. This shows that the least the panel would sink is approximately 0.095 metres, which is less than the current assumed depth of the panel. Thus, if we assume a wood density of $600 kg m^{−3}$ then Rose could, like in the film, float on the panelling. This value of wood density gives a panel mass of 128.4 kg. Using this panel mass we can calculate that the panelling would sink 0.066 metres if nothing was on it, i.e. there would be approximately 3.5 centimetres floating above the water. This is a reasonable value, especially when you consider the Figure below: Figure 12. Screenshot from `Titanic’. Rose swimming away from the panelling, you can see the panel, floating, in the background. If we now repeat the above process, using the new panel mass, but also include Jack’s mass we can see whether Jack could also have floated on the panel. \\[\\begin{eqnarray*} \\rho Vg \u0026amp;=\u0026amp; (M_R+M_P+M_J)g,\\\\ \\rho V \u0026amp;=\u0026amp; M_R+M_P+M_J,\\\\ 1029\\times 1.9\\times H \u0026amp;=\u0026amp; 56.5+128.4+74,\\\\ 1955H \u0026amp;=\u0026amp; 258.9,\\\\ H \u0026amp;=\u0026amp; 0.132. \\end{eqnarray*}\\] So the panel would have sank 0.132 metres, or just over 13 centimetres, some 3 centimetres greater than the depth of the panelling. Conclusion In summary, both Jack and Rose could have fitted on the piece of wooden panelling, but had they both have got on, the panel would have been submerged under water by 3.2 centimetres. However, it is likely that should they both have been on the panelling, they both would have survived (especially if they had sat). This is since the cooling rate is directly related to the amount of exposed surface area and body parts immersed in the water and the best chance of surviving accidental immersion in cold water (\u0026lt;20C) is to keep as much of the body as possible out of the water at all times [12]. Jack certainly would have had a much better chance of survival, compared to the almost zero percent chance he had by staying in the water: Figure 13. Human survival time in cold water. The curves in the Figure indicate the expected survival times at different levels of body insulation in cold water. Low insulation is light or no clothing, medium is wet suit, and high is dry suit. Dashed lines indicated ranges at low or high body mass [12]. This shows that the length of time Jack could have survived in the water would have been approximately 30 minutes. Indeed, only one person who entered the water after the sinking of the Titanic, and was in it for more than a few minutes, survived the ordeal [12]. So there was room on the door (panelling) for two. References [1] https://funypicforyou.blogspot.co.uk/2012/05/titanic-floating-door-facts-explained.html [2] https://www.reddit.com/r/funny/comments/sf1v2/ill_never_let_go_the_titanic_door_experiment/ [3] http://www.imdb.com/title/tt0120338/trivia?tab=tr\u0026amp;item=tr1076740 [4] Archimedes of Syracuse (1897) The Works of Archimedes, edited in modern notation, with introductory chapters by T.L. Heath [5] https://www.huffingtonpost.co.uk/2012/04/18/%20jack-and-rose-titanic-plank-of-wood_n_1434092.html [6] http://news.bbc.co.uk/1/hi/entertainment/2751869.stm [7] https://www.celebheights.com/s/Kate-Winslet-109.html [8] http://www.bangla2000.com/entertainment/celebrities/kate_winslet.shtm [9] https://starsunfolded.com/leonardo-dicaprio/ [10] https://www.engineeringtoolbox.com/wood-density-d_40.html [11] Haisman D. (2009) Titanic: The Edith Brown Story. AuthorHouse, Bloomington, Indiana. [12] Piantadosi, Claude A. (2003) The Biology of Human Survival: Life and Death in Extreme Environments. Oxford University Press, Oxford, United Kingdom. [13] https://salinity.oceansciences.org/ [14] http://www.csgnetwork.com/h2odenscalc.html" }, { "title": "Unblock IP address from connecting via SSH", "url": "/articles/2018/Unblock_IP_on_Linux/", "content": "I manage a small cluster at work and occasionally students get their IP address banned by entering the wrong password when logging into the cluster. I don’t fix the problem often enough to remember how to do it, so I thought I’d jot it down here for future reference. The blocking of IPs is controlled by Fail2ban, which scans log files for IPs which look malicious (e.g. too many password failures, looking for exploits, etc.) and bans them. It does this by updating firewall rules to reject the specific IP. To see the list of what is being blocked run (in a terminal): sudo iptables -L --line-numbers That will give you something like: Chain INPUT (policy ACCEPT) num target prot opt source destination 1 ACCEPT udp -- anywhere anywhere udp xxx:xxxxxx 2 ACCEPT tcp -- anywhere anywhere tcp xxx:xxxxxx 3 ACCEPT udp -- anywhere anywhere udp xxx:xxxxxx 4 ACCEPT tcp -- anywhere anywhere tcp xxx:xxxxxx 5 f2b-sshd tcp -- anywhere anywhere multiport dports ssh 6 REJECT tcp -- anywhere anywhere tcp spts:xxxxxx:xxxxxx dpt:ulistproc reject-with icmp-port-unreachable 7 REJECT udp -- anywhere anywhere udp spts:xxxxxx:xxxxxx dpt:ulistproc reject-with icmp-port-unreachable 8 ACCEPT all -- anywhere anywhere 9 ACCEPT all -- anywhere anywhere 10 ACCEPT tcp -- anywhere anywhere tcp xxx:xxxxxx state NEW 11 ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED 12 ACCEPT tcp -- xxx.xxx.xxx.xxx/24 anywhere tcp xxx:xxxxxx state NEW 13 ACCEPT tcp -- xxx.xxx.xxx.xxx/24 anywhere tcp xxx:xxxxxx state NEW 14 REJECT udp -- anywhere anywhere udp xxx:xxxxxx reject-with icmp-port-unreachable 15 REJECT tcp -- anywhere anywhere tcp xxx:xxxxxx reject-with icmp-port-unreachable 16 REJECT tcp -- anywhere anywhere tcp xxx:xxxxxx reject-with icmp-port-unreachable 17 REJECT tcp -- anywhere anywhere tcp xxx:xxxxxx:0:1023 reject-with icmp-port-unreachable 18 REJECT udp -- anywhere anywhere udp xxx:xxxxxx:0:1023 reject-with icmp-port-unreachable Chain FORWARD (policy DROP) num target prot opt source destination 1 ACCEPT all -- anywhere xxx.xxx.xxx.xxx/24 ctstate RELATED,ESTABLISHED 2 ACCEPT all -- xxx.xxx.xxx.xxx/24 anywhere 3 ACCEPT all -- anywhere anywhere 4 REJECT all -- anywhere anywhere reject-with icmp-port-unreachable 5 REJECT all -- anywhere anywhere reject-with icmp-port-unreachable 6 ACCEPT all -- anywhere anywhere state RELATED,ESTABLISHED 7 ACCEPT all -- anywhere anywhere Chain OUTPUT (policy ACCEPT) num target prot opt source destination 1 ACCEPT udp -- anywhere anywhere udp dpt:bootpc Chain f2b-sshd (1 references) num target prot opt source destination 1 REJECT all -- xxx.xxx.xxx.xxx anywhere reject-with icmp-port-unreachable 2 REJECT all -- 199.188.177.166 anywhere reject-with icmp-port-unreachable 3 RETURN all -- anywhere anywhere What we are interested in is the ‘Chain f2b-sshd’ part at the bottom of the list. Here we can see the (made up) IP address 199.188.177.166 is being banned (“REJECT”). That means that any user trying to SSH into the machine from that IP address will fail. If you have verified that that is the IP address you are expecting and know it is safe to unblock then we can delete the rule using the Chain name (f2b-sshd) and line number (2) using “iptables -D CHAIN LINE_NUMBER” so in this specific case: sudo iptables -D f2b-sshd 2 To make the change permanent you must save your changes: sudo iptables-save" }, { "title": "Probability of World Cup Group Draw", "url": "/articles/2017/Prob_WC_Draw/", "content": "The lineup for the 2018 FIFA World Cup is now complete, and the teams have been sorted into pots ahead of the group stage draw on December 1st. All draws are not equally likely, so what teams is your team likely to face? The format for the group stage was decided in September. Each team has been allocated a pot (1 to 4) based on “sporting principles”. The FIFA World Rankings (as of October 2017) were used to rank the teams in descending order (after Russia, who, as hosts, are the top seeds). The pots are: Russia Spain Denmark Serbia Germany Peru Iceland Nigeria Brazil Switzerland Costa Rica Australia Portugal England Sweden Japan Argentina Colombia Tunisia Morocco Belgium Mexico Egypt Panama Poland Uruguay Senegal Korea Republic France Croatia Iran Saudi Arabia The rules for the draw are described here (with a handy video explanation). Roughly speaking the rules are that, after Russia, who will definitely be assigned to Group A, a random team from each pot will be drawn, and then placed in the next available group (alphabetically) which they are allowed to be put in (subject to constraints such as splitting up the confederation). This process is repeated until all teams have been drawn. However, because of the rules of splitting confederations, each permutation of the 4 pots is not actually possible. So if you focus on one team you can look at what the probabilities of drawing other teams are. For example, the different probabilities of the draw for England are: Pot 1 Pot 2 Pot 3 Pot 4 Russia (12.5%) Denmark (7.1%) Serbia (6.5%) Germany (9.5%) Iceland (7.1%) Nigeria (12.0%) Brazil (20.0%) Costa Rica (17.0%) Australia (13.5%) Portugal (9.5%) England Sweden (7.1%) Japan (13.5%) Argentina (20.0%) Tunisia (15.4%) Morocco (12.1%) Belgium (9.5%) Egypt (15.4%) Panama (15.5%) Poland (9.5%) Senegal (15.4%) Korea Republic (13.5%) France (9.5%) Iran (15.4%) Saudi Arabia (13.5%) The above was created using 1,000,000 simulations. If you want to run your own experiment (with any team of your choice). The code I used can be downloaded here. Running 100,000 simulations will take about 3 minutes. To change the number of simulations and/or the team of interest just edit the first two lines of the code (specifically change the variables sims and pickedTeam). As for England, a South American giant in the form of either Brazil or Argentina from pot 1 is a daunting, but realistic, prospect for us in the Summer. Update England ended up being drawn in Group G alongside Belgium, Tunisia and Panama, the probabilities of this were: Pot Country Percentage Pot 1 Belgium 9.5% Pot 2 England - Pot 3 Tunisia 15.4% Pot 4 Panama 15.5%" }, { "title": "Maths on a Mug 4", "url": "/articles/2017/Maths_on_a_Mug_4/", "content": "This is one of my favourite puzzles. It is a simple, life long game: Using exactly four 4s make as many numbers as you can. You can use the operators \\(+,−,×,\\div,!,\\hat{},.,\\sqrt{},\\Gamma\\) and concatenation. You must use all four 4s. My advice, start at 0 and just work your way up, I save doing these puzzles in my head for times when I have to physically, but not mentally, present. Let me start you off with a couple: \\[\\begin{eqnarray*} 0 \u0026amp;=\u0026amp; 44 - 44, \\\\ 1 \u0026amp;=\u0026amp; \\frac{44}{44},\\\\ 2 \u0026amp;=\u0026amp; \\frac{4}{4} + \\frac{4}{4}. \\end{eqnarray*}\\] The value on the mug is a solution for the number 169. You can quite comfortabely get the first 100 numbers, and then things start to ramp up in difficulty. If you are really keen, the answer to the first 40,000 numbers is available here. Credit to David A. Wheeler, who compiled the “The Definitive Four Fours Answer Key”. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Maths on a Mug 3", "url": "/articles/2017/Maths_on_a_Mug_3/", "content": "Yesterday my (by far) most popular tweet was a picture of a rainbow. So I thought I would do a little diagram of what causes us to see a rainbow. The colours are reversed for a double rainbow because there is a second reflection. Rainbows are always a captivating natural phenomena that people can’t help but point out to one another and they arise from the interplay of light and water droplets in the atmosphere. When sunlight encounters a spherical water droplet, part of it reflects off the droplet’s surface, while another portion enters the droplet, undergoing refraction—a bending of light due to a change in medium. This refracted light travels through the droplet, reflects off the inner surface, and refracts again as it exits. The degree of bending, or refraction, depends on the light’s wavelength. Shorter wavelengths (blue and violet light) refract more than longer wavelengths (red and orange light). This variation causes the dispersion of white sunlight into its constituent colors, forming a spectrum. The refractive index of water, which measures how much light slows down in the medium, varies slightly with wavelength—approximately 1.34 for violet light and 1.33 for red light. This small difference is sufficient to separate sunlight into the vivid colors observed in a rainbow. The formation of a rainbow involves specific angles. As light enters and exits the droplet, the angles of incidence and refraction are related by Snell’s law, which states that the ratio of the sines of these angles is equal to the ratio of the refractive indices of the two media. For a given wavelength, there exists a minimum deviation angle at which the emerging light is most intense. For red light, this angle is approximately 42 degrees relative to the incoming sunlight. This concentration of light at specific angles results in the circular arc of a rainbow. The observer’s position is crucial in perceiving a rainbow. The Sun must be behind the observer, and the water droplets must be in front. The angle at which the light exits the droplets and reaches the observer’s eye determines the specific color seen at each point along the arc. This geometric arrangement explains why rainbows are often seen when the sun is low in the sky, such as during early morning or late afternoon, and why they appear as circular arcs centered opposite the Sun. A detailed description of the mathematics involved was written up in an excellent article in Plus Magazine. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Complex Reflection Groups", "url": "/articles/2017/Complex_Reflection_Groups/", "content": "After a spring clean I came across a talk I gave on the classification of complex reflection groups back in March 2011. Nostalgia set in and I started to read through my notes. Whilst it only deals with the imprimitive complex reflection groups it seemed like a decent summary of the theory. So I thought I would digitise it and leave it here in case anyone ever found it useful. If you want to read (a lot) more about Complex Reflection Groups my undergraduate masters dissertation is available here." }, { "title": "Distribution of Pancake Day", "url": "/articles/2017/Distribution_Pancake_Day/", "content": "Today is pancake day, and it got me thinking about the fact that it once fell on my birthday (March 7th). How long will I have to wait until it next falls on my birthday? Pancake day, or Shrove Tuesday, is the day before the first day of Lent, or exactly 47 days before Easter Sunday. So to calculate when Pancake day will be we just need to work out when Easter Sunday is. The Economist sums this up nicely: “According to the Bible, Jesus held the Last Supper with his disciples on the night of the Jewish festival of Passover, died the next day (Good Friday) and rose again on the third day (the following Sunday). The beginning of Passover is determined by the first full moon after the vernal equinox, which can occur on any day of the week. To ensure that Easter occurs on a Sunday, the Council of Nicaea therefore ruled in 325AD that Easter would be celebrated on the Sunday after the first full moon on or after the vernal equinox. But there’s a twist: if the full moon falls on a Sunday, then Passover begins on a Sunday, so Easter is then delayed by a week to ensure that it still occurs after Passover. To confuse matters further, the council fixed the date of the vernal equinox at March 21st, the date on which it occurred in 325AD (though it now occurs on March 20th), and introduced a set of tables to define when the full moon occurs that do not quite align with the actual astronomical full moon (which means that, in practice, Easter can actually occur before Passover).” Fortunately we can use a computus to determine the date of Easter Sunday. There are many approaches to calculating the date but the one I’ve gone for (in Python) is: import datetime as dt def easterDate(year): a = year % 19 b = year//4 c = (b//25) + 1 d = (c*3)//4 e = ((a*19) - ((c*8 + 5)//25) + d + 15) % 30 f = e + (29578 - a - e*32)//1024 g = f - ((year % 7) + b - d + f + 2) % 7 h = g//32 day = g - h*31 month = h + 3 return dt.date(year, month, day) print(easterDate(2017)) # 2017-04-16 And so Pancake date is found by subtracting 47 days: print(easterDate(2017) - dt.timedelta(days=47)) # 2017-02-28 Today’s date! We get the distribution of Pancake days by looping through all the years which Python’s datetime module can deal with (1900 to 9999) and get: The earliest date that Pancake day can fall on is Feb 3rd (the earliest date for Easter is March 22nd, which occurs if the full moon falls on Saturday March 21st). The latest pancake day is March 9th (that gives the latest Easter as April 25th which needs quite a specific set up – the full moon would have to occur March 20th, meaning the first full moon after March 21st would be April 18th. If however April 18th is a Sunday then Easter is celebrated the following Sunday, April 25th). We can see from the distribution that the tail of these extreme dates occur quite rarely, they then ramp up to a fairly even distribution of dates between Feb 9th and March 4th (excluding Feb 29th, which has an obvious dip). Unfortunately my birthday falls quite far down the tail of the distribution, March 7th, only two days earlier than the latest possible Pancake day. Only 1.67% of Pancake days fall on that date. Which, given that the UK life expectancy for men is 81.5 years, means I should only expect to live to see 1, beautiful, pancake-birthday, which I did, on March 7th 2000. I’ll have to wait (and try and hold out) until 2079 to see another. Here is a link, to the code I used to for this post." }, { "title": "Relationship between Car Insurance Price and Population Density", "url": "/articles/2017/Car_Insurance_Pop_Den/", "content": "There was some discussion in my office today about the correlation between car insurance prices and where you live. The idea being that the prices may be correlated with population density. Exactly how car insurers create their prices is not clear. But there is plenty of discussion online that it is, at least in part, related to your postcode. Many explain this relationship between price and postcode as being down to the crime rate in the area. However one of the PhD students in my office (Steve) said that it was to do with population density in the individual postcodes (the basic idea being more people = more accidents). Indeed, it has already been shown that crime rate is correlated to population density. So showing a relationship between car insurance categories and population density, also shows a correlation between the categories and crime rate (as the online discussion suggests). So this all seemed like an interesting thought and so I went online and grabbed some data. The car insurance prices are split into 7 categories (‘A’ – ‘F’ and ‘Refer’) from here (with original data from motorcarinsuranceuk.co.uk which is no longer availablle). The site mentioned that the data is as “accurate as far as we know but there could be slight differences”. ‘A’ is a ‘good postcode / low risk’ and ‘Refer’ means that an individual would need to contact the individual car insurers (I take this to mean that ‘Refer’ is worse than an ‘F’). This conveniently gives a breakdown of category by postcode. The population density of each postcode can be found from the Office for National Statistics. Specifically from table QS102EW. Creating a box plot (including the mean) of the data (insurance category against population density) shows that there is a correlation between population density and car insurance category as Steve expected. The lower the population density the better category car insurance you can get. The plot was created using the Seaborn: statistical data visualization package." }, { "title": "Number of Unique Song Choices on Desert Island Discs", "url": "/articles/2017/Unique_DID/", "content": "Desert Island Discs celebrates its 75th birthday today, and today’s guest is David Beckham. Beckham will pick 8 song choices to take away with him on a desert island. What I wondered was, will all of his song choices have been picked by guests previously? With over 3,000 episodes there are going to be a lot of repetitions. Obviously the very first guest, Vic Oliver had to pick 8 unique song choices. There was no one before him! The second guest, James Agate also picked 8 unique choices (they were all different to Vic Oliver’s). This continued until the 8th guest, Joan Jay, who only had 7 unique choices (she picked Ave Maria by Charles‐François Gounod the same as Pat Kirkwood). A quick scrape of the BBC 4 webpage gives us all of the song choices of each of the guests. Plotting the data as a histogram of ‘unique’ choices leads to a not unsurprising result: On average a castaway picks 3 unique choices, but really anything between 2 and 5 unique choices is quite probable. Picking 0 unique choices (~100 out of 3096 castaways) and all (8) unique choices (~50 out of 3096 castaways) is not very likely. The thing that is rather interesting though is if you look at the number of unique choices as a time series: The data is obviously very noisy, with 0 and 8 unique choices scattered throughout the time series. But there is a clear trend. I’ve fitted a curve (least-squares) to the data and the results hopefully become clear. There was a decrease in the number of unique choices from the start of the show down until the mid 1980s, and since then there has been an increase again! I would guess that this has something to do with the explosion of popular music, the early choices are dominated by “famous” classical music pieces, but this has decreased over time." }, { "title": "Number of days until first Premier League manager sacking", "url": "/articles/2016/Prem_Sackings/", "content": "I’ll get straight to the point, on average the first Premier League manager is sacked after 77 days. Following the sacking of Francesco Guidolin from Swansea City today (52 days into the season), I wondered how long into the season the first manager is sacked on average. I’m counting the total number of days from the first match of the season to the first sacking (and I mean sacking, I’ve excluded managers leaving by mutual consent, or moving to a different job). The plot is below. Moral of the story is: mean is 77 days and the standard deviation is 47 days." }, { "title": "Maths on a Mug 2", "url": "/articles/2016/Maths_on_a_Mug_2/", "content": "This is a classic Martin Gardner puzzle. One solution is: Construct the additional squares indicated by dotted lines. Angle C is the same as the sum of angles A and D, since they are both made by cutting a square in half along the diagonal. Angle B equals angle D because they are corresponding angles of similar right triangles (made by cutting a rectangle made from two squares in half along the diagonal). That means B can be substituted for D, which automatically makes the C equals the sum of A and B. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Maths on a Mug 1", "url": "/articles/2016/Maths_on_a_Mug_1/", "content": "For Christmas I got a chalkboard mug. Following the suggestion of someone in the office I wrote a maths equation on it, and tweeted it. Now I can’t stop. The only problem is a mug isn’t really a lot of space so I’ve been writing very little on the mug. Leaving the interested reader to find out more if they want, this page will explain a bit more about what is on the mug and provide links for more information. This one is easy to describe, it is simply, in my opinion (and many others!) the most beautiful equation in mathematics. What Does It Mean? At first glance, \\(e^{i\\pi} + 1 = 0\\) might look like a random collection of symbols, but it represents a perfect harmony of fundamental mathematical ideas. Here’s a breakdown of its components: The number \\(e\\): This mathematical constant, approximately 2.718, is the base of natural logarithms. It appears in many areas of mathematics and science, such as exponential growth, decay, and systems like population dynamics or compound interest. The symbol \\(i\\): This is the imaginary unit, defined as \\(i = \\sqrt{-1}\\). While the term “imaginary” might sound abstract, \\(i\\) is a powerful tool used in engineering, physics, and other sciences. The number \\(\\pi\\): Approximately 3.14159, \\(\\pi\\) is the ratio of a circle’s circumference to its diameter. It’s fundamental in geometry and pops up in countless mathematical and scientific applications. The operation \\(e^{i\\pi}\\): This combines exponential growth (\\(e\\)), rotation through imaginary numbers (\\(i\\)), and the concept of \\(\\pi\\) as a measure of a full rotation in radians. Astonishingly, \\(e^{i\\pi}\\) equals \\(-1\\), a completely real number. Adding 1: Adding 1 to -1 gives us 0, a number symbolizing balance or nothingness. Why Is It Beautiful? Euler’s identity connects five of the most important numbers in mathematics (\\(e, i, \\pi, 1,\\) and \\(0\\)) in a way that is both simple and profound. Here’s why it’s considered so elegant: Simplicity and Elegance: The equation uses just three basic operations—exponentiation, addition, and equality—to create a profound relationship between these numbers. Universality: Each component (\\(\\pi\\), \\(e\\), \\(i\\)) comes from a completely different area of mathematics, yet they converge seamlessly. Unexpected Unity: It’s surprising that imaginary numbers, abstract concepts like \\(e\\), and geometric constants like \\(\\pi\\) could relate so closely. A Deeper Truth: The equation emerges naturally from complex numbers and their geometric interpretation, showing how abstract ideas can intertwine beautifully. The Legacy of Euler’s Identity For mathematicians and enthusiasts, Euler’s identity is more than just an equation—it’s a reminder of the elegance and interconnectedness within mathematics. It’s often described as: “The mathematical equivalent of a sonnet.” “A symphony in symbols.” Its simplicity, depth, and universality make it a source of awe and inspiration, proving that even in the abstract world of numbers, beauty exists. Previous Maths on a Mug Next Maths on a Mug" }, { "title": "Easy way to remember sin, cos and tan values", "url": "/articles/2015/sin_cos_tan/", "content": "One of my tutees showed me an easy way to remember sine and cosine values for the common angles. Once you’ve got those, you can also determine tangent, cosecant, secant, and cotangent values. Often, students need to know the sine and cosine values of the most common angles: $0$, $\\frac{\\pi}{6}$, $\\frac{\\pi}{4}$, $\\frac{\\pi}{3}$, and $\\frac{\\pi}{2}$ (or 0°, 30°, 45°, 60°, and 90°). Fortunately, it’s remarkably easy to work these out. First, write down the angles we are looking for: \\[\\begin{array}{cccccc} \u0026amp; 0 \u0026amp; \\frac{\\pi}{6} \u0026amp; \\frac{\\pi}{4} \u0026amp; \\frac{\\pi}{3} \u0026amp; \\frac{\\pi}{2} \\\\ \u0026amp; 0^\\circ \u0026amp; 30^\\circ \u0026amp; 45^\\circ \u0026amp; 60^\\circ \u0026amp; 90^\\circ \\\\ \\hline \\end{array}\\] To determine the sine values, write down the digits 0 to 4: \\[\\begin{array}{cccccc} \u0026amp; 0 \u0026amp; \\frac{\\pi}{6} \u0026amp; \\frac{\\pi}{4} \u0026amp; \\frac{\\pi}{3} \u0026amp; \\frac{\\pi}{2} \\\\ \u0026amp; 0^\\circ \u0026amp; 30^\\circ \u0026amp; 45^\\circ \u0026amp; 60^\\circ \u0026amp; 90^\\circ \\\\ \\hline \\sin \u0026amp; 0 \u0026amp; 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; 4 \\\\ \\end{array}\\] Next, take the square root of each of those numbers: \\[\\begin{array}{cccccc} \u0026amp; 0 \u0026amp; \\frac{\\pi}{6} \u0026amp; \\frac{\\pi}{4} \u0026amp; \\frac{\\pi}{3} \u0026amp; \\frac{\\pi}{2} \\\\ \u0026amp; 0^\\circ \u0026amp; 30^\\circ \u0026amp; 45^\\circ \u0026amp; 60^\\circ \u0026amp; 90^\\circ \\\\ \\hline \\sin \u0026amp; \\sqrt{0} \u0026amp; \\sqrt{1} \u0026amp; \\sqrt{2} \u0026amp; \\sqrt{3} \u0026amp; \\sqrt{4} \\\\ \\end{array}\\] Then, divide each by 2: \\[\\begin{array}{cccccc} \u0026amp; 0 \u0026amp; \\frac{\\pi}{6} \u0026amp; \\frac{\\pi}{4} \u0026amp; \\frac{\\pi}{3} \u0026amp; \\frac{\\pi}{2} \\\\ \u0026amp; 0^\\circ \u0026amp; 30^\\circ \u0026amp; 45^\\circ \u0026amp; 60^\\circ \u0026amp; 90^\\circ \\\\ \\hline \\sin \u0026amp; \\frac{\\sqrt{0}}{2} \u0026amp; \\frac{\\sqrt{1}}{2} \u0026amp; \\frac{\\sqrt{2}}{2} \u0026amp; \\frac{\\sqrt{3}}{2} \u0026amp; \\frac{\\sqrt{4}}{2} \\\\ \\end{array}\\] Simplifying these terms: \\[\\begin{array}{cccccc} \u0026amp; 0 \u0026amp; \\frac{\\pi}{6} \u0026amp; \\frac{\\pi}{4} \u0026amp; \\frac{\\pi}{3} \u0026amp; \\frac{\\pi}{2} \\\\ \u0026amp; 0^\\circ \u0026amp; 30^\\circ \u0026amp; 45^\\circ \u0026amp; 60^\\circ \u0026amp; 90^\\circ \\\\ \\hline \\sin \u0026amp; 0 \u0026amp; \\frac{1}{2} \u0026amp; \\frac{\\sqrt{2}}{2} \u0026amp; \\frac{\\sqrt{3}}{2} \u0026amp; 1 \\\\ \\end{array}\\] To find the cosine values, write the numbers from 4 down to 0, then take the square root of each and divide by 2: \\[\\begin{array}{cccccc} \u0026amp; 0 \u0026amp; \\frac{\\pi}{6} \u0026amp; \\frac{\\pi}{4} \u0026amp; \\frac{\\pi}{3} \u0026amp; \\frac{\\pi}{2} \\\\ \u0026amp; 0^\\circ \u0026amp; 30^\\circ \u0026amp; 45^\\circ \u0026amp; 60^\\circ \u0026amp; 90^\\circ \\\\ \\hline \\sin \u0026amp; 0 \u0026amp; \\frac{1}{2} \u0026amp; \\frac{\\sqrt{2}}{2} \u0026amp; \\frac{\\sqrt{3}}{2} \u0026amp; 1 \\\\ \\cos \u0026amp; 1 \u0026amp; \\frac{\\sqrt{3}}{2} \u0026amp; \\frac{\\sqrt{2}}{2} \u0026amp; \\frac{1}{2} \u0026amp; 0 \\\\ \\end{array}\\] Using these values and basic trigonometric identities, you can find all other trigonometric functions. The key identities to remember are: \\[\\begin{align*} \\tan(\\theta) \u0026amp;= \\frac{\\sin(\\theta)}{\\cos(\\theta)} \\\\ \\sec(\\theta) \u0026amp;= \\frac{1}{\\cos(\\theta)} \\\\ \\csc(\\theta) \u0026amp;= \\frac{1}{\\sin(\\theta)} \\\\ \\cot(\\theta) \u0026amp;= \\frac{1}{\\tan(\\theta)} \\end{align*}\\] With this approach, remembering these trigonometric values becomes straightforward." }, { "title": "Correlation between Astronaut name and Apollo mission", "url": "/articles/2015/Apollo_Astro_Corr/", "content": "Python’s matplotlib includes the impressive XKCDify feature, which allows plots to resemble Randall Munroe’s style from xkcd.com. Upon discovering this feature (matplotlib.pyplot.xkcd()), I sought a creative application beyond replicating classic XKCD plots like xkcd.com/653 and xkcd.com/1220. Deke Slayton, the chief astronaut, selected the crews for the Apollo and Gemini missions, the exact process he used has been somewhat mysterious… One day, while walking to work, I recalled that Armstrong, Aldrin, and Collins were on Apollo 11—all near the beginning of the alphabet. Similarly, Conrad and Bean were on Apollo 12 (though I couldn’t recall the third crew member at that moment). I also remembered Lovell and Haise from Apollo 13, Shepard from Apollo 14, and Young from Apollo 16. This led me to consider that alphabetical order might have ‘influenced’ crew assignments. Hey look! Clearly the order was, at least partially, determined alphabetically. Or perhaps not? This observation brings us to the topic of confidence intervals, which are crucial in statistical analysis. While strong correlations can be found between various factors, this doesn’t necessarily imply causation. For instance, there’s a 0.985 correlation between arcade revenue and computer science doctorates awarded in the U.S. (More examples can be found at tylervigen.com/spurious-correlations.) Even ignoring causation, one thing we should consider is the confidence interval of our correlation. Confidence intervals indicate the reliability of a statistical measure; more data typically leads to greater trust in the result. These intervals are often specified at levels like 95% or 99%. Let’s examine a simple example, calculating a 95% confidence interval for a percentage: A TV advertisement claimed that a certain hair conditioner is the nation’s favorite, with small print stating, “73% of 64 people asked agreed.” This means 47 people concurred (0.73 × 64 ≈ 47). A larger sample size would provide more confidence in this claim. However, we can use the confidence interval to assess the statistical significance of 73% from a sample of 64. The formula for a 95% confidence interval of a percentage is: \\[95\\text{\\%} \\text{ C.I.} = \\pm 1.96 \\times \\sqrt{\\frac{p(1-p)}{n}}\\] where $p$ is the percentage (as a decimal) and $n$ is the sample size. Applying our numbers: \\[\\begin{align*} 95\\text{\\% C.I.} \u0026amp;= \\pm 1.96 \\times \\sqrt{\\frac{0.73 \\times 0.27}{64}} \\\\ \u0026amp;= \\pm 10.9 \\end{align*}\\] This results in a 95% confidence interval of approximately $\\pm$11%, indicating that the true percentage likely falls between 62% and 84%. This technique is valuable when evaluating data; always consider confidence intervals to determine the reliability of a result. In our case, the correlation coefficient between alphabetical ordering and Apollo astronauts is 0.45, suggesting a moderate positive correlation. However, it’s essential to examine the confidence interval for this correlation. Calculating confidence intervals for correlations is more complex and often asymmetric. Detailed information on this topic is available at statology.org). For our data, the confidence interval ranges from 0.023 to 0.738, spanning from no correlation to a strong one. Therefore, it’s uncertain whether we’ve uncovered Deke’s method or not. The key takeaway is the importance of considering confidence intervals in statistical analysis. For those interested, here’s the code used to create the plot, which can serve as a starting point for experimenting with plt.xkcd()." }, { "title": "Rooney Scores 50 goals for England – the data", "url": "/articles/2015/Rooney-50/", "content": "Wayne Rooney has broken the England football goal scoring record with 50 goals. A lot of discussion has therefore been about if he is the greatest English attacker of all time or not. Let’s look at the data. The main argument against Rooney being the greatest is his lack of World Cup Finals goals (1; at the 2014 finals). A comment which is also often made is about the number of “friendly” goals scored. Lineker (with a total of 48 goals for England, 3rd in the all time list) was recently voted as England’s greatest attacker in a BBC poll, managed 10 goals in World Cup finals (including the 1986 Golden Boot). So what if we weight goals differently? What if we make friendly goals worth 0.5 a goal, and “finals” goals (i.e. the World Cup Finals and European Championships Finals) as times 2? What does that do to the all time scoring list? (Here I count goals in the old “Home Championship” as ‘1’ (i.e. the same as World Cup and European qualifiers). Although there is an argument to be had that it should be weighted more heavily since it was the international competition for England before the World Cup got “big” (~pre-1950s). However, for now I’ll leave its weighting as ‘1’.) In that adjusted table we get the following: Ranking Name Adjusted Goals 1 Wayne Rooney 49 2 Gary Lineker 46 3 Bobby Charlton 43 4 Michael Owen 39 5 Jimmy Greaves 34.5 5 Alan Shearer 34. 5 You can see that Rooney is still number 1 - despite down weighting the friendly goals. But okay, perhaps you think friendlies shouldn’t count at all and only competitive fixtures should be counted (the ‘Adjusted Goals’ column is if we again weight finals goals more heavily, the first ranking column is for the ‘non-adjusted’ goals): Ranking Name Goals Adjusted Goals Adjusted Ranking 1 Wayne Rooney 36 42 1 2 Stephen Bloomer 28 28 6 3 Bobby Charlton 27 32 3 4 Michael Owen 26 32 3 5 Gary Lineker 24 34 2 6 Jimmy Greaves 23 24 8 Again, Rooney comes top for both the usual and adjusted goal tallies. (One way) to knock Rooney off that top spot (if you particularly want to for some reason) is to look at the ‘goals per cap ratio’. To use this stat we have to set a cap limit because of the people who didn’t play very much. For example, Albert Allen, Francis Bradshaw, John Veitch, John Yates and Rev. Walter Gilliat who were all capped once by England, and each scored a hattrick on their one, and only, appearance for England. This gives them a rather impressive 3 goals per cap ratio. If we only consider players who have at least 40 caps then: Ranking Name Goals per cap 1 Jimmy Greaves 0.77 2 Gary Lineker 0.6 3 Peter Crouch 0.52 4 Geoff Hurst 0.49 5 Alan Shearer 0.48 6 Wayne Rooney 0.47 Finally, if you again weighted the goals (since Crouch scored 12 of his 22 goals in friendlies) you would find the order is Greaves, Lineker, Hurst, Shearer, Rooney (Crouch drops from 3rd to 8th). I am not going to make any conclusions about if Rooney is England’s greatest attacker or not, that is up to you! All the data I’ve used in this post was collected from englandstats.com and is accurate as of today (11th September, 2015). If you want to play around with the data I’ve collated all the stats (player names, caps, minutes played and goals scored (separated into individual competitions)) into an Excel file available here. Update An updated data file, as of 2024-11-20, is available here. This includes an ‘Adjusted Total’ column which, as above, weights friendly goals as 0.5 and “finals” goals (i.e. the World Cup Finals and European Championships Finals) by a factor of 2." }, { "title": "Safe Reboot of Locked Linux Machine", "url": "/articles/2015/safe-reboot/", "content": "The magic SysRq (system request) allows you to access a low-level message system in the kernel. This will work as long as there hasn’t been a kernel panic, and can provide you with a way of rescuing the system. To access the key combination you need to hold Alt + SysRq + Command where command is the specific feature you want to access. The SysRq key is usually the same as the Print Screen key. A table of the available commands are found at the table at the bottom of this article, but the main one you will probably use is: Alt + SysRq + REISUB (slowly type the R E I S U B) This will kill all process except init, sync the mounted file systems, remount the file systems in read-only mode and reboot the system. There are a few ways to try and remember the order REISUB, the one I use is “Reboot Even If System Utterly Broken”, you could also just remember that it is ‘BUSIER’ backwards. Hope it comes in handy, here is the table of all the commands: Command Description 0-9 Set the console log level, i.e., change the type of kernel messages that are output b Immediately reboot the system, without unmounting or syncing filesystems c Perform a system crash. A crashdump will be taken if configured. d Display all currently held locks e Send the SIGTERM signal to all processes except init (PID 1) g Emergency support for switching back to the kernel’s framebuffer console h Output a terse help document to the console i Send the SIGKILL signal to all processes except init j Forcibly “Just thaw it” – filesystems frozen by the FIFREEZE ioctl k Kill all processes on the current virtual console l Shows a stack backtrace for all active CPUs m Output current memory information to the console n Reset the nice level of all high-priority and real-time tasks o Shut off the system p Output the current registers and flags to the console q Display all active high-resolution timers and clock sources r Switch the keyboard from raw mode s Sync all mounted filesystems t Output a list of current tasks and their information to the console u Remount all mounted filesystems in read-only mode v Forcefully restores framebuffer console w Display list of blocked tasks y Show global CPU registers (SPARC-64 specific) z Dump the ftrace buffer w Display list of blocked tasks y Show global CPU registers (SPARC-64 specific) z Dump the ftrace buffer" }, { "title": "Modified Taylor Diagrams", "url": "/articles/2014/Modified_Taylor_Daigrams/", "content": "Taylor diagrams are a nice way of visualizing validation results of a variety of models. The diagrams can be used to graphically summarize how close a set of patterns (in this case a set of models) match observations. This is done by plotting the standard deviation of the time series of the model values, against the correlation (the Pearson product-moment correlation) between the time series of the model values and the observations. Exploiting the geometric relationship between correlation and standard deviation we can also plot the standard deviation of the model error. Whilst Taylor originally used the “centered pattern RMS difference” my approach (as described in this paper) is to use the standard deviation of the model errors, a more commonly undertstood term. Additionally Taylor’s original approach did not allow for the plotting of model biases, I have added this via a colour scale. Python code to make your own modified Taylor diagrams is avaialble here." }, { "title": "Quick way to remove X Server for running a headless server", "url": "/articles/2014/remove-X-server/", "content": "I use my Raspberry Pi as a headless server so do not need anything that relies on X11, this is a quick way to get rid of everything that depends on X11 to save some space. Although you can get Raspberry Pi OSs which are very minimal (i.e. do not start with the X Server) I’ve found that Raspbian is the most stable (Debian-based) OS for the Raspberry Pi, and so that is what I use. It is also really easy to set up to use as a headless server. Flash the OS to your SD card, boot your Raspberry Pi with it and as long as you have a wired internet connection to it, no need to use a screen. Simply SSH into the machine with the username ‘pi’ and password ‘raspberry’ (if you aren’t sure what the IP address of your Pi is you can either look on your router or I find using the Android App Fing the easiest way). The first time you login you may want to run the Raspbian config programme: sudo raspi-config Now the quick way to remove X11: sudo apt-get remove --auto-remove --purge libx11-.* The ‘–purge’ command in apt-get will remove everything which is associated with libx11, a nice quick way to save a load of space. If you so wish you can always check if a particular package is installed or not with: apt-cache policy package Where package is the package you want to check if it is installed or no. If it is installed the version number will be returned if not you will some text including the line ‘Installed: (none)’." }, { "title": "Making Files and Folders Lowercase", "url": "/articles/2013/making-lowercase/", "content": "Very occasionally I want to make all the files and folders of a particular directory lowercase (usually when porting an existing directory structure from Windows to Linux) this is how I do it. You could set up some form of recursive renaming script in a similar fashion to my post on Batch Process in Bash. But it is much nicer to do it in just the one line, assume we want to change all the file and folder names inside the folder ‘/home/sean/windows/’: find /home/sean/windows/ -depth -exec rename 's/(.*)\\/([^\\/]*)/$1\\/\\L$2/' {} \\; Job done!" }, { "title": "New Pound Coin", "url": "/articles/2013/Pound_Coin/", "content": "Anyone who knows me / has been lucky (!) enough to have been taught by me will know how I like to moan about the mathematics of the new pound coin… In 2014 the Royal Mint announced a new pound coin. A new all singing, all dancing, counterfeiter ruining pound coin. (Approximately one in thirty £1 coins in circulation is counterfeit). They also decided to give a nod to the old thrupenny bit and make it 12 sided, it looked pretty good, and it was claimed to be the “most secure coin in the world”. The Mint were pretty smug: But as soon as I saw this I was worried. I was concerned for people and machines up and down the UK. Why was I? Because 12 is even and coins don’t usually have an even number of sides In fact, as far as I am aware, the only coin in active circulation with an even number of sides, is the Australian 50 cent piece. And why is this? It is all because of “constant diameter”. Fortunately, as an undergrad, I was taught by Prof. Chris Sangwin who wrote ‘How Round is Your Circle’ along with John Bryant [by the way, you should definitely read this book, it’s great!] so this evenness stood out like a sore thumb. You see, whilst a circle has constant diameter (of course it does, it has a constant radius, and diameter = 2 x radius), other shapes exist which don’t have constant radius but do have constant diameter. Indeed there are an infinite number of such shapes. Any odd-sided regular polygon can be used to construct one of these shapes of constant diameter. Now this is pretty useful. So useful in fact that we decided to make our 50p coin (1969-) and 20p coin (1982-) a 7-sided shape of constant diameter. This is great because no matter what way round you put your coin I know the diameter is always the same. Vending machines work! So, when I saw this coin I knew we were in for a world of pain. But I did my civic duty. I reported my worries about the coin and supported the consultation process. That resulted in the updated design: In particular, let me highlight Section 2.3 on page 7 of the report: Some concern was expressed in relation to the even-sided shape, namely that its non-constant diameter could impact the rolling behaviour of the coin (and thereby reduce its security through the automatic vending process) The proposed solution is (Section: 3.6; page 9): The government acknowledges the concerns over the performance of a coin with a non-constant diameter through the automatic vending process. Testing conducted by The Royal Mint, in partnership with a number of respondents, confirmed that these reservations were well-grounded for a 12-sided coin with flat sides and sharp corners. However, the introduction of rounded edges (‘radial chords’) to the design led to a significant improvement in its rolling behaviour, to a level consistent with that required by existing equipment." }, { "title": "Variance in TV Viewing Figures", "url": "/articles/2013/variance-in-tv/", "content": "It is a fairly safe assumption that viewing figures for TV shows typically drop over time. But what I was thinking about this morning is how many people just turn on the TV and watch whatever is on, is that something we can work out from TV videwing figures? To start with lets consider “Grey’s Anatomy”. The viewing figure data is from “ABC Medianet” and initially we just plot the number of viewers per episode which results in the below graph. The first thing to notice in the above plot, is the large spike of around 38 million viewers for episode 24. This is caused by being the lead out show after Super Bowl XL. The brown circles plotted represent the start of a new season, as very often a new season (with the associated advertising) leads to a bump in viewing figures. We can also see from the plot that over time the number of viewers for the show has steadily decreased. And the red and blue curves are polynomial fits to the data to try and track that change in viewers. The red curve ignores the viewing figure from after the Super Bowl, treating it as an outlier. To look at the variation in users we can subtract the overall trend of viewers from the data and just look at the “noise”. However, this “noise” also contains the trend of an increase in viewers at the start of a new season and this diminishing over the season. To try and remove this we look at the Hilbert-Huang transform of the data minus the trend, breaking the curve into 7 intrinsic mode functions (IMF): What we discover in the 4th IMF is the following trend, made larger below so it is more visible, with the red vertical lines indicating where a new season begins. You can see that the peaks do not always match where a season begins (which was also reflected in the very first plot), but very often they do. This means that we can now plot the true “fluctuations” in viewer numbers by subtracting the above graph from our “noise” graph. It looks like, in this plot, that the noise in viewing figures is decreasing over time. However since we know that there is a decreasing number of viewers we calculate the normalised standard deviation for each season. This is done by finding the standard deviation of a season and dividing it by the range of viewers for that season. This is presented in the table below. | Season Number | Standard Deviation (Millions) | Range of Viewers (Million) | (Standard Deviation / Range ) \\* 100 (%) | | :-----------: | :---------------------------: | :------------------------: | :--------------------------------------: | | 1 | 1.19 | 5.97 | 19.99 | | 2 | 1.71 | 9.72 | 17.60 | | 3 | 1.82 | 8.88 | 20.45 | | 4 | 1.39 | 6.82 | 20.31 | | 5 | 1.43 | 5.36 | 26.76 | | 6 | 1.34 | 7.16 | 18.70 | | 7 | 1.09 | 5.19 | 21.10 | | 8 | 0.66 | 3.93 | 16.88 | | 9 | 0.59 | 3.56 | 16.67 | The percentage column has a mean value of 19.83% itself with a standard deviation of 3.06%. So we can conslude that the standard deviation of the noise in viewers is about 20% of the range of viewers. Is this a “standard value”? Or is it somehow tied to Grey’s Anatomy? Below I have generated the same plots as above for the shows ‘How I Met Your Mother’ and ‘Desperate Housewives’ which can be seen in the following galleries: How I Met Your Mother: Desperate Housewives: Desperate Housewives shows a similar pattern to Grey’s Anatomy in terms of the overall viewing figure trend. Increasing initially and then dropping off. ‘How I Met Your Mother’, however, shows a different pattern. Although there are less viewers, the numbers remain fairly constant throughout the show. From the above analysis we get a standard deviation (as a percentage of the range of viewers) as 17.17% (with a standard deviation of ~2.82%) for Desperate Housewives and 21.90% (standard deviation of 2.95%) for How I Met Your Mother. So the three values we’ve got are 19.83%, 17.17% and 21.90%. Giving an average of 19.63%, well three examples are enough for me! It certainly hints at the fact that the spread of noise in viewing figures is independent of the show, and may be a fact of TV viewing figures." }, { "title": "Velocity Required for Loop-the-Loop", "url": "/articles/2013/loop-the-loop/", "content": "In this post we are going to work out (approximately) the minimum speed required to complete a loop-the-loop, we’ll do this via an energy argument. However, for ease, we are going to ignore friction! First we need to find the minimum speed required at the top of the loop. To get the minimum required speed to make the loop-the-loop, at the top of the loop we require the normal force (\\(N\\)) to be \\(0\\). Equating the forces at the top of the loop we have the weight of the car (\\(mg\\)) plus the normal force (\\(N\\)) equal to the centrifugal force (\\(F\\)), which is given by \\(F=\\frac{mv^2}{r}\\) where \\(r\\) is the radius of the circle. Thus, \\[\\begin{eqnarray*} \\frac{mv_t^2}{r} \u0026amp;=\u0026amp; N+mg, \\\\ \\frac{v_t^2}{r} \u0026amp;=\u0026amp; g,\\\\ v_t^2 \u0026amp;=\u0026amp; gr,\\\\ v_t \u0026amp;=\u0026amp; \\sqrt{gr}. \\end{eqnarray*}\\] Giving us a minimum velocity at the top of the loop, \\(v_t\\). We now proceed with the energy argument. The total energy at the top of the loop is equal to the potential energy at the top plus the kinetic energy at the top, respectively these are: \\[\\begin{eqnarray*} PE_t \u0026amp;=\u0026amp; mgh = 2mgr,\\\\ KE_t \u0026amp;=\u0026amp; \\frac{1}{2}mv_t^2 = \\frac{1}{2}mgr. \\end{eqnarray*}\\] Thus the total energy at the top is: \\[\\begin{eqnarray*} E_t \u0026amp;=\u0026amp; 2mgr+\\frac{1}{2}mgr,\\\\ \u0026amp;=\u0026amp; \\frac{5}{2}mgr. \\end{eqnarray*}\\] We now find the total energy at the bottom, since there is no potential energy, this is simply the kinetic energy, \\[E_b = \\frac{1}{2}mv_b^2.\\] Where \\(v_b\\) is the speed we are looking for. With our assumptions, the energy at the top (\\(E_t\\)) equals the energy at the bottom (\\(E_b\\)) giving \\[\\begin{eqnarray*} E_b \u0026amp;=\u0026amp; E_t,\\\\ \\frac{1}{2}mv_b^2 \u0026amp;=\u0026amp; \\frac{5}{2}mgr,\\\\ mv_b^2 \u0026amp;=\u0026amp; 5mgr,\\\\ v_b^2 \u0026amp;=\u0026amp; 5gr,\\\\ v_b \u0026amp;=\u0026amp; \\sqrt{5gr}. \\end{eqnarray*}\\] Thus we have found the speed required to complete a loop the loop of radius \\(r\\). For example, if the loop had a 4 metre diameter (2 metre radius) then the velocity required to complete the loop would be \\(v = \\sqrt{5\\times g\\times 2}=\\sqrt{10g}\\approx 9.9\\). For convience we can approximate \\(\\sqrt{5gr}\\) as: \\[\\sqrt{5\\times 9.81\\times r}=\\sqrt{49.05r}\\approx 7\\sqrt{r}.\\]" }, { "title": "It's Snowing Outside", "url": "/articles/2013/snowing-outside/", "content": "I’m sitting in my office, at home, watching the snow falling outside and it got me thinking…. As is well known, chemically, water is made from hydrogen and oxygen (H2O), specifically, each water molecule is made from two hydrogen atoms bonded with an oxygen atom. In this bonding there are two left over pairs of electrons, and the whole molecule creates a rough tetrahedron. “Luckily” (from the point of view of this post) when dealing with hexagonal ice (ice Ih) (i.e. natural ice /snow) we get a perfect tetrahedron (see here and here for more details of this), hence the six-fold symmetry in snowflakes. Now we know this, we can work out just how unique the snow outside is, or the ice in your cup (perhaps you’re reading this in a warmer climate!) is. How many possible ways are there to create a tetrahedral structure like hexagonal ice? With a little bit of thinking, and twisting the above image in your head, hopefully you’ll agree with me that there are 6 ways of arranging the water molecule in the ‘ice tetrahedron’. However, not every one of these possibilities is possible. Read the Wikipedia page on hydrogen bonds and then you’ll see that in fact only \\(\\frac{3}{2}\\)  possible orientations are allowed. That is, for a crystal of N molecules, there are \\(\\left(\\frac{3}{2}\\right)^N\\) ways to arrange the molecules inside that crystal. So, let us consider a snowball, say of 3 cm radius. How many molecules are there in that much snow? There are a number of ways of arriving at the number of molecules, but I’m going to go for the way which came to me first (rather than perhaps the best / most elegant solution (not that I know what that is!)). I considered the snow as water (because I know the molar mass of water!), so for that we just need to know the density of snow. Not so easy, it depends on various factors; as you have no doubt experienced, new snow feels a lot less dense then snow that has been sitting around for a week. Typically the densities vary between 8% and 50% the density of water. Once the snow is on the ground it settles under its own weight to a density of around 30%. Since the snow that got me thinking about this is fairly new, we’ll go for a density of 20%. That means 30 cm of snow would make 6 cm of water. So our 3 cm radius snow ball has a volume of \\(\\approx 113\\,cm^3\\) which would be \\(22.6\\, cm^3\\) of water. A cubic centimetre of water weights 1 grams so the water content of our snowball would weigh 22.6 g. The molar mass of water is \\(18.01528\\, g\\cdot mol^{-1}\\) so there is \\[22.6 \\times\\frac{1}{18.01528}\\approx 1.25 \\mbox{ moles}.\\] Multiplying this number by Avogadro’s number (\\(6.02\\times 10^{23}\\)) yields the number of molecules in the snowball: \\[1.25 \\times 6.02\\times 10^{23} \\approx 7.5\\times 10^{23}.\\] That means the possible number of orientations of the water molecules is \\[\\left(\\frac{3}{2}\\right)^{7.5\\times 10^{23}} \\approx 10^{10^{23}}.\\] So how big is that number? Well it is stupidly big. We estimate that there is something like \\(10^{80}\\) atoms in the universe which isn’t even close to that number. So it is safe to say that every snowball that has ever been made, or ever will be made by people all over the Earth for the rest of the history of this planet, will always have a unique arrangements of water molecules!" }, { "title": "The Crawshaw Method (for Quadratics)", "url": "/articles/2012/the-Crawshaw-method/", "content": "Something all High School students (and above) need to know is how to factorize quadratic equations. However I am amazed how few people know a decent (and easy) way of doing this! I’ll keep this as simple as possible, and just show you how to use the method. First we consider a simple case (and sadly this is where a lot of the problems arise from!), we want to factorize the quadratic equation, \\[x^2+5x+6.\\] Now, as far as I know, it is universally taught that to factorize this you have to “find” two numbers that times together to make \\(+6\\) (the constant term) and add to make \\(+5\\) (the linear term). This is pretty easy, we choose the numbers \\(+2\\) and \\(+3\\), and then we simply write down the answer, \\[x^2+5x+6=(x+2)(x+3).\\] Easy. The problem is when you arrive at a question like, \\[3x^2-7x-6.\\] If we proceed in the same way as we were `taught’ above we need to find two numbers which times together to make \\(-6\\) and add to make \\(-7\\). We then need to try and work out what on Earth to do with these numbers, the problem is, it is not in any way obvious and quite a lot of guess work, or at least trial and error, is required.  The answer we would be looking for was \\[3x^2-7x-6=(3x+2)(x-3).\\] I think what is clear is that we need a concise, dare I say easy, method to solve any quadratic equation. Let me introduce the Crawshaw method. We first will apply the method to the ‘easy’ quadratic formula (the method is identical for all quadratics), you only need to remember that \\(x=1x\\), only we don’t write the \\(1\\). So the quadratic to solve is \\(x^2+5x+6\\). The first step of the method tells us that we need to find two numbers which add together to make \\(+5\\) and times to make \\(1\\times 6=6\\), that is, the coefficient of the quadratic term (\\(1\\)) multiplied by the constant term (\\(6\\)). These numbers are \\(2\\) and \\(3\\). The second step is to rewrite the linear term as a combination of the numbers we have just found. So for this question the linear term is \\(+5x\\) the numbers we found were \\(2\\) and \\(3\\) so we will replace \\(+5x\\) in the quadratic with \\(+2x+3x\\). So our quadratic is now: \\[x^2+2x+3x+6.\\] Which is of course the exact same equation but now we can continue with the method. We now draw an imaginary line down the middle of the equation: \\[x^2+2x\\quad/\\quad+3x+6.\\] Next we factorize the left hand side, getting: \\[x(x+2)\\quad/\\quad+3x+6.\\] We then need to factorize the right hand side, but making sure that the bracket terms are the same, i.e. \\[x(x+2)\\quad/\\quad+3(x+2).\\] Then finally we get our two brackets (for the factorized answer) as the one bracket we already have, and the two terms that are on the outside of the brackets (i.e. in this case \\(x\\) and \\(+3\\)). Thus our answer becomes \\[(x+2)(x+3).\\] This method does seem more difficult for this ‘easy’ equation (i.e. one where the coefficient of the quadratic term is \\(1\\)) but the beauty of the Crawshaw method is that it works exactly the same for all quadratic equations. Now let us consider the second equation, that is \\(3x^2-7x-6\\). We approach it in exactly the same way. First find two numbers which times together to make \\(3\\times -6 = -18\\) (that is the coefficient of the quadratic term multiplied by the constant term) and add together to make \\(-7\\). Such numbers are \\(-9\\) and \\(+2\\). So we now rewrite the linear term with these numbers (i.e. replacing \\(-7x=-9x+2x\\)) so we get: \\[3x^2-9x+2x-6.\\] Now we draw an imaginary line down the middle and factorize both sides of the equation: \\[\\begin{eqnarray*} 3x^2-9x \\quad \u0026amp;/\u0026amp; \\quad 2x-6,\\\\ 3x(x-3) \\quad \u0026amp;/\u0026amp; \\quad +2(x-3). \\end{eqnarray*}\\] So we then have our factorized solution, \\[3x^2-7x-6=(3x+2)(x-3).\\] See, wasn’t that easier? I present to you, the Crawshaw method." }, { "title": "A Short History of Nearly Everything - Some Corrections", "url": "/articles/2012/a-short-history/", "content": "I have just finished the book “A Short History of Nearly Everything” by Bill Bryson and I thought it was absolutely fantastic. There were a couple of mistakes that I noticed (and I am sure others that I did not - by ‘noticed’ I mean ‘didn’t believe the numbers and so checked them out’) that I thought I should correct. This post just provides a few scientific corrections, and is meant in no way as disrespect to what I believe to be a truly brilliant book. On a diagram of the solar system to scale, with Earth reduced to about the diameter of a pea, Jupiter would be over a thousand feet away and Pluto would be a mile and a half distant (and about the size of a bacterium, so you wouldn’t be able to see it anyway). Okay, assume that the radius of a pea is about \\(4\\) millimetres, or \\(4 \\times 10^{-3}\\) metres. The mean radius of the Earth is \\(6.371 \\times 10^3\\) metres, or the peas radius is \\(\\frac{1}{1592750000}\\) of the Earth’s. We will use this fraction as the scale for the rest of the system. The distance between the Earth and Jupiter is \\(5.808\\) AU so on our scale that turns out to be \\(1790\\) feet. Pluto (which is \\(32.15\\) AU away) would be \\(1.88\\) miles away. So in this sense Bryson underestimated their distances (slightly). However, as for the size of Pluto, it has a mean radius of \\(1.173 \\times 10^6\\) metres, so on our scale that would become \\(7.4 \\times 10^{-4}\\) metres, i.e. a total diameter of 1.5 millimetres, very visible and about 75 times larger than a median bacterium! Protons are so small that a little dib of ink like the dot on this i can hold something in the region of 500,000,000,000 of them, rather more than the number of seconds contained in half a million years. I think the approximate size of a ‘dot’ is about \\(0.5\\) millimetres, so that means the area, in metres, of such a dot is: \\(\\pi\\cdot 0.00025^2=1.963\\times 10^{-7}\\). The size of a proton is \\(8.77 \\times 10^{-16}\\) metres, and so the number of protons that would fit in such a dot is \\(223,887,732\\). Or \\(2233\\) times less than the Bryson stated number. As for the number of seconds in half a million years: \\(1 \\mbox{ year } = 31556926 \\mbox{ seconds}\\) and \\(500,000 \\mbox{ years } = 15,800,000,000,000 \\mbox{ seconds}\\). Quite a lot more than what was stated, in fact, \\(500,000,000,000\\) seconds is only 15844 years. Even so, it [the Voyager spacecraft] took them nine years to reach Uranus and a dozen to cross the orbit of Pluto. Actually, Voyager 1 never got to Uranus nor cross the orbit of Pluto, only Voyager 2 did. [Betelgeuse is] fifty thousand light years away. Betelgeuse is around \\(643 \\pm 143\\) light years away. It is thought that our entire planet may contain, at any given moment, fewer than twenty francium atoms. There is almost certainly a lot more than twenty francium atoms, according to Adloff and Kauffman, there is 550 grams in the Earth’s crust. It took thousands of workers to clear 1.8 billion tonnes of debris from the 6.5 hectares of the World Trade Center site. 1.8 million tonnes. It rises exponentially, so that a 7.3 quake is fifty ties more powerful than a 6.3 earthquake and 2,500 time more powerful than a 5.3 earthquake. Exactly what the mistake is here depends slightly on what you interpret the word powerful means. The size of an earthquake increases by a factor of 10 as magnitude increases by one whole number. So a magnitude 7.3 earthquake is 10 times larger than a 6.3, and 100 times larger than 5.3. The amount of energy released increases by a factor of 31.6 \\(\\left(10^{1.5}\\right)\\). So a 7.3 earthquake releases roughly 32 times more energy than a magnitude 6.3; and about 1,000 times more energy than a 5.3. It has been calculated that if you sunk a well to the center [of the Earth] and dropped a brick into it, it would take only 45 minutes for it to hit the bottom. The terminal velocity of an object falling through the air is the maximum speed that the object will reach when the resistance of the medium through which it falls (in this case, air) prevents further acceleration. This velocity depends on various factors, including the mass and aerodynamic properties of the object (such as its shape and surface area), as well as the density of the air. For a brick, calculating its terminal velocity involves considering its drag coefficient, cross-sectional area perpendicular to the direction of motion, and mass. Using sensible estimates for this value we could calculate the terminal velocity of a house brick to be approximately 45 meters per second. Given that the Earth’s radius is approximately 6371 km, the brick would take about 39 hours to fall to the centre (given all the other difficulties with this thought experiment!)." }, { "title": "1 = 2", "url": "/articles/2012/1-2/", "content": "Many people have seen “proofs” that \\(1 = 2\\). However there are always a rather obvious fallacy - a division by 0. A classic such example is: \\[\\begin{eqnarray*} a \u0026amp;=\u0026amp; b, \\\\ a^2 \u0026amp;=\u0026amp; ab, \\\\ a^2 - b^2 \u0026amp;=\u0026amp; ab - b^2, \\\\ (a+b)(a-b) \u0026amp;=\u0026amp; b(a-b), \\\\ (a+b) \u0026amp;=\u0026amp; b, \\\\ a+a \u0026amp;=\u0026amp; a, \\\\ 2a \u0026amp;=\u0026amp; a, \\\\ 2 \u0026amp;=\u0026amp; 1. \\end{eqnarray*}\\] This is a decent “trick” to show people, but as I said above, the mistake simply comes from the division by 0 on the 5th line. However, here is a different “proof”, can you see the mistake? Consider the equation, \\(2=x^{x^{x^{\\ldots}}}\\), with an infinite number of \\(x\\)’s. If we add some brackets to the equation we get, \\(2=x^{\\left(x^{x^{\\ldots}}\\right)}\\) and then we can substitute the first equation in to get: \\[2=x^2.\\] So \\(x=\\sqrt{2}\\), and hence \\[2=\\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{\\ldots}}}.\\] Then if we repeat the exact same above process with the equation \\(4=x^{x^{x^{\\ldots}}}\\) we see that \\(4=x^4\\) and so again \\(x=\\sqrt{2}\\). So this time, \\[4=\\sqrt{2}^{\\sqrt{2}^{\\sqrt{2}^{\\ldots}}}.\\] And so \\(2=4\\) and thus \\(1=2\\)! Where is the mistake?" }, { "title": "Sine Rule on a Sphere", "url": "/articles/2012/sine-rule-on-a-sphere/", "content": "A couple of days ago I wrote about how to find the cosine rule on a sphere. In this post I’ll show you the sine rule on a sphere. Consider the following triangle on a sphere: The cosine rule (on a sphere) is: \\[\\cos(c)=\\cos(a)\\cos(b)+\\sin(a)\\sin(b)\\cos(C).\\] The sine rule is then, where \\(A\\) is the angle at \\(\\textbf{w}\\) (opposite side \\(a\\)) and \\(B\\) is the angle at \\(\\textbf{v}\\) (opposite side \\(b\\)): \\[\\frac{\\sin(A)}{\\sin(a)}=\\frac{\\sin(B)}{\\sin(b)}=\\frac{\\sin(C)}{\\sin(c)}.\\] The proof is a simply a rearranging exercise from the cosine rule, with the formula \\(\\cos^2(C)+\\sin^2(C)=1\\). You can find the proof below, but I do recommend having a go yourself. Rearrange \\(\\cos(c)=\\cos(a)\\cos(b)+\\sin(a)\\sin(b)\\cos(C),\\) to get: \\[\\cos(C)=\\frac{\\cos(c)-\\cos(a)\\cos(b)}{\\sin(a)\\sin(b)}.\\] Now we use \\(\\sin^2(C)=1-\\cos^2(C)\\) substituting the above in for \\(\\cos^2(C)\\): \\[\\begin{eqnarray*} \\sin^2(C) \u0026amp;=\u0026amp; 1-\\left(\\frac{\\cos(c)-\\cos(a)\\cos(b)}{\\sin(a)\\sin(b)}\\right)^2,\\\\ \\sin^2(C) \u0026amp;=\u0026amp; \\frac{\\sin^2(a)\\sin^2(b)-(\\cos(c)-\\cos(a)\\cos(b))^2}{\\sin^2(a)\\sin^2(b)}. \\end{eqnarray*}\\] Now again use \\(\\sin^2(C)=1-\\cos^2(C)\\) but for \\(a\\) and \\(b\\) to get: \\[\\sin^2(C)=\\frac{(1-\\cos^2(a))(1-\\cos^2(b))-(\\cos(c)-\\cos(a)\\cos(b))^2}{\\sin^2(a)\\sin^2(b)}.\\] Times everything out and cancel to get: \\[\\sin^2(C)=\\frac{1-\\cos^2(a)-\\cos^2(b)-\\cos^2(c)+2\\cos(a)\\cos(b)\\cos(c)}{\\sin^2(a)\\sin^2(b)}.\\] Finally divide everything by \\(\\sin^2(c)\\) to get the required form: \\[\\frac{\\sin^2(C)}{\\sin^2(c)}=\\frac{1-\\cos^2(a)-\\cos^2(b)-\\cos^2(c)+2\\cos(a)\\cos(b)\\cos(c)}{\\sin^2(a)\\sin^2(b)\\sin^2(c)}.\\] Which is what we wanted, since the right hand side is symmetrical in \\(a\\), \\(b\\) and \\(c\\), i.e. \\[\\frac{\\sin^2(A)}{\\sin^2(a)} = \\frac{\\sin^2(B)}{\\sin^2(b)} = \\frac{\\sin^2(C)}{\\sin^2(c)}.\\] But since our angle must be between \\(0\\) and \\(\\pi\\), all the terms are positive, so we can take the square root to get the sine rule on a sphere: \\[\\frac{\\sin(A)}{\\sin(a)}=\\frac{\\sin(B)}{\\sin(b)}=\\frac{\\sin(C)}{\\sin(c)}.\\]" }, { "title": "Cosine Rule on a Sphere", "url": "/articles/2012/cosine-rule-on-a-sphere/", "content": "As all (well, at least those that can remember) 15+ year olds know, to find the length of a side of a non-right angled triangle you can’t use Pythagoras’ theorem and instead require the so called ‘Cosine Rule’. Simply, the cosine rule is used to find (for example) the length of side \\(c\\) in: You use the following formula: \\[c^2 = a^2 + b^2 - 2ab\\cos(\\gamma).\\] But what happens if we move away from Euclidean Geometry and that our triangle is now sitting on a sphere? I.e. Logic tells us that of course the same cosine rule can not be applied to find the side \\(c\\). Instead we use an alternative formula, namely: \\[\\cos(c)=\\cos(a)\\cos(b)+\\sin(a)\\sin(b)\\cos(C).\\] The proof is relatively straight forward, try having a go before looking below: Let \\(\\textbf{u}\\), \\(\\textbf{v}\\) and \\(\\textbf{w}\\) be unit vectors from the middle of the sphere to the points \\(u\\), \\(v\\) and \\(w\\) of our triangle. The lengths of the sides are then give by the dot product: \\[\\begin{eqnarray*} \\cos(a) \u0026amp;=\u0026amp; \\textbf{u}\\cdot\\textbf{v},\\\\ \\cos(b) \u0026amp;=\u0026amp; \\textbf{u}\\cdot\\textbf{w},\\\\ \\cos(c) \u0026amp;=\u0026amp; \\textbf{v}\\cdot\\textbf{w}. \\end{eqnarray*}\\] The angle \\(C\\) is the inverse cosine of two tangents, say \\(\\textbf{t}_a\\) and \\(\\textbf{t}_b\\), which are along the sides \\(a\\) and \\(b\\), i.e. \\[\\begin{eqnarray*} C \u0026amp;=\u0026amp; \\cos^{-1}(\\textbf{t}_a\\cdot\\textbf{t}_b),\\\\ \\cos(C) \u0026amp;=\u0026amp; \\textbf{t}_a\\cdot\\textbf{t}_b. \\end{eqnarray*}\\] But we know more information about these tangent vectors. For instance we know that the they are perpendicular to \\(\\textbf{u}\\) which is given by the component of \\(\\textbf{v}\\) perpendiculat to \\(\\textbf{u}\\). This gives (once normalized): \\[\\textbf{t}_a=\\frac{\\textbf{v}-\\textbf{u}(\\textbf{u}\\cdot\\textbf{v})}{|\\textbf{v}-\\textbf{u}(\\textbf{u}\\cdot\\textbf{v})|} = \\frac{\\textbf{v}-\\textbf{u}\\cos(a)}{\\sin(a)}.\\] Similarly, \\[\\textbf{t}_b=\\frac{\\textbf{w}-\\textbf{u}(\\textbf{u}\\cdot\\textbf{w})}{|\\textbf{w}-\\textbf{u}(\\textbf{u}\\cdot\\textbf{w})|} = \\frac{\\textbf{w}-\\textbf{u}\\cos(b)}{\\sin(b)}.\\] So substituting these values into the above equation we get: \\[\\begin{eqnarray*} \\cos(C) \u0026amp;=\u0026amp; \\frac{\\textbf{v}-\\textbf{u}\\cos(a)}{\\sin(a)}\\cdot \\frac{\\textbf{w}-\\textbf{u}\\cos(b)}{\\sin(b)},\\\\ \\cos(C) \u0026amp;=\u0026amp; \\frac{\\textbf{v}\\cdot\\textbf{w}-(\\textbf{v}\\cdot\\textbf{u})\\cos(b)-(\\textbf{u}\\cdot\\textbf{w})\\cos(a)+(\\textbf{u}\\cdot\\textbf{u})\\cos(a)\\cos(b)}{\\sin(a)\\sin(b)},\\\\ \\cos(C) \u0026amp;=\u0026amp; \\frac{\\cos(c)-\\cos(a)\\cos(b)}{\\sin(a)\\sin(b)}. \\end{eqnarray*}\\] Which when rearranged gives the required cosine rule: \\[\\cos(c)=\\cos(a)\\cos(b)+\\sin(a)\\sin(b)\\cos(C).\\] Follow this link if you want to see the sine rule on a sphere." }, { "title": "Batch Process in Bash", "url": "/articles/2011/batch-process-in-bash/", "content": "I am writing this at 0230 because Joe just asked me how to do it, hence someone else in the world might also want to do it. The task is to do something (the same something) to a large group of files in linux. For example convert a folder of wma files to mp3’s (for this example you will need to have ffmpeg installed). We add the condition that we want to keep the file names as they are, and just convert them. It is fairly simple, we will write a script to do this (although you could do it straight from the terminal), for this example we will write a script called wma2mp3: gedit wma2mp3 (Obviously change gedit for your favourite editor). Now add to that: for i in *.wma; do filename=${i%.*} ffmpeg -i \"$filename.wma\" \"$filename.mp3\" done Then all you need to do is place that file in the folder where you want to run it, then run it, and then you’re done! Again this is just an example and you can modify this to anything you want. If you actually want to change wma’s to mp3’s there are a few more options you may want to use, but I thought this would be a better ‘template’. For one more example, say you want to convert all your ps files in a folder to pdf’s (here you will need to have ps2pdf installed) we would use: for i in *.ps; do filename=${i%.*} ps2pdf \"$filename.ps\" \"$filename.pdf\" done Hope that helps!" }, { "title": "God's Number is 20", "url": "/articles/2011/gods-number-is-20/", "content": "The Rubik Cube was invented by one Erno Rubik in 1974 and has gone one to be one of the most popular puzzles in the world, having sold over 350 million units worldwide, even though many people can not solve it! Although perhaps that isn’t overly surprising given the vast number of possible positions that the cube can take, some 43,252,003,274,489,856,000 (that is approximately 43 quintillion) with only 1 correct solution, i.e. if you want to solve it, randomly turning the cube almost certainly won’t get you there. To put that number in perspective, imagine 43 quintillion standard size (5.7cm) Rubik’s cubes stacked on top of each other. This would cover a distance of about 260 light years, or the distance from Earth to our next nearest star (i.e. Proxima Centauri, not the Sun) and back 30 times! But where does this number come from? A standard 3x3x3 Rubik’s cube has 8 corner pieces, 12 edge pieces and 6 centres. Since the centre pieces are fixed, there is nothing we can do to move them, so they do not enter our calculation to the number of possible positions. As there are 8 corner pieces we have \\(8!\\) (that is 8 factorial or \\(8!=8 \\times 7 \\times 6 \\times 5 \\times 4 \\times 3 \\times 2 \\times 1=40320\\)) ways of positioning them, and we are free to place the first 7 where ever we want, however once these 7 are positioned, the 8th is clearly forced to be in a particular place, so we have \\(3^7\\) (\\(3^7=2187\\)) possibile positions for these \\(8!\\) options. The 3 arises since each corner piece has 3 colours on it. So far we have counted \\[8! \\times 3^7 = 88179840.\\] Now, for the 12 edges, you may imagine that we have \\(12!\\) options, however we have to note that an odd permutation of corners results in an odd permutation of the edges, thus there are only \\(\\frac{12!}{2}\\) options. Then in the same ways as the corners we count how many possible positions we can have, this time each piece only has 2 colours on it, but in a similar way once we have chosen where the first 11 go, the 12th piece is fixed, thus giving \\(2^{11}\\) options. So in total we have \\[8! \\times 3^7 \\times \\frac{12!}{2} \\times 2^{11} = 43,252,003,274,489,856,000\\] possible positions, which is what we wanted to show. Possibly one of the reason for the Rubik’s cube continued success is the fact that when you have solved it once, you want to mix it up and start again but this time do it quicker! This process just repeats and since 1981 there have been World Championships for ‘speedsolving’, plus a variety of other competitions. The current fastest recorded time of solving a Rubik’s cube is 6.65 seconds, solved by Feliks Zemdegs, a truly unbelievable feat. (Update: as of July 2023 the record is 3.13 seconds, achieved by Max Park). But should we care about Rubik’s cube? Are they just for fun? Well from a mathematical point of view, they are very interesting: For starters, Rubik’s cube are really very useful for examples and considering finite group theory, in fact the Group Theory Wikipedia page has a picture of a Rubik cube at the very start. Tom Davis has written a very good document on Group Theory via Rubik’s Cubes which if you are interested in such things I would recommend you check it out. However the point of this blog post is actually to do with the question: ‘what is the maximum number of moves it takes to solve a Rubik’s cube’? The real quest is to find an algorithm which will solve any Rubik cube in the minimum number of moves, this is called ‘God’s Algorithm’. The number of moves that this algorithm takes, in the worst case, is called ‘God’s Number’. The first work on this was done in 1980s and in 1981 Thistlethwaite proved that God’s number must be greater than or equal to 18 and less than or equal to 52. Over time these bounds were tightened, and it was in 1995 that the lower bound was first proved to be 20, this was done by Michael Reid (he had previously shown that the upper bound is less than or equal to 29). In the late 2000s Tomas Rokicki did some great work on tightening the upper bounds, culminating in this paper showing the upper bound to be less than or equal to 22. But, given the title of this blog, I hope you have worked out that the upper bound has since be refined to 20, that is both the lower bound and upper bound are 20, thus God’s Number is 20! And who is to thank for this? Well it is thanks to Morley Davidson, John Dethridge, Herbert Kociemba and Tomas Rokicki, not to mention Google, who donated idle computer time. They completed the solution to the problem, by first writing a computer program that could solve the position of any Rubik’s cube in about 20 seconds (not necessarily an optimal solution, but one that took less than 20 moves), and reduced the number of cases they actually needed to solve, via a variety of methods, including considering symmetry, this reduced the number of possible considerations to 55,882,296. To check this number of possibilities would have taken approximately 35 years with a high end computer, and this is where Google came in. The job was spread across a number of computers at Google which then completed the task in a few weeks. Proving that in fact every possible Rubik’s cube position can be solved in 20 moves or less! So ‘God’s Number’ is 20, quite a remarkable fact." }, { "title": "Recurring Decimals - Something on the Repetend", "url": "/articles/2010/recurring-decimals/", "content": "Here is an interesting fact I “discovered” today (I know many have before, but it was a personal discovery) a nice little fact about recurring decimals. By a recurring decimal I mean a decimal which eventually becomes periodic, i.e. \\(0.333\\cdots\\). However from here on I will use the overbar notation, so the repetend (the bit which is repeated) is overlined giving us: \\[0.333\\cdots = 0.\\overline{3}.\\] Now any fraction (in its lowest terms) which has a prime denominator (which isn’t 2 or 5) will result in a recurring decimal, i.e. \\(\\frac{2}{13}=0.\\overline{153846}\\), and it is these fractions (with prime denominator) that I am going to start with. The interesting thing is, it is very easy to work out how many digits long the repetend is for such fractions (we’ll briefly talk about other fractions in just two ticks). The number of digits for the fraction \\(\\frac{n}{p}\\) (where \\(n\\) and \\(p\\) are coprime as to meet the condition that the fraction is in its lowest form) is always less than or equal to \\(p-1\\). For example, \\(\\frac{1}{7} = 0.\\overline{142857}\\) and sure enough that repetend has \\(7-1=6\\) digits and \\(\\frac{2}{11}=0.\\overline{18}\\) and that repetend has \\(2\\le 10=11-1\\) digits. A striking further observation (we’ll talk about how you prove these shortly) tells us that the number of digits in the repetend is a factor of \\(p-1\\). So in our example of \\(\\frac{2}{11}\\) we got a length of 2, which is a factor of \\(11-1=10\\). In fact we can make this a bit more firm, rather than saying it is some number less than \\(p-1\\). The number of digits is equal to the order of \\(10\\mbox{ modulo } p\\). Order in this context refers to the smallest such integer \\(m\\) such that \\(10^m \\equiv 1\\mbox{ modulo } p\\). Again, take our examples. For \\(\\frac{1}{7}\\) we have that \\(10^6 \\equiv 1 \\mbox{ mod } 7\\) and thus the number of digits in the repetend is 6. Whereas for \\(\\frac{2}{11}\\), we get that \\(10^2 \\equiv 1 \\mbox{ mod } 11\\) and so the number of digits in that repetend must be 2, which it is. The proof is fairly simple, have a go yourself. The hint (if you need it) is to use Fermat’s Little Theorem, i.e. \\(10^{p-1}\\equiv 1\\mbox{ mod } p\\). In passing, I’ll also mention that if you have a fraction of them form \\(\\frac{1}{k}\\), where \\(k\\) is any integer (not necessarily prime as in the case above) then the number of digits in the repetend (if there are any!) is less than or equal to \\(k-1\\). So a fairly similar result holds, but note, this is only works when the numerator is 1." }, { "title": "Hilbert's Hotel", "url": "/articles/2010/HilbertHotel/", "content": "David Hilbert was a greatly influential mathematician born in 1862. He worked in a variety of areas including infinite sets and it is in this area that he presented his ‘Hotel’, or his ‘paradox of the Grand Hotel’. Which helps the reader consider the infinite., so lets give it a go: Consider a hotel with a countably infinite number of rooms. Countably infinite is a set which has the same number of elements (cardinality) of the set of natural numbers (the counting numbers: 0, 1, 2, 3, …, there is no fixed decision on whether 0 is a natural number or not, it isn’t important here but I thought I would mention it). Now consider that all of the rooms in the hotel are currently occupied, you may think that the manager should put up a ‘No Vacancy’ sign, but perhaps not… Suppose that a guest turns up looking for a room. What should the manager say? Well that is easy, he gets every guest already in the hotel to move up one room. So the person in room 1 should move to room 2, the person in room 2 moves to room 3 etc. etc. We know this can continue since there is an infinite number of rooms in the hotel. So then we get that room 1 is empty, and we can put the guest in that room. This same method will work for any finite number of guests that show up at the hotel! Now how about if countably infinite number of people turn up asking for a room in the hotel. Can we fit infinitely more people into the hotel? Once again the manager has no problems, he asks the guests to move rooms. He asks the person in room 1 to move to room 2, the person in room 2 to room 4, the person in room 3 to room 6 and in general the person in room \\(n\\) to \\(2n\\). Again we can do this because there is an infinite number of rooms in the hotel! Now \\(2n\\) is always even, so all the odd numbered rooms are free, and the number of odd numbers is the same as the number of natural numbers, so there are countably infinite spaces. The fact that the odd numbers and natural numbers have the same cardinality itself is counter-intuitive, but hopefully I can convince you that it is true very quickly: \\[\\begin{eqnarray*} 1 \u0026amp;\\longrightarrow \u0026amp; 1 \\\\ 3 \u0026amp;\\longrightarrow \u0026amp; 2 \\\\ 5 \u0026amp;\\longrightarrow \u0026amp; 3 \\\\ 7 \u0026amp;\\longrightarrow \u0026amp; 4 \\\\ 9 \u0026amp;\\longrightarrow \u0026amp; 5 \\\\ 11 \u0026amp;\\longrightarrow \u0026amp; 6 \\\\ \\vdots \u0026amp;\\longrightarrow \u0026amp; \\vdots \\end{eqnarray*}\\] So there is a clear mapping between the natural numbers and the odd numbers. Hence the infinite number of guests can fit in the hotel. The final case to consider, what if a countably infinite number of coaches turn up, each with a countably infinite number of passengers. Is there room for all of them in the hotel? You’ve guessed it, there is! As long as the seats on the coaches are numbered (we can avoid this condition if we can use the beast which is the ‘Axiom of Choice’) we can find everyone a room. First we want to empty all of the odd rooms again, we do this the same as above, move everyone in room \\(n\\) to room \\(2n\\). Now take the 1st coach, say the seats are numbered \\(1, 2, 3, \\ldots\\) then we can place the person in coach 1, seat 1 in room \\(3^1\\), seat 2 in room \\(3^2\\), seat 3 in room \\(3^3\\) etc. etc. Now for coach 2, again with seats numbered \\(1, 2, 3, \\ldots\\) we place the person in seat 1 in room \\(5^1\\), seat 2 to room \\(5^2\\) etc. etc. So in general for coach \\(i\\) and the person in seat \\(n\\) we can place them in room \\(p^n\\) where \\(p\\) is \\(i+1\\)th prime number, and there we have it! We have taken our hotel, with a countably infinite number of rooms, each of which is occupied, and managed to find space for an infinite number of coaches, each with an infinite number of passengers. This is a great concept to test your understanding of infinity. The reason that this all works is because in a hotel with infinitely many rooms the statements ‘every room is occupied’ and ‘no more guests can stay in the hotel’ are not the same." }, { "title": "Probability of Sharing a Birthday", "url": "/articles/2010/sharingABirthday/", "content": "Yesterday was my wife’s birthday and it reminded me about the so called ‘Birthday Problem’. Or, in a group what is the likelihood of two people sharing a birthday? The result is quite surprising… Obviously if the group is of size 366 you are guranteed that at least 2 people must share a birthday (for this, and for the rest of the blog I will ignore February 29th Birthdays, sorry if you happen to be born on that day!) but what is very suprising is that you need only 57 people to have 99% chance and 23 for over a 50% chance. The graph for this is below: The probability of at least two people in a room sharing a birthday as the number of people in that room grows. Let me show you where this comes from. First, to make life a bit easier for ourselves, I am going to define some assumptions: we ignore leap years, the probability of twins and also assume that you are equally likely to be born on any of the 365 days of the year. Now, define \\(P(n)\\) to be the probability of at least two people in a group of \\(n\\) sharing a birthday, and \\(P^{'}(n)\\) the probability of there not being two people sharing a birthday. Also, since \\(P(n)\\) and \\(P^{'}(n)\\) are the only two possibilities and are mutually exclusive we have that: \\[P(n)=1-P^{'}(n).\\] The easiest way of working this out is to first calculate the probability that all \\(n\\) birthdays are different. For only 1 person the probability that that 1 person does not share a birthday with someone is \\(\\frac{365}{365}\\). For 2 people, the probability that the second person has a different birthday from person 1 is \\(\\frac{364}{365}\\). The probability that person 3 has a different birthday from 1 and 2 is \\(\\frac{363}{365}\\), and so on and so on. Finally we take note that when events are independent of each other (as they are in this case), the probability of them all occurring is equal to the product of the probabilities. So we can now calculate the probability of there not being two people sharing a birthday (obviously if \\(n \u0026gt; 365\\) then \\(P^{'}(n)=0\\), this is by the pigeonhole principle). So for \\(n \\le 365\\): \\[\\begin{eqnarray*} P^{'}(n) \u0026amp;=\u0026amp; \\frac{365}{365} \\times \\frac{364}{365} \\times \\frac{363}{365} \\times \\frac{362}{365} \\times \\cdots \\times \\frac{366-n}{365}, \\\\ \u0026amp;=\u0026amp; \\frac{365 \\times 364 \\times 363 \\times 362 \\times \\cdots \\times (366-n)}{365^n}, \\\\ \u0026amp;=\u0026amp; \\frac{365!}{365^n(365-n)!}. \\end{eqnarray*}\\] So, the probability of at least two people sharing a birthday in a group of size \\(n\\) is: \\[P(n)=1-\\frac{365!}{365^n(365-n)!}.\\] And this equation gives you the probablility of two people sharing a birthday in a group of size \\(n\\). If you want to work the probability out for a different value of \\(n\\) just plug the value you want into the equation above and run the calculations. It is a suprising result and counter-intuitive, but always a good one to get out at birthday parties…" } ]